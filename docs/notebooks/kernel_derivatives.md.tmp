<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script>
(function() {
  function addWidgetsRenderer() {
    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var scriptElement = document.createElement('script');
    var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    scriptElement.src = widgetRendererSrc;
    document.body.appendChild(scriptElement);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
# Kernel Derivatives


**Resources**

* [Differentating Gaussian Processes](http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf) - Andrew McHutchen

## Paper Idea

**Linear Operators and Stochastic Partial Differential Equations in GPR** - Simo Särkkä - [PDF](https://users.aalto.fi/~ssarkka/pub/spde.pdf)

> Expresses derivatives of GPs as operators

[**Demo Colab Notebook**](https://colab.research.google.com/drive/1pbb0qlypJCqPTN_cu2GEkkKLNXCYO9F2)

He looks at ths special case where we have a GP with a mean function zero and a covariance matrix $K$ defined as:
$$
\mathbb{E}[f(\mathbf{x})f^\top(\mathbf{x'})] = K_{ff}(\mathbf{x,x'})
$$
So in GP terminology:
$$
f(\mathbf(x)) \sim \mathcal{GP}(\mathbf{0}, K_{ff}(\mathbf{x,x'}))
$$
We use the rulse for linear transformations of GPs to obtain the different transformations of the kernel matrix. 

Let's define the notation for the derivative of a kernel matrix. Let $g(\cdot)$ be the derivative operator on a function $f(\cdot)$. So:
$$
g(\mathbf{x}) = \mathcal{L}_x f(\mathbf{x})
$$

So now, we want to define the cross operators between the derivative $g(\cdot)$ and the function $f(\cdot)$. 

**Example**: He draws a distinction between the two operators with an example of how this works in practice. So let's take the linear operator $\mathcal{L}_{x}=(1, \frac{\partial}{\partial x})$. This operator:

* acts on a scalar GP $f(x)$
* a scalar input $x$ 
* a covariance function $k_{ff}(x,x')$ 
* outputs a scalar value $y$



We can get the following transformations:
$$
\begin{aligned}
K_{gf}(\mathbf{x,x'})
&= \mathcal{L}_x f(\mathbf{x}) f(\mathbf{x}) = \mathcal{L}_xK_{ff}(\mathbf{x,x'}) \\
K_{fg}(\mathbf{x,x'})
&= f(\mathbf{x}) f(\mathbf{x'}) \mathcal{L}_{x'} = K_{ff}(\mathbf{x,x'})\mathcal{L}_{x'} \\
K_{gg}(\mathbf{x,x'})
&= \mathcal{L}_x f(\mathbf{x}) f(\mathbf{x'}) \mathcal{L}_{x'}
= \mathcal{L}_xK_{ff}(\mathbf{x,x'})\mathcal{L}_{x'}^\top \\
\end{aligned}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
#@title Packages
import functools

import jax
import jax.numpy as jnp
import numpy as onp
from sklearn.metrics.pairwise import rbf_kernel as rbf_sklearn
# Plotting libraries
import matplotlib.pyplot as plt
plt.style.use(['seaborn-paper'])
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
#@title Plot Functions

def plot_kernel_mat(K):
    # plot
    plt.figure()
    plt.imshow(K, cmap='Reds')
    plt.title(r'$K_{ff}$, (rbf)', fontsize=20, weight='bold')
    plt.tight_layout()
    plt.show()
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
#@title Data

def get_1d_data(N=30, sigma_inputs=0.15, sigma_obs=0.15, N_test=400):
    onp.random.seed(0)
    X = jnp.linspace(-10, 10, N)
    # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X)
    Y = jnp.sin(1.0 * jnp.pi / 1.6 * jnp.cos(5 + .5 * X))
    Y += sigma_obs * onp.random.randn(N)
    X += sigma_inputs * onp.random.randn(N)
    Y -= jnp.mean(Y)
    Y /= jnp.std(Y)



    X_test = jnp.linspace(-11, 11, N_test) 
    X_test += sigma_inputs * onp.random.randn(N_test)

    X = X[:, None]
    X_test = X[:, None]

    assert X.shape == (N,1)
    assert Y.shape == (N,)

    return X, Y, X_test

def get_2d_data(N=30, sigma_obs=0.15, N_test=400):
    onp.random.seed(0)
    X1 = jnp.linspace(-10, 10, N)
    X2 = jnp.linspace(-5, 2, N)
    # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X)
    Y = jnp.sin(1.0 * jnp.pi / 1.6 * jnp.cos(5 + .5 * X1)) + jnp.exp(X2)
    Y += sigma_obs * onp.random.randn(N)
    Y -= jnp.mean(Y)
    Y /= jnp.std(Y)



    X1_test = jnp.linspace(-11, 11, N_test)
    X2_test = jnp.linspace(-6, 4, N_test) 

    X = jnp.vstack((X1,X2)).T
    X_test = jnp.vstack((X1_test,X2_test)).T

    assert X.shape == (N,2)
    assert Y.shape == (N,)

    return X, Y, X_test

# Get Data
X, Y, X_test = get_1d_data(100, sigma_inputs=0.0, sigma_obs=0.1, N_test=100)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/emmanuel/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU.
  warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
## Kernel Function
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
#@title Kernel Functions

# Squared Euclidean Distance Formula
@jax.jit
def sqeuclidean_distance(x, y):
    return jnp.sum((x-y)**2)

# RBF Kernel
@jax.jit
def rbf_kernel(params, x, y):
    return jnp.exp( - params['gamma'] * sqeuclidean_distance(x, y))
    
# Covariance Matrix
def covariance_matrix(kernel_func, x, y):
    mapx1 = jax.vmap(lambda x, y: kernel_func(x, y), in_axes=(0, None), out_axes=0)
    mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1)
    return mapx2(x, y)
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### RBF Kernel
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
X, Y, X_test = get_2d_data(10, sigma_obs=0.1)

test_X = X[:1, :]
test_Y = X[:1, :]

rbf_x_sk = rbf_sklearn(
    onp.array(test_X.reshape(1, -1)), 
    onp.array(test_Y.reshape(1, -1)), 
    gamma=1.0
)
print(rbf_x_sk.shape, test_X.shape)

params = {'gamma': 1.0, 'var_f': 1.0}
gamma=1.0
rbf_k_ = functools.partial(rbf_kernel, params)
rbf_x = rbf_k_(
    test_X.squeeze(), 
    test_Y.squeeze()
)

onp.testing.assert_array_almost_equal(onp.array(rbf_x), rbf_x_sk)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stdout output_text">
<pre>(1, 1) (1, 2)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
### Kernel Matrix

We defined all of our functions above with only dimensions in mind, not the number of samples or the batch size. So we need to account for that. So if we wanted to calculate the kernel matrix, we would have to loop through all of the samples and calculate the products individually, which would take a long time; especially for large amounts of data. 

> Avoid Loops at all cost in python...

Fortunately, Jax has this incredible function `vmap` which handles batching automatically at apparently, no extra cost. So we can write our functions to account for vectors without having to care about the batch size and then use the `vmap` function to essentially "vectorize" our functions. It essentially allows us to take a product between a matrix and a sample or two vectors of multiple samples. Let's go through an example of how we can construct our kernel matrix.

1. We need to map all points with one vector to another.

We're going to take a single sample from $X'$ and take the rbf kernel between it and all of $X$. So:

$$\text{vmap}_f(\mathbf{X}, \mathbf{x})$$

where $X\in \mathbb{R}^{N \times D}$ is a matrix and $\mathbf{x} \in \mathbb{R}^{D}$ is a vector.
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# Gram Matrix
def gram(func, x, y):
    return jax.vmap(lambda x1: jax.vmap(lambda y1: func(x1, y1))(y))(x)
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# map function 1
mapx1 = jax.vmap(lambda x, y: rbf_kernel(params, x, y), in_axes=(0, None), out_axes=0)

# test the mapping
x1_mapped = mapx1(X, X[0, :])

# Check output shapes, # of dimensions
assert x1_mapped.shape[0] == X.shape[0]   
assert jnp.ndim(x1_mapped) == 1   
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
This that's good: we have an array of size $N$. So we've effectively mapped all points from one array to the other. 

So now we can do another vector mapping which allows us to take all samples of $X'$ and map them against all samples of $X$. So it'll be a `vmap` of a `vmap`. Then we'll get the $N\times N$ kernel matrix.
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1)

K = mapx2(X, X)

# Check output shapes, # of dimensions
assert K.shape[0] == X.shape[0], X.shape[0]   
assert jnp.ndim(K) == 2     

rbf_x_sk = rbf_sklearn(X, X, 1.0)


onp.testing.assert_array_almost_equal(onp.array(rbf_x_sk), K)
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
So great! We now have our kernel matrix. Let's plot it and check to see if it matches the manually constructed kernel matrix.

Great! We have a vectorized kernel function and we were still able to construct our functions in terms of vectors only! This is nice for me personally because I've always struggled with understanding some of the coding when trying to deal with samples/batch-sizes. Most pseudo-code is written in vector format so paper $\rightarrow$ has always been a painful transition for me. So now, let's wrap this in a nice function so that we can finish "wrap up" this model.
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
X, Y, X_test = get_2d_data(10, sigma_obs=0.1)

test_X = X.copy()#[:2, :]
test_Y = X.copy() #[:2, :]

rbf_x_sk = rbf_sklearn(
    onp.array(test_X), 
    onp.array(test_Y), 
    gamma=1.0
)

params = {'gamma': 1.0, 'var_f': 1.0}
rbf_k_ = functools.partial(rbf_kernel, params)
rbf_x = covariance_matrix(
    rbf_k_,
    test_X, 
    test_Y
)

onp.testing.assert_array_almost_equal(onp.array(rbf_x), rbf_x_sk)

plot_kernel_mat(rbf_x)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">


<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
#@title Tests

kx = rbf_kernel(params, X[0], X[0])

# check, the output should be 1.0
assert kx == 1.0, f"Output: {kx}"

kx = rbf_kernel(params, X[0], X[1])

# check, the output should NOT be 1.0
assert kx != 1.0, f"Output: {kx}"


# dk_dx = drbf_kernel(gamma, X[0], X[0])

# # check, the output should be 0.0
# assert dk_dx == 0.0, f"Output: {dk_dx}"

# dk_dx = drbf_kernel(gamma, X[0], X[1])

# # check, the output should NOT be 0.0
# assert dk_dx != 0.0, f"Output: {dk_dx}"
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
#@title Speed Test
    
# Covariance Matrix
def covariance_matrix(kernel_func, x, y):
    mapx1 = jax.vmap(lambda x, y: kernel_func(x, y), in_axes=(0, None), out_axes=0)
    mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1)
    return mapx2(x, y)

def gram(func, x, y):
    return jax.vmap(lambda x1: jax.vmap(lambda y1: func(x1, y1))(x))(y)

rbf_K = functools.partial(rbf_kernel, params)
rbf_cov =  jax.jit(functools.partial(covariance_matrix, rbf_K))
rbf_x = rbf_cov(test_X,  test_Y)


rbf_cov2 =  jax.jit(functools.partial(gram, rbf_K))
rbf_x2 = rbf_cov2(test_X,  test_Y)

onp.testing.assert_array_almost_equal(onp.array(rbf_x), onp.array(rbf_x2))
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
%timeit _ = rbf_cov(test_X,  test_Y)
%timeit _ = rbf_cov2(test_X,  test_Y)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stdout output_text">
<pre>182 µs ± 20.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
167 µs ± 941 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
## 1. Cross-Covariance Term - 1st Derivative


We can calculate the cross-covariance term $K_{fg}(\mathbf{x,x})$. We apply the following operation

$$
K_{fg}(x,x') = k_{ff}(\mathbf{x,x'})(1, \frac{\partial}{\partial x'})
$$
If we multiply the terms across, we get:
$$
K_{fg}(x,x') = k_{ff}(\mathbf{x,x'})\frac{\partial k_{ff}(\mathbf{x,x'})}{\partial x'}
$$

For the RBF Kernel, it's this:

$$\frac{\partial k(x,y)}{\partial x^j}=-2 \gamma (x^j - y^j) k(x,y)$$

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
### Single Sample
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
X, Y, X_test = get_1d_data(10, sigma_obs=0.1)

test_X = X[0:1, :]
test_Y = X[1:2, :]
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### From Scratch
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
def drbf_kernel_scratch(gamma, X, Y):
    dK_fg_ = onp.empty(X.shape[-1])
    
    constant = - 2 * gamma
    
    k_val = rbf_sklearn(onp.array(X), onp.array(Y), gamma=gamma)
    
    for idim in range(X.shape[1]):
        
        x_val = X[:, idim] - Y[:, idim]

        dK_fg_[idim] = constant * k_val *  x_val 
    return dK_fg_
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
dK_fg_ = drbf_kernel_scratch(gamma, test_X, test_Y)
print(dK_fg_)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.01392619]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### Jax
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# define the cross operator K_fg(x, y), dK wrt x
drbf_kernel_fg = jax.jacobian(rbf_kernel, argnums=(1))

# calculate for a single sample
dK_fg = drbf_kernel_fg(params, test_X[0,:], test_Y[0,:])
print(dK_fg)

```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.01392619]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
### Multiple Dimensions
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
X, Y, X_test = get_2d_data(10, sigma_obs=0.1)

test_X = X[0:1, :]
test_Y = X[1:2, :]
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### From Scratch
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
dK_fg_ = drbf_kernel_scratch(gamma, test_X, test_Y)
print(dK_fg_)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.0173953  0.00608835]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### Jax
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# define the cross operator K_fg(x, y), dK wrt x
drbf_kernel_fg = jax.jacobian(rbf_kernel, argnums=(1))

# calculate for a single sample
dK_fg = drbf_kernel_fg(params, test_X[0,:], test_Y[0,:])
print(dK_fg)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.0173953  0.00608835]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
### Multiple Samples (Batches)
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
X, Y, X_test = get_2d_data(10, sigma_obs=0.1)

test_X = X
test_Y = X
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### From Scratch
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
dK_fg_ = onp.empty((test_X.shape[0], test_X.shape[0], test_X.shape[1]))

for i in range(test_X.shape[0]):
    for j in range(test_Y.shape[0]):
        
        dK_fg_[i, j, :] = drbf_kernel_scratch(gamma, onp.array(test_X[i, :]).reshape(1,-1), onp.array(test_Y[j, :]).reshape(1, -1))
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### Jax


</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# define the cross operator K_fg(x, y), dK wrt x
drbf_kernel_fg = jax.jacobian(rbf_kernel, argnums=(1))

K_func = functools.partial(drbf_kernel_fg, params)
dK_fg = gram(K_func, test_X, test_Y)


onp.testing.assert_array_almost_equal(onp.array(dK_fg), dK_fg_)
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
## 2. Cross-Covariance Term - 2nd Derivative

$$
\frac{\partial^2 k(x,y)}{\partial x^{j^2}} =
2 \gamma \left[ 2\gamma(x^j - y^j)^2 - 1\right] k(\mathbf{x}, \mathbf{y})
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### From Scratch
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
def d2rbf_kernel_scratch_jac(gamma, X, Y):
    d2K_fg2_ = onp.empty(X.shape[-1])
    
    constant = 2 * gamma
    
    k_val = rbf_sklearn(onp.array(X), onp.array(Y), gamma=gamma)
    
    for idim in range(X.shape[1]):
        
        x_val = constant * (X[:, idim] - Y[:, idim]) ** 2 - 1

        d2K_fg2_[idim] = constant * k_val *  x_val 
    return d2K_fg2_
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
d2K_fg2_ = onp.empty((test_X.shape[0], test_X.shape[0], test_X.shape[1]))

for i in range(test_X.shape[0]):
    for j in range(test_Y.shape[0]):
        
        d2K_fg2_[i, j, :] = d2rbf_kernel_scratch_jac(gamma, onp.array(test_X[i, :]).reshape(1,-1), onp.array(test_Y[j, :]).reshape(1, -1))
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### Jax
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# define the cross operator K_fg(x, y), dK wrt x
dK_fg_func = jax.hessian(rbf_kernel, argnums=(1))

K_func = functools.partial(dK_fg_func, params)
d2K_fg2 = covariance_matrix(K_func, test_X, test_Y)

d2K_fg2 = jnp.diagonal(d2K_fg2, axis1=2, axis2=3)

d2K_fg2.shape
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">


<div class="output_text output_subarea output_execute_result">
<pre>(10, 10, 2)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
onp.testing.assert_array_almost_equal(onp.array(d2K_fg2), d2K_fg2_)
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
## 3. Cross-Covariance Term - 2nd Derivative (Partial Derivatives)

$$
\frac{\partial^2 k(x,y)}{\partial x^j \partial y^k} =
4 \gamma^2 (x^k - y^k)(x^j - y^j) k(\mathbf{x}, \mathbf{y})
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### From Scratch
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
def d2rbf_kernel_scratch_hessian(gamma, X, Y):
    d2K_fg2_ = onp.empty((X.shape[-1], X.shape[-1]))
    
    constant = 2 * gamma
    constant_sq = constant ** 2
    
    k_val = rbf_sklearn(onp.array(X), onp.array(Y), gamma=gamma)
    
    for idim in range(X.shape[1]):
        for jdim in range(X.shape[1]):
        
            x_val = constant * (1 - constant * (X[:, idim] - Y[:, idim]) * (X[:, jdim] - Y[:, jdim]))# - constant

            d2K_fg2_[idim, jdim] = k_val *  x_val 
    return d2K_fg2_
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
d2K_fg2_ = onp.empty((test_X.shape[0], test_X.shape[0], test_X.shape[1], test_X.shape[1]))

for i in range(test_X.shape[0]):
    for j in range(test_Y.shape[0]):
        
        d2K_fg2_[i, j, ...] = d2rbf_kernel_scratch_hessian(gamma, onp.array(test_X[i, :]).reshape(1,-1), onp.array(test_Y[j, :]).reshape(1, -1))
```

</div>

</div>
<div class="cell border-box-sizing text_cell rendered" markdown="1">
<div class="inner_cell" markdown="1">
<div class="text_cell_render border-box-sizing rendered_html" markdown="1">
#### Jax
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
# define the cross operator K_fg(x, y), dK wrt x
dK_fg_func = jax.hessian(rbf_kernel, argnums=(1))

K_func = functools.partial(dK_fg_func, params)
d2K_fg2 = covariance_matrix(K_func, test_X, test_Y)

d2K_fg2.shape
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">


<div class="output_text output_subarea output_execute_result">
<pre>(10, 10, 2, 2)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
onp.testing.assert_array_almost_equal(onp.array(onp.diagonal(d2K_fg2, axis1=2, axis2=3 )), jnp.diagonal(d2K_fg2, axis1=2, axis2=3))
```

</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python
onp.testing.assert_array_almost_equal(onp.array(d2K_fg2), d2K_fg2_)
```

</div>

<div class="output_wrapper" markdown="1">
<div class="output" markdown="1">


<div class="output_area" markdown="1">
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AssertionError</span>                            Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-67-a1b4fece15e3&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>onp<span class="ansi-blue-fg">.</span>testing<span class="ansi-blue-fg">.</span>assert_array_almost_equal<span class="ansi-blue-fg">(</span>onp<span class="ansi-blue-fg">.</span>array<span class="ansi-blue-fg">(</span>d2K_fg2<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> d2K_fg2_<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/.conda/envs/jax_py38/lib/python3.8/site-packages/numpy/testing/_private/utils.py</span> in <span class="ansi-cyan-fg">assert_array_almost_equal</span><span class="ansi-blue-fg">(x, y, decimal, err_msg, verbose)</span>
<span class="ansi-green-intense-fg ansi-bold">   1043</span>         <span class="ansi-green-fg">return</span> z <span class="ansi-blue-fg">&lt;</span> <span class="ansi-cyan-fg">1.5</span> <span class="ansi-blue-fg">*</span> <span class="ansi-cyan-fg">10.0</span><span class="ansi-blue-fg">**</span><span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">-</span>decimal<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1044</span> 
<span class="ansi-green-fg">-&gt; 1045</span><span class="ansi-red-fg">     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,
</span><span class="ansi-green-intense-fg ansi-bold">   1046</span>              header<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;Arrays are not almost equal to %d decimals&#39;</span> <span class="ansi-blue-fg">%</span> decimal<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1047</span>              precision=decimal)

<span class="ansi-green-fg">~/.conda/envs/jax_py38/lib/python3.8/site-packages/numpy/testing/_private/utils.py</span> in <span class="ansi-cyan-fg">assert_array_compare</span><span class="ansi-blue-fg">(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)</span>
<span class="ansi-green-intense-fg ansi-bold">    844</span>                                 verbose<span class="ansi-blue-fg">=</span>verbose<span class="ansi-blue-fg">,</span> header<span class="ansi-blue-fg">=</span>header<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                                 names=(&#39;x&#39;, &#39;y&#39;), precision=precision)
<span class="ansi-green-fg">--&gt; 846</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">raise</span> AssertionError<span class="ansi-blue-fg">(</span>msg<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    847</span>     <span class="ansi-green-fg">except</span> ValueError<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>         <span class="ansi-green-fg">import</span> traceback

<span class="ansi-red-fg">AssertionError</span>: 
Arrays are not almost equal to 6 decimals

Mismatched elements: 112 / 400 (28%)
Max absolute difference: 4.
Max relative difference: 2.40703559
 x: array([[[[-2.000000e+00,  0.000000e+00],
         [ 0.000000e+00, -2.000000e+00]],
...
 y: array([[[[ 2.000000e+00,  2.000000e+00],
         [ 2.000000e+00,  2.000000e+00]],
...</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered" markdown="1">
<div class="input">

```python

```

</div>

</div>


