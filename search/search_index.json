{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Input Uncertainty for Gaussian Processes \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Documentation: jejjohnson.github.io/uncertain_gps Repo: github.com/jejjohnson/uncertain_gps A graphical model of a GP algorithm with the addition of uncertainty component for the input. This repository is home to my studies on uncertain inputs for Gaussian processes. Gaussian processes are a kernel Bayesian framework that is known to generalize well for small datasets and also offers predictive mean and predictive variance estimates. It is one of the most complete models that model uncertainty. Demo showing the error bars for a standard GP predictive variance and an augmented predictive variance estimate using Taylor expansions. In this repository, I am interested in exploring the capabilities and limits with Gaussian process regression algorithms when handling noisy inputs. Input uncertainty is often not talked about in the machine learning literature, so I will be exploring this in great detail for my thesis. Methods \u00b6 Taylor Approximation \u00b6 \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} We can approximate the predictive mean and variance equations via a Taylor expansion which adds a corrective term w.r.t. the derivative of the function and the known variance. This assumes we know the variance and we don't modify the predictive mean of the learned GP function. 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation. Unscented Transforms \u00b6 The predictive mean and variance using the Unscented transformation. Variational Inference \u00b6 TODO Monte Carlo Estimation \u00b6 The predictive mean and variance using the MonteCarlo sampling of the posterior. The predictive mean and variance using the MonteCarlo for training. My Resources \u00b6 Literature Review \u00b6 I have gone through most of the relevant works related to noisy inputs in the context of Gaussian processes. It is very extensive and it also offers some literature that is relevant but may not explicitly mentioned uncertain inputs in the paper. Documentation \u00b6 I have some documentation which has all of my personal notes and derivations related to GPs and noisy inputs. Some highlights include the Taylor approximation, moment matching and variational inference. GP Model Zoo \u00b6 I have documented and try to keep up with some of the latest Gaussian process literature in my repository.","title":"Input Uncertainty for Gaussian Processes"},{"location":"#input-uncertainty-for-gaussian-processes","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Documentation: jejjohnson.github.io/uncertain_gps Repo: github.com/jejjohnson/uncertain_gps A graphical model of a GP algorithm with the addition of uncertainty component for the input. This repository is home to my studies on uncertain inputs for Gaussian processes. Gaussian processes are a kernel Bayesian framework that is known to generalize well for small datasets and also offers predictive mean and predictive variance estimates. It is one of the most complete models that model uncertainty. Demo showing the error bars for a standard GP predictive variance and an augmented predictive variance estimate using Taylor expansions. In this repository, I am interested in exploring the capabilities and limits with Gaussian process regression algorithms when handling noisy inputs. Input uncertainty is often not talked about in the machine learning literature, so I will be exploring this in great detail for my thesis.","title":"Input Uncertainty for Gaussian Processes"},{"location":"#methods","text":"","title":"Methods"},{"location":"#taylor-approximation","text":"\\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} We can approximate the predictive mean and variance equations via a Taylor expansion which adds a corrective term w.r.t. the derivative of the function and the known variance. This assumes we know the variance and we don't modify the predictive mean of the learned GP function. 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation.","title":"Taylor Approximation"},{"location":"#unscented-transforms","text":"The predictive mean and variance using the Unscented transformation.","title":"Unscented Transforms"},{"location":"#variational-inference","text":"TODO","title":"Variational Inference"},{"location":"#monte-carlo-estimation","text":"The predictive mean and variance using the MonteCarlo sampling of the posterior. The predictive mean and variance using the MonteCarlo for training.","title":"Monte Carlo Estimation"},{"location":"#my-resources","text":"","title":"My Resources"},{"location":"#literature-review","text":"I have gone through most of the relevant works related to noisy inputs in the context of Gaussian processes. It is very extensive and it also offers some literature that is relevant but may not explicitly mentioned uncertain inputs in the paper.","title":"Literature Review"},{"location":"#documentation","text":"I have some documentation which has all of my personal notes and derivations related to GPs and noisy inputs. Some highlights include the Taylor approximation, moment matching and variational inference.","title":"Documentation"},{"location":"#gp-model-zoo","text":"I have documented and try to keep up with some of the latest Gaussian process literature in my repository.","title":"GP Model Zoo"},{"location":"MonteCarlo/demo/","text":"MCMC eGP \u00b6 TLDR I did a quick experiment where I look at how we can impact the error bars when doing a fully Bayesian GP (i.e. GP with MCMC inference). I have 3 cases where I use no prior on the inputs, where I use a modest prior on the inputs, and one where I use the exact known prior on the inputs. The results are definitely different than what I'm used to because I actually trained the GP knowing the priors. The error bars were reduced which I guess makes sense. TODO : Do the MCMC where we approximate the posterior when we trained the GP with uncertain inputs. Posterior Approximation Training Exact Prior Known Input Error Experiment \u00b6 Code Blocks Install ! pip install jax jaxlib numpyro Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.62) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.42) Collecting numpyro \u001b[?25l Downloading https://files.pythonhosted.org/packages/b8/58/54e914bb6d8ee9196f8dbf28b81057fea81871fc171dbee03b790336d0c5/numpyro-0.2.4-py3-none-any.whl (159kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.5MB/s \u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.2.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.3) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.38.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.12.0) \u001b[31mERROR: numpyro 0.2.4 has requirement jax==0.1.57, but you'll have jax 0.1.62 which is incompatible.\u001b[0m \u001b[31mERROR: numpyro 0.2.4 has requirement jaxlib==0.1.37, but you'll have jaxlib 0.1.42 which is incompatible.\u001b[0m Installing collected packages: numpyro Successfully installed numpyro-0.2.4 Imports #@title packages import time import numpy as onp from dataclasses import dataclass import jax from jax import vmap import jax.numpy as np import jax.random as random import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS import matplotlib import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline Data #@title Data def get_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = np . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = np . sin ( 1.0 * np . pi / 1.6 * np . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= np . mean ( Y ) Y /= np . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = np . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) return X , Y , X_test GP Model \u00b6 #@title GP Model # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = np . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * np . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * np . eye ( X . shape [ 0 ]) return k def model ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15 * np.ones((Xmu.shape[0],)))) X = Xmu # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( ' \\n MCMC elapsed time:' , time . time () - start ) return mcmc . get_samples () # do GP prediction for a given set of hyperparameters. this makes use of the well-known # formula for gaussian process predictions def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = np . linalg . inv ( k_XX ) K = k_pp - np . matmul ( k_pX , np . matmul ( K_xx_inv , np . transpose ( k_pX ))) sigma_noise = np . sqrt ( np . clip ( np . diag ( K ), a_min = 0. )) * jax . random . normal ( rng_key , X_test . shape [: 1 ]) mean = np . matmul ( k_pX , np . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise Experiment \u00b6 @dataclass class args : num_data = 60 num_warmup = 100 num_chains = 1 num_samples = 1_000 device = 'cpu' sigma_inputs = 0.3 sigma_obs = 0.05 numpyro . set_platform ( args . device ) X , Y , X_test = get_data ( args . num_data , sigma_inputs = args . sigma_inputs , sigma_obs = args . sigma_obs ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( model , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:11<00:00, 96.81it/s, 7 steps of size 6.48e-01. acc. prob=0.94] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1.97 0.23 1.97 1.58 2.34 650.87 1.00 kernel_noise 0.04 0.01 0.04 0.02 0.05 637.46 1.00 kernel_var 1.15 0.65 0.98 0.34 1.97 563.69 1.00 Number of divergences: 0 MCMC elapsed time: 14.073462963104248 Predictions \u00b6 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] GP Model - Uncertain Inputs \u00b6 def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] Results \u00b6 Exact Known Input Error Prior","title":"MCMC eGP"},{"location":"MonteCarlo/demo/#mcmc-egp","text":"TLDR I did a quick experiment where I look at how we can impact the error bars when doing a fully Bayesian GP (i.e. GP with MCMC inference). I have 3 cases where I use no prior on the inputs, where I use a modest prior on the inputs, and one where I use the exact known prior on the inputs. The results are definitely different than what I'm used to because I actually trained the GP knowing the priors. The error bars were reduced which I guess makes sense. TODO : Do the MCMC where we approximate the posterior when we trained the GP with uncertain inputs. Posterior Approximation Training Exact Prior Known Input Error","title":"MCMC eGP"},{"location":"MonteCarlo/demo/#experiment","text":"Code Blocks Install ! pip install jax jaxlib numpyro Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.62) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.42) Collecting numpyro \u001b[?25l Downloading https://files.pythonhosted.org/packages/b8/58/54e914bb6d8ee9196f8dbf28b81057fea81871fc171dbee03b790336d0c5/numpyro-0.2.4-py3-none-any.whl (159kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.5MB/s \u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.2.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.3) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.38.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.12.0) \u001b[31mERROR: numpyro 0.2.4 has requirement jax==0.1.57, but you'll have jax 0.1.62 which is incompatible.\u001b[0m \u001b[31mERROR: numpyro 0.2.4 has requirement jaxlib==0.1.37, but you'll have jaxlib 0.1.42 which is incompatible.\u001b[0m Installing collected packages: numpyro Successfully installed numpyro-0.2.4 Imports #@title packages import time import numpy as onp from dataclasses import dataclass import jax from jax import vmap import jax.numpy as np import jax.random as random import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS import matplotlib import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline Data #@title Data def get_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = np . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = np . sin ( 1.0 * np . pi / 1.6 * np . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= np . mean ( Y ) Y /= np . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = np . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) return X , Y , X_test","title":"Experiment"},{"location":"MonteCarlo/demo/#gp-model","text":"#@title GP Model # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = np . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * np . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * np . eye ( X . shape [ 0 ]) return k def model ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15 * np.ones((Xmu.shape[0],)))) X = Xmu # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( ' \\n MCMC elapsed time:' , time . time () - start ) return mcmc . get_samples () # do GP prediction for a given set of hyperparameters. this makes use of the well-known # formula for gaussian process predictions def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = np . linalg . inv ( k_XX ) K = k_pp - np . matmul ( k_pX , np . matmul ( K_xx_inv , np . transpose ( k_pX ))) sigma_noise = np . sqrt ( np . clip ( np . diag ( K ), a_min = 0. )) * jax . random . normal ( rng_key , X_test . shape [: 1 ]) mean = np . matmul ( k_pX , np . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise","title":"GP Model"},{"location":"MonteCarlo/demo/#experiment_1","text":"@dataclass class args : num_data = 60 num_warmup = 100 num_chains = 1 num_samples = 1_000 device = 'cpu' sigma_inputs = 0.3 sigma_obs = 0.05 numpyro . set_platform ( args . device ) X , Y , X_test = get_data ( args . num_data , sigma_inputs = args . sigma_inputs , sigma_obs = args . sigma_obs ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( model , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:11<00:00, 96.81it/s, 7 steps of size 6.48e-01. acc. prob=0.94] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1.97 0.23 1.97 1.58 2.34 650.87 1.00 kernel_noise 0.04 0.01 0.04 0.02 0.05 637.46 1.00 kernel_var 1.15 0.65 0.98 0.34 1.97 563.69 1.00 Number of divergences: 0 MCMC elapsed time: 14.073462963104248","title":"Experiment"},{"location":"MonteCarlo/demo/#predictions","text":"# do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"Predictions"},{"location":"MonteCarlo/demo/#gp-model-uncertain-inputs","text":"def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"GP Model - Uncertain Inputs"},{"location":"MonteCarlo/demo/#results","text":"Exact Known Input Error Prior","title":"Results"},{"location":"Notes/approximate/","text":"Error Propagation in Gaussian Transformations \u00b6 We're in the setting where we have some inputs that come from a distribution \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we have some outputs y\\in\\mathbb{R} y\\in\\mathbb{R} . Typically we have some function f(\\mathbf) f(\\mathbf) that maps the points f:\\mathbb{R}^D \\rightarrow \\mathbb{R} f:\\mathbb{R}^D \\rightarrow \\mathbb{R} . So like we do with GPs, we have the following function\" y=f(\\mathbf{x}) y=f(\\mathbf{x}) Change of Variables \u00b6 To have this full mapping, we would need to use the change of variables method because we are doing a transformation between of probability distributions. \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} This is very expensive and can be very difficult depending upon the function. Conditional Gaussian Distribution \u00b6 Alternatively, if we know that f() f() is described by a Gaussian distribution, then we can find the joint distribution between \\mathbf{x},y \\mathbf{x},y . This can be described as: \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) Taylor Expansions \u00b6 by using a Taylor series expansion. Let \\mu_x \\mu_x be the true inputs and we perturb these by some noise \\delta_x \\delta_x which is described by a normal distribution \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) . So we can write an expression for the function f(\\mathbf{x}) f(\\mathbf{x}) . \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} where \\nabla_x f(\\mu_x) \\nabla_x f(\\mu_x) is the jacobian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x , \\nabla_{xx}f(\\mu_x) \\nabla_{xx}f(\\mu_x) is the hessian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x and e_i e_i is a ones vector (essentially the trace of a full matrix). Joint Distribution \u00b6 So, we want \\tilde{f}(\\mathbf{x}) \\tilde{f}(\\mathbf{x}) which is a joint distribution of \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} . But as we said, this is difficult to compute so we do a Taylor approximation to this function \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} . But we still want the full joint distribution , ideally Gaussian. So that would mean we at least need the expectation and the covariance. \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] Derivation Mean Function The mean function is the easiest to derive. We can just take the expectation of the first two terms and we'll see why all the higher order terms disappear. \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} Covariance Function The covariance function is a bit harder. But again, the final expression is quite simple. \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} So now we can have a full expression for the joint distribution with the Taylor expansion. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) where: \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} Joint Distribution for Additive Noise \u00b6 So in this setting, we are closer to what we typically use for a GP model. y = f(\\mathbf{x}) + \\epsilon y = f(\\mathbf{x}) + \\epsilon where we have noisy inputs with additive Gaussian noise \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we assume additive Gaussian noise for the outputs \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) . We want a joint distribution of \\mathbf{x},y \\mathbf{x},y . So using the same sequences of steps as above, we actually get a very similar joint distribution, just with an additional term. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) where \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} So if we want to make predictions with our new model, we will have the final equation as: \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"Gaussian Approximations"},{"location":"Notes/approximate/#error-propagation-in-gaussian-transformations","text":"We're in the setting where we have some inputs that come from a distribution \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we have some outputs y\\in\\mathbb{R} y\\in\\mathbb{R} . Typically we have some function f(\\mathbf) f(\\mathbf) that maps the points f:\\mathbb{R}^D \\rightarrow \\mathbb{R} f:\\mathbb{R}^D \\rightarrow \\mathbb{R} . So like we do with GPs, we have the following function\" y=f(\\mathbf{x}) y=f(\\mathbf{x})","title":"Error Propagation in Gaussian Transformations"},{"location":"Notes/approximate/#change-of-variables","text":"To have this full mapping, we would need to use the change of variables method because we are doing a transformation between of probability distributions. \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} This is very expensive and can be very difficult depending upon the function.","title":"Change of Variables"},{"location":"Notes/approximate/#conditional-gaussian-distribution","text":"Alternatively, if we know that f() f() is described by a Gaussian distribution, then we can find the joint distribution between \\mathbf{x},y \\mathbf{x},y . This can be described as: \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Conditional Gaussian Distribution"},{"location":"Notes/approximate/#taylor-expansions","text":"by using a Taylor series expansion. Let \\mu_x \\mu_x be the true inputs and we perturb these by some noise \\delta_x \\delta_x which is described by a normal distribution \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) . So we can write an expression for the function f(\\mathbf{x}) f(\\mathbf{x}) . \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} where \\nabla_x f(\\mu_x) \\nabla_x f(\\mu_x) is the jacobian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x , \\nabla_{xx}f(\\mu_x) \\nabla_{xx}f(\\mu_x) is the hessian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x and e_i e_i is a ones vector (essentially the trace of a full matrix).","title":"Taylor Expansions"},{"location":"Notes/approximate/#joint-distribution","text":"So, we want \\tilde{f}(\\mathbf{x}) \\tilde{f}(\\mathbf{x}) which is a joint distribution of \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} . But as we said, this is difficult to compute so we do a Taylor approximation to this function \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} . But we still want the full joint distribution , ideally Gaussian. So that would mean we at least need the expectation and the covariance. \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] Derivation Mean Function The mean function is the easiest to derive. We can just take the expectation of the first two terms and we'll see why all the higher order terms disappear. \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} Covariance Function The covariance function is a bit harder. But again, the final expression is quite simple. \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} So now we can have a full expression for the joint distribution with the Taylor expansion. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) where: \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned}","title":"Joint Distribution"},{"location":"Notes/approximate/#joint-distribution-for-additive-noise","text":"So in this setting, we are closer to what we typically use for a GP model. y = f(\\mathbf{x}) + \\epsilon y = f(\\mathbf{x}) + \\epsilon where we have noisy inputs with additive Gaussian noise \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we assume additive Gaussian noise for the outputs \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) . We want a joint distribution of \\mathbf{x},y \\mathbf{x},y . So using the same sequences of steps as above, we actually get a very similar joint distribution, just with an additional term. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) where \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} So if we want to make predictions with our new model, we will have the final equation as: \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"Joint Distribution for Additive Noise"},{"location":"Notes/basics/","text":"Gaussian Process Basics \u00b6 Data \u00b6 Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise. Model \u00b6 Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} We specify a mean function, m m and a covariance function k k . Likelihood , {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} which describes the dataset Marginal Likelihood , {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} . Posterior \u00b6 First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F}) Deterministic Inputs \u00b6 In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) Probabilistic Inputs \u00b6 In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} Variational GP Models \u00b6 Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Sparse GP Models \u00b6 Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Basics"},{"location":"Notes/basics/#gaussian-process-basics","text":"","title":"Gaussian Process Basics"},{"location":"Notes/basics/#data","text":"Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise.","title":"Data"},{"location":"Notes/basics/#model","text":"Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} We specify a mean function, m m and a covariance function k k . Likelihood , {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} which describes the dataset Marginal Likelihood , {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} .","title":"Model"},{"location":"Notes/basics/#posterior","text":"First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F})","title":"Posterior"},{"location":"Notes/basics/#deterministic-inputs","text":"In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I})","title":"Deterministic Inputs"},{"location":"Notes/basics/#probabilistic-inputs","text":"In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned}","title":"Probabilistic Inputs"},{"location":"Notes/basics/#variational-gp-models","text":"Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right)","title":"Variational GP Models"},{"location":"Notes/basics/#sparse-gp-models","text":"Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Sparse GP Models"},{"location":"Notes/error_propagation/","text":"Error Propagation \u00b6 Taylor Series Expansion Law of Error Propagation Proof: Mean Function Proof: Variance Function Resources Taylor Series Expansion \u00b6 A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) Law of Error Propagation \u00b6 This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} Proof: Mean Function \u00b6 Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} Proof: Variance Function \u00b6 Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. Resources \u00b6 Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Error Propagation"},{"location":"Notes/error_propagation/#error-propagation","text":"Taylor Series Expansion Law of Error Propagation Proof: Mean Function Proof: Variance Function Resources","title":"Error Propagation"},{"location":"Notes/error_propagation/#taylor-series-expansion","text":"A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right)","title":"Taylor Series Expansion"},{"location":"Notes/error_propagation/#law-of-error-propagation","text":"This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top}","title":"Law of Error Propagation"},{"location":"Notes/error_propagation/#proof-mean-function","text":"Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned}","title":"Proof: Mean Function"},{"location":"Notes/error_propagation/#proof-variance-function","text":"Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself.","title":"Proof: Variance Function"},{"location":"Notes/error_propagation/#resources","text":"Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Resources"},{"location":"Notes/literature/","text":"Uncertain Inputs in Gaussian Processe \u00b6 Motivation \u00b6 This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs. Algorithms \u00b6 Error-In-Variables Regression \u00b6 This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ] Monte Carlo Sampling \u00b6 So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this. Taylor Expansion \u00b6 Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ] Moment Matching \u00b6 This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end) Covariance Functions \u00b6 Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code Iterative \u00b6 Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR Linearized (Unscented) Approximation \u00b6 This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach. Heteroscedastic Likelihood Models \u00b6 Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code Latent Variable Models \u00b6 Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016) Latent Covariates \u00b6 Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) Variational Strategies \u00b6 Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Appendix \u00b6 Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Literature \u00b6 Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform Sigma Point Transform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel Toolboxes \u00b6 Emukit Connecting Concepts \u00b6 Moment Matching \u00b6 Derivatives of GPs \u00b6 Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018) Extended Kalman Filter \u00b6 This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code Uncertain Inputs in other ML fields \u00b6 Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation Key Equations \u00b6 Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Literature"},{"location":"Notes/literature/#uncertain-inputs-in-gaussian-processe","text":"","title":"Uncertain Inputs in Gaussian Processe"},{"location":"Notes/literature/#motivation","text":"This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs.","title":"Motivation"},{"location":"Notes/literature/#algorithms","text":"","title":"Algorithms"},{"location":"Notes/literature/#error-in-variables-regression","text":"This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ]","title":"Error-In-Variables Regression"},{"location":"Notes/literature/#monte-carlo-sampling","text":"So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this.","title":"Monte Carlo Sampling"},{"location":"Notes/literature/#taylor-expansion","text":"Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ]","title":"Taylor Expansion"},{"location":"Notes/literature/#moment-matching","text":"This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end)","title":"Moment Matching"},{"location":"Notes/literature/#covariance-functions","text":"Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code","title":"Covariance Functions"},{"location":"Notes/literature/#iterative","text":"Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR","title":"Iterative"},{"location":"Notes/literature/#linearized-unscented-approximation","text":"This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach.","title":"Linearized (Unscented) Approximation"},{"location":"Notes/literature/#heteroscedastic-likelihood-models","text":"Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code","title":"Heteroscedastic Likelihood Models"},{"location":"Notes/literature/#latent-variable-models","text":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016)","title":"Latent Variable Models"},{"location":"Notes/literature/#latent-covariates","text":"Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019)","title":"Latent Covariates"},{"location":"Notes/literature/#variational-strategies","text":"Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Variational Strategies"},{"location":"Notes/literature/#appendix","text":"","title":"Appendix"},{"location":"Notes/literature/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"Notes/literature/#literature","text":"Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform Sigma Point Transform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel","title":"Literature"},{"location":"Notes/literature/#toolboxes","text":"Emukit","title":"Toolboxes"},{"location":"Notes/literature/#connecting-concepts","text":"","title":"Connecting Concepts"},{"location":"Notes/literature/#moment-matching_1","text":"","title":"Moment Matching"},{"location":"Notes/literature/#derivatives-of-gps","text":"Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018)","title":"Derivatives of GPs"},{"location":"Notes/literature/#extended-kalman-filter","text":"This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code","title":"Extended Kalman Filter"},{"location":"Notes/literature/#uncertain-inputs-in-other-ml-fields","text":"Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation","title":"Uncertain Inputs in other ML fields"},{"location":"Notes/literature/#key-equations","text":"Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Key Equations"},{"location":"Notes/mm/","text":"Moment-Matching \u00b6","title":"Moment-Matching"},{"location":"Notes/mm/#moment-matching","text":"","title":"Moment-Matching"},{"location":"Notes/next/","text":"Next Steps \u00b6 So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen: 1. Apply these algorithms to different problems (other than dynamical systems) \u00b6 It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it? 2. Improve the Kernel Expectation Calculations \u00b6 So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there. 3. Think about the problem differently \u00b6 An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too! 4. Think about pragmatic solutions \u00b6 Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free. 5. Figure Out how to extend it to Deep GPs \u00b6 So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"Next Steps"},{"location":"Notes/next/#next-steps","text":"So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen:","title":"Next Steps"},{"location":"Notes/next/#1-apply-these-algorithms-to-different-problems-other-than-dynamical-systems","text":"It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it?","title":"1. Apply these algorithms to different problems (other than dynamical systems)"},{"location":"Notes/next/#2-improve-the-kernel-expectation-calculations","text":"So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there.","title":"2. Improve the Kernel Expectation Calculations"},{"location":"Notes/next/#3-think-about-the-problem-differently","text":"An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too!","title":"3. Think about the problem differently"},{"location":"Notes/next/#4-think-about-pragmatic-solutions","text":"Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free.","title":"4. Think about pragmatic solutions"},{"location":"Notes/next/#5-figure-out-how-to-extend-it-to-deep-gps","text":"So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"5. Figure Out how to extend it to Deep GPs"},{"location":"Notes/software/","text":"Software \u00b6 GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch. GPy \u00b6 This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems. My Model Zoo \u00b6 Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github GPFlow \u00b6 This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting. Pyro \u00b6 This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research. My Model Zoo \u00b6 Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab GPyTorch \u00b6 This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications. Summary \u00b6 Algorithms Implemented \u00b6 Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Software"},{"location":"Notes/software/#software","text":"GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch.","title":"Software"},{"location":"Notes/software/#gpy","text":"This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems.","title":"GPy"},{"location":"Notes/software/#my-model-zoo","text":"Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github","title":"My Model Zoo"},{"location":"Notes/software/#gpflow","text":"This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting.","title":"GPFlow"},{"location":"Notes/software/#pyro","text":"This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research.","title":"Pyro"},{"location":"Notes/software/#my-model-zoo_1","text":"Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab","title":"My Model Zoo"},{"location":"Notes/software/#gpytorch","text":"This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications.","title":"GPyTorch"},{"location":"Notes/software/#summary","text":"","title":"Summary"},{"location":"Notes/software/#algorithms-implemented","text":"Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Algorithms Implemented"},{"location":"Notes/stochastic/","text":"","title":"Stochastic"},{"location":"Notes/taylor/","text":"Linearization (Taylor Expansions) \u00b6 Conditional Gaussian Distributions I: Additive Noise Model ( x,f x,f ) Other GP Methods II: Non-Additive Noise Model III: Quadratic Approximation Literature Supplementary Error Propagation Fubini's Theorem Law of Iterated Expecations Conditional Variance Analytical Moments \u00b6 The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the Mean Function \u00b6 \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function \u00b6 The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} Taylor Approximation \u00b6 We will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Linearized Predictive Mean and Variance \u00b6 \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term. Conditional Gaussian Distributions \u00b6 I: Additive Noise Model ( x,f x,f ) \u00b6 This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP} {-1}K_{*} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top . Other GP Methods \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{ z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{ } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. II: Non-Additive Noise Model \u00b6 III: Quadratic Approximation \u00b6 Literature \u00b6 Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference. Supplementary \u00b6 Error Propagation \u00b6 To see more about error propagation and the relation to the mean and variance, see here . Fubini's Theorem \u00b6 Law of Iterated Expecations \u00b6 Conditional Variance \u00b6","title":"Linearization (Taylor Expansions)"},{"location":"Notes/taylor/#linearization-taylor-expansions","text":"Conditional Gaussian Distributions I: Additive Noise Model ( x,f x,f ) Other GP Methods II: Non-Additive Noise Model III: Quadratic Approximation Literature Supplementary Error Propagation Fubini's Theorem Law of Iterated Expecations Conditional Variance","title":"Linearization (Taylor Expansions)"},{"location":"Notes/taylor/#analytical-moments","text":"The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the","title":"Analytical Moments"},{"location":"Notes/taylor/#mean-function","text":"\\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned}","title":"Mean Function"},{"location":"Notes/taylor/#variance-function","text":"The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned}","title":"Variance Function"},{"location":"Notes/taylor/#taylor-approximation","text":"We will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned}","title":"Taylor Approximation"},{"location":"Notes/taylor/#linearized-predictive-mean-and-variance","text":"\\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.","title":"Linearized Predictive Mean and Variance"},{"location":"Notes/taylor/#conditional-gaussian-distributions","text":"","title":"Conditional Gaussian Distributions"},{"location":"Notes/taylor/#i-additive-noise-model-xfxf","text":"This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP} {-1}K_{*} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"I: Additive Noise Model (x,fx,f)"},{"location":"Notes/taylor/#other-gp-methods","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{ z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{ } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Other GP Methods"},{"location":"Notes/taylor/#ii-non-additive-noise-model","text":"","title":"II: Non-Additive Noise Model"},{"location":"Notes/taylor/#iii-quadratic-approximation","text":"","title":"III: Quadratic Approximation"},{"location":"Notes/taylor/#literature","text":"Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Literature"},{"location":"Notes/taylor/#supplementary","text":"","title":"Supplementary"},{"location":"Notes/taylor/#error-propagation","text":"To see more about error propagation and the relation to the mean and variance, see here .","title":"Error Propagation"},{"location":"Notes/taylor/#fubinis-theorem","text":"","title":"Fubini's Theorem"},{"location":"Notes/taylor/#law-of-iterated-expecations","text":"","title":"Law of Iterated Expecations"},{"location":"Notes/taylor/#conditional-variance","text":"","title":"Conditional Variance"},{"location":"Notes/uncertainty/","text":"What is Uncertainty? \u00b6","title":"What is Uncertainty?"},{"location":"Notes/uncertainty/#what-is-uncertainty","text":"","title":"What is Uncertainty?"},{"location":"Notes/vi/","text":"Uncertain Inputs GPs - Variational Strategies \u00b6 This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Posterior Approximations Variational GP Model with Latent Inputs Evidence Lower Bound (ELBO) Uncertain Inputs Case I - Strong Prior Case II - Regularized Strong Prior Case III - Prior with Openness Case IV - Bonus, Conservative Freedom Resources Important Papers Summary Thesis Talks Posterior Approximations \u00b6 What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. Variational GP Model with Latent Inputs \u00b6 Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) Evidence Lower Bound (ELBO) \u00b6 In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. Uncertain Inputs \u00b6 So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties. Case I - Strong Prior \u00b6 Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes. Case II - Regularized Strong Prior \u00b6 This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) Case III - Prior with Openness \u00b6 The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. Case IV - Bonus, Conservative Freedom \u00b6 Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options Resources \u00b6 Important Papers \u00b6 These are the important papers that helped me understand what was going on throughout the learning process. Summary Thesis \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Talks \u00b6 Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Uncertain Inputs GPs - Variational Strategies"},{"location":"Notes/vi/#uncertain-inputs-gps-variational-strategies","text":"This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Posterior Approximations Variational GP Model with Latent Inputs Evidence Lower Bound (ELBO) Uncertain Inputs Case I - Strong Prior Case II - Regularized Strong Prior Case III - Prior with Openness Case IV - Bonus, Conservative Freedom Resources Important Papers Summary Thesis Talks","title":"Uncertain Inputs GPs - Variational Strategies"},{"location":"Notes/vi/#posterior-approximations","text":"What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more.","title":"Posterior Approximations"},{"location":"Notes/vi/#variational-gp-model-with-latent-inputs","text":"Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right)","title":"Variational GP Model with Latent Inputs"},{"location":"Notes/vi/#evidence-lower-bound-elbo","text":"In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.","title":"Evidence Lower Bound (ELBO)"},{"location":"Notes/vi/#uncertain-inputs","text":"So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.","title":"Uncertain Inputs"},{"location":"Notes/vi/#case-i-strong-prior","text":"Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.","title":"Case I - Strong Prior"},{"location":"Notes/vi/#case-ii-regularized-strong-prior","text":"This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right)","title":"Case II - Regularized Strong Prior"},{"location":"Notes/vi/#case-iii-prior-with-openness","text":"The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise.","title":"Case III - Prior with Openness"},{"location":"Notes/vi/#case-iv-bonus-conservative-freedom","text":"Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options","title":"Case IV - Bonus, Conservative Freedom"},{"location":"Notes/vi/#resources","text":"","title":"Resources"},{"location":"Notes/vi/#important-papers","text":"These are the important papers that helped me understand what was going on throughout the learning process.","title":"Important Papers"},{"location":"Notes/vi/#summary-thesis","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Summary Thesis"},{"location":"Notes/vi/#talks","text":"Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Talks"},{"location":"Taylor/2.0_linearized_gp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linearized GP \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 73.05it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34] Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () 1 st Order Taylor Expansion \u00b6 # =========================== # 1st Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Predictive Mean mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) pred_grad_f = jax . jit ( jax . vmap ( jax . grad ( mean_f , argnums = ( 0 )), in_axes = ( 0 ))) dmu_y = pred_grad_f ( Xtest ) # Predictive Variance var_correction_o1 = jnp . diag ( jnp . dot ( jnp . dot ( dmu_y , input_cov ), dmu_y . T )) # Uncertainty mu_y = mu_y_o1 uncertainty_t1 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt . show () 2 nd Order Taylor Expansion \u00b6 # =========================== # 2nd Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Mean function correction mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_df2 = jax . jit ( jax . vmap ( jax . hessian ( mean_f , argnums = ( 0 )), in_axes = ( 0 )) ) d2mu_y = mu_df2 ( Xtest ) d2mu_y = jnp . dot ( d2mu_y , input_cov ) mu_y_o2 = 0.5 * jnp . trace ( d2mu_y , axis1 = 1 , axis2 = 2 ) mu_y = mu_y_o1 + mu_y_o2 # Variance Function Correction var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_f = functools . partial ( predictive_variance , params , gp_priors , X , y ) pred_var_f = jax . jit ( jax . vmap ( jax . hessian ( var_f , argnums = ( 0 )), in_axes = ( 0 , None , None )) ) d2var_y2 = pred_var_f ( Xtest , True , False ) d2var_y2 = jnp . dot ( d2var_y2 , input_cov ) var_correction_o2 = jnp . trace ( d2var_y2 , axis1 = 1 , axis2 = 2 ) # Uncertainty uncertainty_t2 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze () + var_correction_o2 . squeeze () ) plt . plot ( mu_y_o1 + mu_y_o2 ) plt . plot ( mu_y_o1 ) plt . show () fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_2o.png\") plt . show () Results \u00b6 Standard GP Error Bars \u00b6 Taylor Expansion (1 st Order) \u00b6 Taylor Expansion (2 nd Order) \u00b6 Difference in 1 st and 2 nd Order \u00b6 fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), # label=r\"Predictive Mean\", color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"blue\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.4 , color = \"yellow\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_diff.png\") plt . show ()","title":"2.0 linearized gp"},{"location":"Taylor/2.0_linearized_gp/#linearized-gp","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Linearized GP"},{"location":"Taylor/2.0_linearized_gp/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2","title":"Imports"},{"location":"Taylor/2.0_linearized_gp/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Data"},{"location":"Taylor/2.0_linearized_gp/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"Taylor/2.0_linearized_gp/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"Taylor/2.0_linearized_gp/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 73.05it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34]","title":"Training"},{"location":"Taylor/2.0_linearized_gp/#predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Predictions"},{"location":"Taylor/2.0_linearized_gp/#1st-order-taylor-expansion","text":"# =========================== # 1st Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Predictive Mean mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) pred_grad_f = jax . jit ( jax . vmap ( jax . grad ( mean_f , argnums = ( 0 )), in_axes = ( 0 ))) dmu_y = pred_grad_f ( Xtest ) # Predictive Variance var_correction_o1 = jnp . diag ( jnp . dot ( jnp . dot ( dmu_y , input_cov ), dmu_y . T )) # Uncertainty mu_y = mu_y_o1 uncertainty_t1 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt . show ()","title":"1st Order Taylor Expansion"},{"location":"Taylor/2.0_linearized_gp/#2nd-order-taylor-expansion","text":"# =========================== # 2nd Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Mean function correction mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_df2 = jax . jit ( jax . vmap ( jax . hessian ( mean_f , argnums = ( 0 )), in_axes = ( 0 )) ) d2mu_y = mu_df2 ( Xtest ) d2mu_y = jnp . dot ( d2mu_y , input_cov ) mu_y_o2 = 0.5 * jnp . trace ( d2mu_y , axis1 = 1 , axis2 = 2 ) mu_y = mu_y_o1 + mu_y_o2 # Variance Function Correction var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_f = functools . partial ( predictive_variance , params , gp_priors , X , y ) pred_var_f = jax . jit ( jax . vmap ( jax . hessian ( var_f , argnums = ( 0 )), in_axes = ( 0 , None , None )) ) d2var_y2 = pred_var_f ( Xtest , True , False ) d2var_y2 = jnp . dot ( d2var_y2 , input_cov ) var_correction_o2 = jnp . trace ( d2var_y2 , axis1 = 1 , axis2 = 2 ) # Uncertainty uncertainty_t2 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze () + var_correction_o2 . squeeze () ) plt . plot ( mu_y_o1 + mu_y_o2 ) plt . plot ( mu_y_o1 ) plt . show () fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_2o.png\") plt . show ()","title":"2nd Order Taylor Expansion"},{"location":"Taylor/2.0_linearized_gp/#results","text":"","title":"Results"},{"location":"Taylor/2.0_linearized_gp/#standard-gp-error-bars","text":"","title":"Standard GP Error Bars"},{"location":"Taylor/2.0_linearized_gp/#taylor-expansion-1st-order","text":"","title":"Taylor Expansion (1st Order)"},{"location":"Taylor/2.0_linearized_gp/#taylor-expansion-2nd-order","text":"","title":"Taylor Expansion (2nd Order)"},{"location":"Taylor/2.0_linearized_gp/#difference-in-1st-and-2nd-order","text":"fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), # label=r\"Predictive Mean\", color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"blue\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.4 , color = \"yellow\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_diff.png\") plt . show ()","title":"Difference in 1st and 2nd Order"},{"location":"Taylor/error_propagation/","text":"Error Propagation \u00b6 Taylor Series Expansion \u00b6 A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) Law of Error Propagation \u00b6 This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} Proof: Mean Function \u00b6 Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} Proof: Variance Function \u00b6 Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. Resources \u00b6 Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Error Propagation"},{"location":"Taylor/error_propagation/#error-propagation","text":"","title":"Error Propagation"},{"location":"Taylor/error_propagation/#taylor-series-expansion","text":"A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right)","title":"Taylor Series Expansion"},{"location":"Taylor/error_propagation/#law-of-error-propagation","text":"This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top}","title":"Law of Error Propagation"},{"location":"Taylor/error_propagation/#proof-mean-function","text":"Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned}","title":"Proof: Mean Function"},{"location":"Taylor/error_propagation/#proof-variance-function","text":"Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself.","title":"Proof: Variance Function"},{"location":"Taylor/error_propagation/#resources","text":"Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Resources"},{"location":"Taylor/scratch/","text":"","title":"Scratch"},{"location":"Taylor/taylor/","text":"Linearized GP \u00b6 Recall the GP formulation: y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) Recall the posterior formulas: \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} In the case where we have uncertain inputs \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) , this function needs to be modified in order to accommodate the uncertainty The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the posterior distribution wrt the inputs. Mean Function \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} Taylor Approximation \u00b6 Taking complete expectations can be very expensive because we need to take the expectation wrt to the inputs through nonlinear terms such as the kernel functions and their inverses. So, we will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Note To see more about error propagation and the relation to the mean and variance, see here . So expanding these equations gives us the following: \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term. Examples \u00b6 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation. Sparse GPs \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} So the new predictive functions will be: \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. Literature \u00b6 Theory Bayesian Filtering and Smoothing - Smio Sarkka ()- Book Modelling and Control of Dynamic Systems Using GP Models - Jus Kocijan () - Book Applied to Gaussian Processes Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Linearized GP"},{"location":"Taylor/taylor/#linearized-gp","text":"Recall the GP formulation: y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) Recall the posterior formulas: \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} In the case where we have uncertain inputs \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) , this function needs to be modified in order to accommodate the uncertainty The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the posterior distribution wrt the inputs. Mean Function \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned}","title":"Linearized GP"},{"location":"Taylor/taylor/#taylor-approximation","text":"Taking complete expectations can be very expensive because we need to take the expectation wrt to the inputs through nonlinear terms such as the kernel functions and their inverses. So, we will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Note To see more about error propagation and the relation to the mean and variance, see here . So expanding these equations gives us the following: \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.","title":"Taylor Approximation"},{"location":"Taylor/taylor/#examples","text":"1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation.","title":"Examples"},{"location":"Taylor/taylor/#sparse-gps","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} So the new predictive functions will be: \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Sparse GPs"},{"location":"Taylor/taylor/#literature","text":"Theory Bayesian Filtering and Smoothing - Smio Sarkka ()- Book Modelling and Control of Dynamic Systems Using GP Models - Jus Kocijan () - Book Applied to Gaussian Processes Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Literature"},{"location":"notebooks/1.0_gp_basics/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Gaussian Process Regression \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) Data \u00b6 def get_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 1 , 1 , N ) Y = X + 0.2 * jnp . power ( X , 3.0 ) + 0.5 * jnp . power ( 0.5 + X , 2.0 ) * jnp . sin ( 4.0 * X ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = jnp . linspace ( - 1.2 , 1.2 , N_test ) return X [:, None ], Y [:, None ], X_test [:, None ], None logger . setLevel ( logging . INFO ) , y , Xtest , ytest = get_data () print ( X . shape , y . shape ) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . show () (30, 1) (30, 1) Gaussian Process \u00b6 Model: GP Prior \u00b6 Parameters : X, Y, $\\theta= $ (Likelihood Parameters, Kernel Parameters) Compute the Kernel Matrix Compute the Mean function Sample from the Multivariate Normal Distribution Kernel Function \u00b6 k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) <span><span class=\"MathJax_Preview\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right)</span><script type=\"math/tex\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) # ARD Kernel @jax . jit def ard_kernel ( params , x , y ): # divide by the length scale x = x / params [ 'length_scale' ] y = y / params [ 'length_scale' ] # return the ard kernel return params [ 'var_f' ] * jnp . exp ( - sqeuclidean_distance ( x , y ) ) params = { 'var_f' : 1.0 , 'sigma' : 1.0 } Kernel Matrix \u00b6 # Gram Matrix def gram ( func , params , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ))( y ))( x ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] # test_X = x_plot[0, :] cov_f = functools . partial ( gram , rbf_kernel ) K_ = cov_f ( params , Xtest , X ) print ( K_ . shape ) K_ = cov_f ( params , X , Xtest ) print ( K_ . shape ) (400, 30) (30, 400) Mean Function \u00b6 Honestly, I never work with mean functions. I always assume a zero-mean function and that's it. I don't really know anyone who works with mean functions either. I've seen it used in deep Gaussian processes but I have no expertise in which mean functions to use. So, we'll follow the community standard for now: zero mean function def zero_mean ( x ): return jnp . zeros ( x . shape [ 0 ]) 3. Compute Model \u00b6 Now we have all of the components to make our GP prior function. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) # define mean function mu_f = zero_mean # define covariance function params = { 'gamma' : 1.0 , 'var_f' : 1.0 } cov_f = functools . partial ( gram , rbf_kernel ) mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = X [ 0 , :]) Checks \u00b6 So I'm still getting used to the vmap . So in theory, this function should work for a vector \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} and for a batch of samples X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} # checks - 1 vector (D) test_X = X [ 0 , :] . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) print ( mu_x . shape , cov_x . shape ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 # checks - 1 vector with batch size (NxD) test_X = X . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (1,) (1, 1) Woot! Success! So now we can technically sample from this GP prior distribution. 4. Sampling from GP Prior \u00b6 from scipy.stats import multivariate_normal as scio_mvn Scipy \u00b6 # checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # check outputs assert mu_x . shape == ( n_samples ,) assert cov_x . shape == ( n_samples , n_samples ) # draw random samples from distribution n_functions = 10 y_samples = stats . multivariate_normal . rvs ( mean = mu_x , cov = cov_x , size = n_functions ) assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-182-7c0d1026349a> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # check outputs <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' Note - The positive semi-definite error \u00b6 I believe that's due to the diagonals being off. Normally we add something called jitter. This allows the matrix to be positive semi-definite. mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ]) # draw random samples from distribution n_functions = 10 y_samples = scio_mvn . rvs ( mean = mu_x , cov = cov_x_ , size = n_functions ) print ( y_samples . shape ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-183-12ab200a8e55> in <module> ----> 1 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 2 3 # make it semi-positive definite with jitter 4 jitter = 1e-6 5 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ] ) <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' And now we don't have that message. This is a small thing but it's super important and can lead to errors in the optimization if not addressed. Jax \u00b6 # checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 key = jax . random . PRNGKey ( 0 ) y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) # check assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-184-9b06421fbf35> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # make it semi-positive definite with jitter <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' 4. Posterior \u00b6 Conditioned on the observations, can we make predictions. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def cholesky_factorization ( K , Y ): # cho factor the cholesky logger . debug ( f \"ChoFactor: K { K . shape } \" ) L = jax . scipy . linalg . cho_factor ( K , lower = True ) logger . debug ( f \"Output, L: { L [ 0 ] . shape } , { L [ 1 ] } \" ) # weights logger . debug ( f \"Input, ChoSolve(L, Y): { L [ 0 ] . shape , Y . shape } \" ) weights = jax . scipy . linalg . cho_solve ( L , Y ) logger . debug ( f \"Output, alpha: { weights . shape } \" ) return L , weights jitter = 1e-6 def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] test_X = Xtest [ 0 , :] prior_funcs = ( mu_f , cov_f ) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , X_new = test_X ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (1,) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (1,),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (1, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (1,), KxX @ alpha: (1, 30) @ (30, 1) DEBUG:root:Output, mu_y: (1, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 1) DEBUG:root:Output, v: (30, 1) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[1])). DEBUG:root:Output, Kxx: (1, 1) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (1, 1), v:(30, 1) DEBUG:root:Output: cov(x*, x*) - (1, 1) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 1) DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling _where for args (ShapedArray(bool[1,1]), ShapedArray(float32[1,1]), ShapedArray(float32[1,1])). DEBUG:root:Output, diag(Kxx): (1,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (1, 30), (30, 30), (1, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[1,30]), ShapedArray(float32[1,30])). DEBUG:root:Output, var_y: (1,), 0.03,0.03 (1, 1) (1, 1) (1,) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,400])). DEBUG:root:Output, v: (30, 400) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling _where for args (ShapedArray(bool[400,400]), ShapedArray(float32[400,400]), ShapedArray(float32[400,400])). DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.00,0.03 (400, 1) (400, 400) (400,) plt . plot ( var_y . squeeze ()) [<matplotlib.lines.Line2D at 0x7f06943d8a30>] test_X . shape , mu_y . shape ((1,), (400, 1)) uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) [<matplotlib.lines.Line2D at 0x7f06741adfd0>] 5. Loss - Log-Likelihood \u00b6 From Scratch \u00b6 # @jax.jit def cholesky_factorization ( K , Y ): # cho factor the cholesky L = jax . scipy . linalg . cho_factor ( K ) # weights weights = jax . scipy . linalg . cho_solve ( L , Y ) return L , weights def nll_scratch ( gp_priors , params , X , Y ) -> float : ( mu_func , cov_func ) = gp_priors # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # y_mean = jnp.mean(Y, axis=1) # Y -= y_mean # print(mu_x.shape, Kxx.shape) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== # print(f\"Problem:\", X.shape, Y.shape, Kxx.shape) # print(f\"Y: {Y.shape}, Kxx: {Kxx.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ 'likelihood_noise' ] + 1e-5 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) # L = jax.scipy.linalg.cholesky(Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]), lower=True) # alpha = jax.scipy.linalg.solve_triangular(L.T, jax.scipy.linalg.solve_triangular(L, y, lower=True)) # print(f\"Y: {Y.shape}, alpha: {alpha.shape}\") logging . debug ( f \"Y: { Y . shape } ,alpha: { alpha . shape } \" ) log_likelihood = - 0.5 * jnp . einsum ( \"ik,ik->k\" , Y , alpha ) #* jnp.dot(Y.T, alpha) # log_likelihood -= jnp . sum ( jnp . log ( jnp . diag ( L ))) log_likelihood -= ( Kxx . shape [ 0 ] / 2 ) * jnp . log ( 2 * jnp . pi ) # log_likelihood -= jnp.sum(-0.5 * np.log(2 * 3.1415) - params['var_f']**2) return - jnp . sum ( log_likelihood ) # # print(L.shape, alpha.shape) # # cho factor the cholesky # K_gp = Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]) # # L = jax.scipy.linalg.cholesky(K_gp) # # assert np.testing.assert_array_almost_equal(K_gp, L @ L.T), # return jax.scipy.stats.multivariate_normal.logpdf(Y, mean=mu_x, cov=K_gp) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = nll_scratch ( prior_funcs , params , X , y ) print ( nll ) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Y: (30, 1),alpha:(30, 1) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _where for args (ShapedArray(bool[30,30]), ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). 54.878708 Auto-Batching with VMAP \u00b6 nll_scratch_vec = jax . vmap ( nll_scratch , in_axes = ( None , None , 0 , 0 )) nll = nll_scratch_vec ( params , prior_funcs , X , y [:, None ]) print ( nll . sum ()) (1,) (1, 1) Y: (1,), alpha: (1,) -209.98637 Refactor - Built-in Function \u00b6 It turns out that the jax library already has the logpdf for the multivariate_normal already implemented. So we can just use that. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def marginal_likelihood ( prior_params , params , Xtrain , Ytrain ): # unpack params ( mu_func , cov_func ) = prior_params # ========================== # 1. GP Prior # ========================== mu_x = mu_f ( Xtrain ) logging . debug ( f \"mu: { mu_x . shape } \" ) Kxx = cov_f ( params , Xtrain , Xtrain ) logging . debug ( f \"Kxx: { Kxx . shape } \" ) # print(\"MLL (GPPR):\", Xtrain.shape, Ytrain.shape) # mu_x, Kxx = gp_prior(params, mu_f=mu_func, cov_f=cov_func , x=Xtrain) # =========================== # 2. GP Likelihood # =========================== K_gp = Kxx + ( params [ 'likelihood_noise' ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]) logging . debug ( f \"K_gp: { K_gp . shape } \" ) # print(\"MLL (GPLL):\", Xtrain.shape, Ytrain.shape) # =========================== # 3. Built-in GP Likelihood # =========================== logging . debug ( f \"Input: { Ytrain . squeeze () . shape } , mu: { mu_x . shape } , K: { K_gp . shape } \" ) log_prob = jax . scipy . stats . multivariate_normal . logpdf ( Ytrain . squeeze (), mean = jnp . zeros ( Ytrain . shape [ 0 ]), cov = K_gp ) logging . debug ( f \"LogProb: { log_prob . shape } \" ) nll = jnp . sum ( log_prob ) return - nll logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = marginal_likelihood ( prior_funcs , params , X , y ) print ( nll ) DEBUG:root:mu: (30,) DEBUG:root:Kxx: (30, 30) DEBUG:root:K_gp: (30, 30) DEBUG:root:Input: (30,), mu: (30,), K: (30, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30]), ShapedArray(float32[30])). DEBUG:root:LogProb: () 54.937344 logger . setLevel ( logging . INFO ) % timeit _ = nll_scratch ( prior_funcs , params , X , y ) % timeit _ = marginal_likelihood ( prior_funcs , params , X , y ) 18.3 ms \u00b1 2.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 26 ms \u00b1 906 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 6. Training \u00b6 def softplus ( x ): return np . logaddexp ( x , 0. ) logger . setLevel ( logging . INFO ) X , y , Xtest , ytest = get_data ( 30 ) params = { 'gamma' : 10. , # 'length_scale': 1.0, # 'var_f': 1.0, 'likelihood_noise' : 1e-3 , } # Nice Trick for better training of params def saturate ( params ): return { ikey : softplus ( ivalue ) for ( ikey , ivalue ) in params . items ()} params = saturate ( params ) cov_f = functools . partial ( gram , rbf_kernel ) gp_priors = ( mu_f , cov_f ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( nll_scratch , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) # MEAN FUNCTION mu_f = zero_mean # l_val = mll_loss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) l_vals = mll_loss ( saturate ( params ), X , y ) # print('MLL (vector):', l_val) # print('MLL (samples):', l_vals) # dl_val = dloss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) dl_vals = dloss ( saturate ( params ), X , y ) # print('dMLL (vector):', dl_val)| # print('dMLL (samples):', dl_vals) # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # print(\"BEOFRE!\") # print(X.shape, y.shape) # print(\"PARAMS\", params) # print(opt_state) # value and gradient of loss function loss = mll_loss ( params , X , y ) grads = dloss ( params , X , y ) # # print(f\"VALUE:\", value) # print(\"During! v\", value) # print(\"During! p\", params) # print(\"During! g\", grads) # update parameter state opt_state = opt_update ( 0 , grads , opt_state ) # get new params params = get_params ( opt_state ) # print(\"AFTER! v\", value) # print(\"AFTER! p\", params) # print(\"AFTER! g\", grads) return params , opt_state , loss # initialize optimizer opt_init , opt_update , get_params = optimizers . adam ( step_size = 1e-2 ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) # print(\"PARAMS!\", params) n_epochs = 2_000 learning_rate = 0.01 losses = list () import tqdm with tqdm . trange ( n_epochs ) as bar : for i in bar : postfix = {} # params = saturate(params) # get nll and grads # nll, grads = dloss(params, X, y) params , opt_state , value = step ( params , X , y , opt_state ) # update params # params, momentums, scales, nll = train_step(params, momentums, scales, X, y) for ikey in params . keys (): postfix [ ikey ] = f \" { params [ ikey ] : .2f } \" # params[ikey] += learning_rate * grads[ikey].mean() losses . append ( value . mean ()) postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) params = saturate ( params ) # params = log_params(params) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:05<00:00, 335.90it/s, gamma=1.58, likelihood_noise=-2.76, Loss=10.66] plt . plot ( losses ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977} 7. Predictions \u00b6 def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) logging . debug ( f \"Output, Kxx: { Kxx . shape } , { Kxx . min () } , { Kxx . max () } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") L = jax . scipy . linalg . cholesky ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-7 ) * jnp . eye ( Kxx . shape [ 0 ]), lower = True ) logging . debug ( f \"Output, L: { L . shape } , { L . min () } , { L . max () } \" ) alpha = jax . scipy . linalg . solve_triangular ( L . T , jax . scipy . linalg . solve_triangular ( L , Y , lower = True ) ) # (L, lower), alpha = cholesky_factorization( # , Y # ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } , { alpha . min () } , { alpha . max () } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } , { v . min () : .2f } , { v . max () : .2f } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977} # print(X.shape, y.shape, test_X.shape) logger . setLevel ( logging . DEBUG ) # x_plot = jnp.linspace(X.min(), X.max(), 1_000)[:, None] print ( X . shape , y . shape , Xtest . shape ) mu_y , cov_y , var_y = posterior ( params , gp_priors , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) # onp.testing.assert_array_almost_equal(jncov_y, var_y) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (30, 1) (30, 1) (400, 1) DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Output, Kxx: (30, 30), 0.000838853360619396, 1.0 DEBUG:root:Solving Cholesky Factorization... DEBUG:root:Output, L: (30, 30),0.0,1.0303192138671875 DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, L: (30, 30), alpha: (30, 1),-8.470149993896484,10.187272071838379 DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:root:Output, v: (30, 400), -0.17,0.75 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -2.54,3.05 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -5.07,13.96 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.01,0.12 (400, 1) (400, 400) (400,) print ( var_y . min (), var_y . max (), cov_y . min (), cov_y . max ()) plt . plot ( var_y . squeeze ()) 0.07021278 0.18080568 0.05092573 0.18080568 [<matplotlib.lines.Line2D at 0x7f065c01e5b0>] uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . show ()","title":"1.0 gp basics"},{"location":"notebooks/1.0_gp_basics/#gaussian-process-regression","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Gaussian Process Regression"},{"location":"notebooks/1.0_gp_basics/#imports","text":"import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ])","title":"Imports"},{"location":"notebooks/1.0_gp_basics/#data","text":"def get_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 1 , 1 , N ) Y = X + 0.2 * jnp . power ( X , 3.0 ) + 0.5 * jnp . power ( 0.5 + X , 2.0 ) * jnp . sin ( 4.0 * X ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = jnp . linspace ( - 1.2 , 1.2 , N_test ) return X [:, None ], Y [:, None ], X_test [:, None ], None logger . setLevel ( logging . INFO ) , y , Xtest , ytest = get_data () print ( X . shape , y . shape ) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . show () (30, 1) (30, 1)","title":"Data"},{"location":"notebooks/1.0_gp_basics/#gaussian-process","text":"","title":"Gaussian Process"},{"location":"notebooks/1.0_gp_basics/#model-gp-prior","text":"Parameters : X, Y, $\\theta= $ (Likelihood Parameters, Kernel Parameters) Compute the Kernel Matrix Compute the Mean function Sample from the Multivariate Normal Distribution","title":"Model: GP Prior"},{"location":"notebooks/1.0_gp_basics/#kernel-function","text":"k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) <span><span class=\"MathJax_Preview\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right)</span><script type=\"math/tex\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) # ARD Kernel @jax . jit def ard_kernel ( params , x , y ): # divide by the length scale x = x / params [ 'length_scale' ] y = y / params [ 'length_scale' ] # return the ard kernel return params [ 'var_f' ] * jnp . exp ( - sqeuclidean_distance ( x , y ) ) params = { 'var_f' : 1.0 , 'sigma' : 1.0 }","title":"Kernel Function"},{"location":"notebooks/1.0_gp_basics/#kernel-matrix","text":"# Gram Matrix def gram ( func , params , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ))( y ))( x ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] # test_X = x_plot[0, :] cov_f = functools . partial ( gram , rbf_kernel ) K_ = cov_f ( params , Xtest , X ) print ( K_ . shape ) K_ = cov_f ( params , X , Xtest ) print ( K_ . shape ) (400, 30) (30, 400)","title":"Kernel Matrix"},{"location":"notebooks/1.0_gp_basics/#mean-function","text":"Honestly, I never work with mean functions. I always assume a zero-mean function and that's it. I don't really know anyone who works with mean functions either. I've seen it used in deep Gaussian processes but I have no expertise in which mean functions to use. So, we'll follow the community standard for now: zero mean function def zero_mean ( x ): return jnp . zeros ( x . shape [ 0 ])","title":"Mean Function"},{"location":"notebooks/1.0_gp_basics/#3-compute-model","text":"Now we have all of the components to make our GP prior function. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) # define mean function mu_f = zero_mean # define covariance function params = { 'gamma' : 1.0 , 'var_f' : 1.0 } cov_f = functools . partial ( gram , rbf_kernel ) mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = X [ 0 , :])","title":"3. Compute Model"},{"location":"notebooks/1.0_gp_basics/#checks","text":"So I'm still getting used to the vmap . So in theory, this function should work for a vector \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} and for a batch of samples X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} # checks - 1 vector (D) test_X = X [ 0 , :] . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) print ( mu_x . shape , cov_x . shape ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 # checks - 1 vector with batch size (NxD) test_X = X . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (1,) (1, 1) Woot! Success! So now we can technically sample from this GP prior distribution.","title":"Checks"},{"location":"notebooks/1.0_gp_basics/#4-sampling-from-gp-prior","text":"from scipy.stats import multivariate_normal as scio_mvn","title":"4. Sampling from GP Prior"},{"location":"notebooks/1.0_gp_basics/#scipy","text":"# checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # check outputs assert mu_x . shape == ( n_samples ,) assert cov_x . shape == ( n_samples , n_samples ) # draw random samples from distribution n_functions = 10 y_samples = stats . multivariate_normal . rvs ( mean = mu_x , cov = cov_x , size = n_functions ) assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-182-7c0d1026349a> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # check outputs <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma'","title":"Scipy"},{"location":"notebooks/1.0_gp_basics/#note-the-positive-semi-definite-error","text":"I believe that's due to the diagonals being off. Normally we add something called jitter. This allows the matrix to be positive semi-definite. mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ]) # draw random samples from distribution n_functions = 10 y_samples = scio_mvn . rvs ( mean = mu_x , cov = cov_x_ , size = n_functions ) print ( y_samples . shape ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-183-12ab200a8e55> in <module> ----> 1 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 2 3 # make it semi-positive definite with jitter 4 jitter = 1e-6 5 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ] ) <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' And now we don't have that message. This is a small thing but it's super important and can lead to errors in the optimization if not addressed.","title":"Note - The positive semi-definite error"},{"location":"notebooks/1.0_gp_basics/#jax","text":"# checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 key = jax . random . PRNGKey ( 0 ) y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) # check assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-184-9b06421fbf35> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # make it semi-positive definite with jitter <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma'","title":"Jax"},{"location":"notebooks/1.0_gp_basics/#4-posterior","text":"Conditioned on the observations, can we make predictions. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def cholesky_factorization ( K , Y ): # cho factor the cholesky logger . debug ( f \"ChoFactor: K { K . shape } \" ) L = jax . scipy . linalg . cho_factor ( K , lower = True ) logger . debug ( f \"Output, L: { L [ 0 ] . shape } , { L [ 1 ] } \" ) # weights logger . debug ( f \"Input, ChoSolve(L, Y): { L [ 0 ] . shape , Y . shape } \" ) weights = jax . scipy . linalg . cho_solve ( L , Y ) logger . debug ( f \"Output, alpha: { weights . shape } \" ) return L , weights jitter = 1e-6 def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] test_X = Xtest [ 0 , :] prior_funcs = ( mu_f , cov_f ) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , X_new = test_X ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (1,) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (1,),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (1, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (1,), KxX @ alpha: (1, 30) @ (30, 1) DEBUG:root:Output, mu_y: (1, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 1) DEBUG:root:Output, v: (30, 1) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[1])). DEBUG:root:Output, Kxx: (1, 1) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (1, 1), v:(30, 1) DEBUG:root:Output: cov(x*, x*) - (1, 1) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 1) DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling _where for args (ShapedArray(bool[1,1]), ShapedArray(float32[1,1]), ShapedArray(float32[1,1])). DEBUG:root:Output, diag(Kxx): (1,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (1, 30), (30, 30), (1, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[1,30]), ShapedArray(float32[1,30])). DEBUG:root:Output, var_y: (1,), 0.03,0.03 (1, 1) (1, 1) (1,) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,400])). DEBUG:root:Output, v: (30, 400) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling _where for args (ShapedArray(bool[400,400]), ShapedArray(float32[400,400]), ShapedArray(float32[400,400])). DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.00,0.03 (400, 1) (400, 400) (400,) plt . plot ( var_y . squeeze ()) [<matplotlib.lines.Line2D at 0x7f06943d8a30>] test_X . shape , mu_y . shape ((1,), (400, 1)) uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) [<matplotlib.lines.Line2D at 0x7f06741adfd0>]","title":"4. Posterior"},{"location":"notebooks/1.0_gp_basics/#5-loss-log-likelihood","text":"","title":"5. Loss - Log-Likelihood"},{"location":"notebooks/1.0_gp_basics/#from-scratch","text":"# @jax.jit def cholesky_factorization ( K , Y ): # cho factor the cholesky L = jax . scipy . linalg . cho_factor ( K ) # weights weights = jax . scipy . linalg . cho_solve ( L , Y ) return L , weights def nll_scratch ( gp_priors , params , X , Y ) -> float : ( mu_func , cov_func ) = gp_priors # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # y_mean = jnp.mean(Y, axis=1) # Y -= y_mean # print(mu_x.shape, Kxx.shape) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== # print(f\"Problem:\", X.shape, Y.shape, Kxx.shape) # print(f\"Y: {Y.shape}, Kxx: {Kxx.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ 'likelihood_noise' ] + 1e-5 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) # L = jax.scipy.linalg.cholesky(Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]), lower=True) # alpha = jax.scipy.linalg.solve_triangular(L.T, jax.scipy.linalg.solve_triangular(L, y, lower=True)) # print(f\"Y: {Y.shape}, alpha: {alpha.shape}\") logging . debug ( f \"Y: { Y . shape } ,alpha: { alpha . shape } \" ) log_likelihood = - 0.5 * jnp . einsum ( \"ik,ik->k\" , Y , alpha ) #* jnp.dot(Y.T, alpha) # log_likelihood -= jnp . sum ( jnp . log ( jnp . diag ( L ))) log_likelihood -= ( Kxx . shape [ 0 ] / 2 ) * jnp . log ( 2 * jnp . pi ) # log_likelihood -= jnp.sum(-0.5 * np.log(2 * 3.1415) - params['var_f']**2) return - jnp . sum ( log_likelihood ) # # print(L.shape, alpha.shape) # # cho factor the cholesky # K_gp = Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]) # # L = jax.scipy.linalg.cholesky(K_gp) # # assert np.testing.assert_array_almost_equal(K_gp, L @ L.T), # return jax.scipy.stats.multivariate_normal.logpdf(Y, mean=mu_x, cov=K_gp) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = nll_scratch ( prior_funcs , params , X , y ) print ( nll ) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Y: (30, 1),alpha:(30, 1) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _where for args (ShapedArray(bool[30,30]), ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). 54.878708","title":"From Scratch"},{"location":"notebooks/1.0_gp_basics/#auto-batching-with-vmap","text":"nll_scratch_vec = jax . vmap ( nll_scratch , in_axes = ( None , None , 0 , 0 )) nll = nll_scratch_vec ( params , prior_funcs , X , y [:, None ]) print ( nll . sum ()) (1,) (1, 1) Y: (1,), alpha: (1,) -209.98637","title":"Auto-Batching with VMAP"},{"location":"notebooks/1.0_gp_basics/#refactor-built-in-function","text":"It turns out that the jax library already has the logpdf for the multivariate_normal already implemented. So we can just use that. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def marginal_likelihood ( prior_params , params , Xtrain , Ytrain ): # unpack params ( mu_func , cov_func ) = prior_params # ========================== # 1. GP Prior # ========================== mu_x = mu_f ( Xtrain ) logging . debug ( f \"mu: { mu_x . shape } \" ) Kxx = cov_f ( params , Xtrain , Xtrain ) logging . debug ( f \"Kxx: { Kxx . shape } \" ) # print(\"MLL (GPPR):\", Xtrain.shape, Ytrain.shape) # mu_x, Kxx = gp_prior(params, mu_f=mu_func, cov_f=cov_func , x=Xtrain) # =========================== # 2. GP Likelihood # =========================== K_gp = Kxx + ( params [ 'likelihood_noise' ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]) logging . debug ( f \"K_gp: { K_gp . shape } \" ) # print(\"MLL (GPLL):\", Xtrain.shape, Ytrain.shape) # =========================== # 3. Built-in GP Likelihood # =========================== logging . debug ( f \"Input: { Ytrain . squeeze () . shape } , mu: { mu_x . shape } , K: { K_gp . shape } \" ) log_prob = jax . scipy . stats . multivariate_normal . logpdf ( Ytrain . squeeze (), mean = jnp . zeros ( Ytrain . shape [ 0 ]), cov = K_gp ) logging . debug ( f \"LogProb: { log_prob . shape } \" ) nll = jnp . sum ( log_prob ) return - nll logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = marginal_likelihood ( prior_funcs , params , X , y ) print ( nll ) DEBUG:root:mu: (30,) DEBUG:root:Kxx: (30, 30) DEBUG:root:K_gp: (30, 30) DEBUG:root:Input: (30,), mu: (30,), K: (30, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30]), ShapedArray(float32[30])). DEBUG:root:LogProb: () 54.937344 logger . setLevel ( logging . INFO ) % timeit _ = nll_scratch ( prior_funcs , params , X , y ) % timeit _ = marginal_likelihood ( prior_funcs , params , X , y ) 18.3 ms \u00b1 2.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 26 ms \u00b1 906 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)","title":"Refactor - Built-in Function"},{"location":"notebooks/1.0_gp_basics/#6-training","text":"def softplus ( x ): return np . logaddexp ( x , 0. ) logger . setLevel ( logging . INFO ) X , y , Xtest , ytest = get_data ( 30 ) params = { 'gamma' : 10. , # 'length_scale': 1.0, # 'var_f': 1.0, 'likelihood_noise' : 1e-3 , } # Nice Trick for better training of params def saturate ( params ): return { ikey : softplus ( ivalue ) for ( ikey , ivalue ) in params . items ()} params = saturate ( params ) cov_f = functools . partial ( gram , rbf_kernel ) gp_priors = ( mu_f , cov_f ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( nll_scratch , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) # MEAN FUNCTION mu_f = zero_mean # l_val = mll_loss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) l_vals = mll_loss ( saturate ( params ), X , y ) # print('MLL (vector):', l_val) # print('MLL (samples):', l_vals) # dl_val = dloss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) dl_vals = dloss ( saturate ( params ), X , y ) # print('dMLL (vector):', dl_val)| # print('dMLL (samples):', dl_vals) # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # print(\"BEOFRE!\") # print(X.shape, y.shape) # print(\"PARAMS\", params) # print(opt_state) # value and gradient of loss function loss = mll_loss ( params , X , y ) grads = dloss ( params , X , y ) # # print(f\"VALUE:\", value) # print(\"During! v\", value) # print(\"During! p\", params) # print(\"During! g\", grads) # update parameter state opt_state = opt_update ( 0 , grads , opt_state ) # get new params params = get_params ( opt_state ) # print(\"AFTER! v\", value) # print(\"AFTER! p\", params) # print(\"AFTER! g\", grads) return params , opt_state , loss # initialize optimizer opt_init , opt_update , get_params = optimizers . adam ( step_size = 1e-2 ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) # print(\"PARAMS!\", params) n_epochs = 2_000 learning_rate = 0.01 losses = list () import tqdm with tqdm . trange ( n_epochs ) as bar : for i in bar : postfix = {} # params = saturate(params) # get nll and grads # nll, grads = dloss(params, X, y) params , opt_state , value = step ( params , X , y , opt_state ) # update params # params, momentums, scales, nll = train_step(params, momentums, scales, X, y) for ikey in params . keys (): postfix [ ikey ] = f \" { params [ ikey ] : .2f } \" # params[ikey] += learning_rate * grads[ikey].mean() losses . append ( value . mean ()) postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) params = saturate ( params ) # params = log_params(params) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:05<00:00, 335.90it/s, gamma=1.58, likelihood_noise=-2.76, Loss=10.66] plt . plot ( losses ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977}","title":"6. Training"},{"location":"notebooks/1.0_gp_basics/#7-predictions","text":"def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) logging . debug ( f \"Output, Kxx: { Kxx . shape } , { Kxx . min () } , { Kxx . max () } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") L = jax . scipy . linalg . cholesky ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-7 ) * jnp . eye ( Kxx . shape [ 0 ]), lower = True ) logging . debug ( f \"Output, L: { L . shape } , { L . min () } , { L . max () } \" ) alpha = jax . scipy . linalg . solve_triangular ( L . T , jax . scipy . linalg . solve_triangular ( L , Y , lower = True ) ) # (L, lower), alpha = cholesky_factorization( # , Y # ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } , { alpha . min () } , { alpha . max () } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } , { v . min () : .2f } , { v . max () : .2f } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977} # print(X.shape, y.shape, test_X.shape) logger . setLevel ( logging . DEBUG ) # x_plot = jnp.linspace(X.min(), X.max(), 1_000)[:, None] print ( X . shape , y . shape , Xtest . shape ) mu_y , cov_y , var_y = posterior ( params , gp_priors , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) # onp.testing.assert_array_almost_equal(jncov_y, var_y) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (30, 1) (30, 1) (400, 1) DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Output, Kxx: (30, 30), 0.000838853360619396, 1.0 DEBUG:root:Solving Cholesky Factorization... DEBUG:root:Output, L: (30, 30),0.0,1.0303192138671875 DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, L: (30, 30), alpha: (30, 1),-8.470149993896484,10.187272071838379 DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:root:Output, v: (30, 400), -0.17,0.75 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -2.54,3.05 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -5.07,13.96 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.01,0.12 (400, 1) (400, 400) (400,) print ( var_y . min (), var_y . max (), cov_y . min (), cov_y . max ()) plt . plot ( var_y . squeeze ()) 0.07021278 0.18080568 0.05092573 0.18080568 [<matplotlib.lines.Line2D at 0x7f065c01e5b0>] uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . show ()","title":"7. Predictions"},{"location":"notebooks/1.1_gp_refactored/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Gaussian Process Regression \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:07<00:00, 71.09it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34] Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) Results \u00b6 fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () Jax \u00b6 # checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 key = jax . random . PRNGKey ( 0 ) y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) # check assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-184-9b06421fbf35> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # make it semi-positive definite with jitter <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma'","title":"1.1 gp refactored"},{"location":"notebooks/1.1_gp_refactored/#gaussian-process-regression","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Gaussian Process Regression"},{"location":"notebooks/1.1_gp_refactored/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ])","title":"Imports"},{"location":"notebooks/1.1_gp_refactored/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , )","title":"Data"},{"location":"notebooks/1.1_gp_refactored/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"notebooks/1.1_gp_refactored/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"notebooks/1.1_gp_refactored/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:07<00:00, 71.09it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34]","title":"Training"},{"location":"notebooks/1.1_gp_refactored/#predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ())","title":"Predictions"},{"location":"notebooks/1.1_gp_refactored/#results","text":"fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Results"},{"location":"notebooks/1.1_gp_refactored/#jax","text":"# checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 key = jax . random . PRNGKey ( 0 ) y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) # check assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-184-9b06421fbf35> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # make it semi-positive definite with jitter <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma'","title":"Jax"},{"location":"notebooks/2.0_linearized_gp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linearized GP \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 73.05it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34] Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () 1 st Order Taylor Expansion \u00b6 # =========================== # 1st Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Predictive Mean mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) pred_grad_f = jax . jit ( jax . vmap ( jax . grad ( mean_f , argnums = ( 0 )), in_axes = ( 0 ))) dmu_y = pred_grad_f ( Xtest ) # Predictive Variance var_correction_o1 = jnp . diag ( jnp . dot ( jnp . dot ( dmu_y , input_cov ), dmu_y . T )) # Uncertainty mu_y = mu_y_o1 uncertainty_t1 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt . show () 2 nd Order Taylor Expansion \u00b6 # =========================== # 2nd Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Mean function correction mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_df2 = jax . jit ( jax . vmap ( jax . hessian ( mean_f , argnums = ( 0 )), in_axes = ( 0 )) ) d2mu_y = mu_df2 ( Xtest ) d2mu_y = jnp . dot ( d2mu_y , input_cov ) mu_y_o2 = 0.5 * jnp . trace ( d2mu_y , axis1 = 1 , axis2 = 2 ) mu_y = mu_y_o1 + mu_y_o2 # Variance Function Correction var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_f = functools . partial ( predictive_variance , params , gp_priors , X , y ) pred_var_f = jax . jit ( jax . vmap ( jax . hessian ( var_f , argnums = ( 0 )), in_axes = ( 0 , None , None )) ) d2var_y2 = pred_var_f ( Xtest , True , False ) d2var_y2 = jnp . dot ( d2var_y2 , input_cov ) var_correction_o2 = jnp . trace ( d2var_y2 , axis1 = 1 , axis2 = 2 ) # Uncertainty uncertainty_t2 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze () + var_correction_o2 . squeeze () ) plt . plot ( mu_y_o1 + mu_y_o2 ) plt . plot ( mu_y_o1 ) plt . show () fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_2o.png\") plt . show () Results \u00b6 Standard GP Error Bars \u00b6 Taylor Expansion (1 st Order) \u00b6 Taylor Expansion (2 nd Order) \u00b6 Difference in 1 st and 2 nd Order \u00b6 fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), # label=r\"Predictive Mean\", color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"blue\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.4 , color = \"yellow\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_diff.png\") plt . show ()","title":"2.0 linearized gp"},{"location":"notebooks/2.0_linearized_gp/#linearized-gp","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Linearized GP"},{"location":"notebooks/2.0_linearized_gp/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2","title":"Imports"},{"location":"notebooks/2.0_linearized_gp/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Data"},{"location":"notebooks/2.0_linearized_gp/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"notebooks/2.0_linearized_gp/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"notebooks/2.0_linearized_gp/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 73.05it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34]","title":"Training"},{"location":"notebooks/2.0_linearized_gp/#predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Predictions"},{"location":"notebooks/2.0_linearized_gp/#1st-order-taylor-expansion","text":"# =========================== # 1st Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Predictive Mean mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) pred_grad_f = jax . jit ( jax . vmap ( jax . grad ( mean_f , argnums = ( 0 )), in_axes = ( 0 ))) dmu_y = pred_grad_f ( Xtest ) # Predictive Variance var_correction_o1 = jnp . diag ( jnp . dot ( jnp . dot ( dmu_y , input_cov ), dmu_y . T )) # Uncertainty mu_y = mu_y_o1 uncertainty_t1 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt . show ()","title":"1st Order Taylor Expansion"},{"location":"notebooks/2.0_linearized_gp/#2nd-order-taylor-expansion","text":"# =========================== # 2nd Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Mean function correction mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_df2 = jax . jit ( jax . vmap ( jax . hessian ( mean_f , argnums = ( 0 )), in_axes = ( 0 )) ) d2mu_y = mu_df2 ( Xtest ) d2mu_y = jnp . dot ( d2mu_y , input_cov ) mu_y_o2 = 0.5 * jnp . trace ( d2mu_y , axis1 = 1 , axis2 = 2 ) mu_y = mu_y_o1 + mu_y_o2 # Variance Function Correction var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_f = functools . partial ( predictive_variance , params , gp_priors , X , y ) pred_var_f = jax . jit ( jax . vmap ( jax . hessian ( var_f , argnums = ( 0 )), in_axes = ( 0 , None , None )) ) d2var_y2 = pred_var_f ( Xtest , True , False ) d2var_y2 = jnp . dot ( d2var_y2 , input_cov ) var_correction_o2 = jnp . trace ( d2var_y2 , axis1 = 1 , axis2 = 2 ) # Uncertainty uncertainty_t2 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze () + var_correction_o2 . squeeze () ) plt . plot ( mu_y_o1 + mu_y_o2 ) plt . plot ( mu_y_o1 ) plt . show () fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_2o.png\") plt . show ()","title":"2nd Order Taylor Expansion"},{"location":"notebooks/2.0_linearized_gp/#results","text":"","title":"Results"},{"location":"notebooks/2.0_linearized_gp/#standard-gp-error-bars","text":"","title":"Standard GP Error Bars"},{"location":"notebooks/2.0_linearized_gp/#taylor-expansion-1st-order","text":"","title":"Taylor Expansion (1st Order)"},{"location":"notebooks/2.0_linearized_gp/#taylor-expansion-2nd-order","text":"","title":"Taylor Expansion (2nd Order)"},{"location":"notebooks/2.0_linearized_gp/#difference-in-1st-and-2nd-order","text":"fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), # label=r\"Predictive Mean\", color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"blue\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.4 , color = \"yellow\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_diff.png\") plt . show ()","title":"Difference in 1st and 2nd Order"},{"location":"notebooks/3.0_unscented_gp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); GP - Unscented Transformation \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations SSMToyBox GPyTorch PILCO PyRoboLearn FilterPy - UKF | Unscented Transform Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import pathlib PATH = pathlib . Path ( here ()) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import numpyro import numpyro.distributions as dist import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () % load_ext autoreload % autoreload 2 Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 71.46it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34] Predictions \u00b6 Standard Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/mc/1d_gp.png\" )) plt . show () Uncsented Transformation \u00b6 from typing import Optional def unscented_sigma_points ( n_features : int , kappa : Optional [ float ] = None , alpha : float = 1.0 ): \"\"\"Unscented sigma points Parameters ---------- n_features : int, dimenion of the input random variables kappa : float, Optional scaling parameter alpha : float, Optional, default=1.0 parameter affecting the covariance Returns ------- sigma_points : np.ndarray, (n_features, n_sigma_points) Unscented sigma points \"\"\" kappa = jnp . max ([ 3.0 - n_features , 0.0 ]) if kappa is None else kappa lam = alpha ** 2 * ( n_features + kappa ) - n_features c = jnp . sqrt ( n_features + lam ) sigma_points = jnp . hstack (( jnp . zeros (( n_features , 1 )), c * jnp . eye ( n_features ), - c * jnp . eye ( n_features ) )) return sigma_points def unscented_weights ( n_features : int , kappa : Optional [ float ] = None , alpha : float = 1.0 , beta : float = 2.0 , ): \"\"\"Unscented transform weights. Parameters ---------- n_features : int Dimension of the input random variable. kappa : float, optional Scaling parameter. alpha : float, optional Parameter affecting covariance. Controls the spread of points (1e-3, 1) beta : float, optional Parameter affecting covariance. Used to encode additional (high-order) knowledge about the underlying Gaussian representation Returns ------- w : (num_points, ) ndarray Unscented weights for the transformed mean. wc : (num_points, ) ndarray Unscented weights for the transformed covariance. \"\"\" kappa = jnp . max ([ 3.0 - n_features , 0.0 ]) if kappa is None else kappa lam = alpha ** 2 * ( n_features + kappa ) - n_features wm = 1.0 / ( 2.0 * ( n_features + lam )) * jnp . ones ( 2 * n_features ) wc = wm . copy () wm0 = lam / ( n_features + lam ) wc0 = wm0 + ( 1 - alpha ** 2 + beta ) return jnp . hstack (( wm0 , wm )), jnp . hstack (( wc0 , wc )) X . shape , input_cov . shape logger . setLevel ( logging . DEBUG ) n_features = X . shape [ 1 ] n_mc_samples = 1000 # sigma point transform parameters alpha = 1. beta = 20.0 kappa = 1. Wm , Wc = unscented_weights ( n_features , kappa = kappa , alpha = alpha ) Wm = jnp . diag ( Wm ) Wc = jnp . diag ( Wc ) logging . debug ( f \"Wm, Wc: { Wm . shape } , { Wc . shape } \" ) # get unit sigma points key = jax . random . PRNGKey ( 0 ) unit_sigma_points = unscented_sigma_points ( n_features , kappa = kappa , alpha = alpha ) logging . debug ( f \"Sigma Points: { unit_sigma_points . shape } \" ) logging . debug ( f \"Xtest: { Xtest . shape } \" ) # form sigma-points from unit sigma-points x_ = Xtest + jnp . dot ( jnp . linalg . cholesky ( input_cov ), unit_sigma_points ) logging . debug ( f \"x_: { x_ . shape } \" ) # push sigma points through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) outputs = mu_f_batch ( x_ ) logging . debug ( f \"outputs: { outputs . shape } \" ) # output mean outputs_mean = jnp . mean ( outputs , axis = 1 ) . T logging . debug ( f \"outputs (mean): { outputs_mean . shape } \" ) # output covariance # logging.debug(f\"Cov: {outputs.shape}, {outputs_mean.shape}\") dfx = outputs - outputs_mean [:, None ] logging . debug ( f \"dfx: { dfx . shape } \" ) cov_f = jnp . dot ( jnp . dot ( dfx , Wc ), dfx . T ) logging . debug ( f \"cov_f: { cov_f . shape } \" ) 2020-05-04 13:00:00,582:DEBUG:Wm, Wc: (3, 3), (3, 3) 2020-05-04 13:00:00,587:DEBUG:Sigma Points: (1, 3) 2020-05-04 13:00:00,588:DEBUG:Xtest: (1000, 1) 2020-05-04 13:00:00,593:DEBUG:x_: (1000, 3) 2020-05-04 13:00:00,597:DEBUG:Compiling <unnamed function> for args (ShapedArray(float32[1000,3]),). 2020-05-04 13:00:00,658:DEBUG:outputs: (1000, 3) 2020-05-04 13:00:00,660:DEBUG:outputs (mean): (1000,) 2020-05-04 13:00:00,662:DEBUG:dfx: (1000, 3) 2020-05-04 13:00:00,663:DEBUG:cov_f: (1000, 1000) uncertainty = 1.96 * jnp . sqrt ( jnp . diag ( cov_f )) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/unscented/1d_gp_un.png\" )) # fig.savefig(\"figures/jaxgp/examples/mc/1d_gp_mc.png\") plt . show () More Points \u00b6 from typing import Optional def unscented_weights ( n_features : int , kappa : Optional [ float ] = None , alpha : float = 1.0 , beta : float = 2.0 , ): \"\"\"Unscented transform weights. Parameters ---------- n_features : int Dimension of the input random variable. kappa : float, optional Scaling parameter. alpha : float, optional Parameter affecting covariance. Controls the spread of points (1e-3, 1) beta : float, optional Parameter affecting covariance. Used to encode additional (high-order) knowledge about the underlying Gaussian representation Returns ------- w : (num_points, ) ndarray Unscented weights for the transformed mean. wc : (num_points, ) ndarray Unscented weights for the transformed covariance. \"\"\" tau = alpha ** 2 * ( n_features + kappa ) - n_features weight_mean_0 = tau / ( n_features + tau ) weight_cov_0 = tau / ( n_features + tau ) + ( 1 - alpha ** 2 + beta ) weight = 1. / ( 2. * ( n_features + tau ) ) gamma = jnp . sqrt ( n_features + tau ) return gamma , weight , weight_mean_0 , weight_cov_0 def generate_sigma_points ( x_mu , x_cov , gamma ): \"\"\"Unscented sigma points Parameters ---------- n_features : int, dimenion of the input random variables kappa : float, Optional scaling parameter alpha : float, Optional, default=1.0 parameter affecting the covariance Returns ------- sigma_points : np.ndarray, (n_features, n_sigma_points) Unscented sigma points \"\"\" term = gamma * np . sqrt ( x_cov . T ) ksigma_points = np . vstack (( x_mu , x_mu + term , x_mu - term )) return sigma_points gamma , weight , weight_mean_0 , weight_cov_0 = unscented_weights ( n_features = X [ 0 ] . shape [ 0 ], kappa = 0 , alpha = 1. , beta = 2. ) sigma_points = generate_sigma_points ( X [ 0 ], input_cov , gamma ) sigma_points . shape (1, 3)","title":"3.0 unscented gp"},{"location":"notebooks/3.0_unscented_gp/#gp-unscented-transformation","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations SSMToyBox GPyTorch PILCO PyRoboLearn FilterPy - UKF | Unscented Transform","title":"GP - Unscented Transformation"},{"location":"notebooks/3.0_unscented_gp/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import pathlib PATH = pathlib . Path ( here ()) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import numpyro import numpyro.distributions as dist import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () % load_ext autoreload % autoreload 2","title":"Imports"},{"location":"notebooks/3.0_unscented_gp/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , )","title":"Data"},{"location":"notebooks/3.0_unscented_gp/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"notebooks/3.0_unscented_gp/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"notebooks/3.0_unscented_gp/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 71.46it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34]","title":"Training"},{"location":"notebooks/3.0_unscented_gp/#predictions","text":"","title":"Predictions"},{"location":"notebooks/3.0_unscented_gp/#standard-predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/mc/1d_gp.png\" )) plt . show ()","title":"Standard Predictions"},{"location":"notebooks/3.0_unscented_gp/#uncsented-transformation","text":"from typing import Optional def unscented_sigma_points ( n_features : int , kappa : Optional [ float ] = None , alpha : float = 1.0 ): \"\"\"Unscented sigma points Parameters ---------- n_features : int, dimenion of the input random variables kappa : float, Optional scaling parameter alpha : float, Optional, default=1.0 parameter affecting the covariance Returns ------- sigma_points : np.ndarray, (n_features, n_sigma_points) Unscented sigma points \"\"\" kappa = jnp . max ([ 3.0 - n_features , 0.0 ]) if kappa is None else kappa lam = alpha ** 2 * ( n_features + kappa ) - n_features c = jnp . sqrt ( n_features + lam ) sigma_points = jnp . hstack (( jnp . zeros (( n_features , 1 )), c * jnp . eye ( n_features ), - c * jnp . eye ( n_features ) )) return sigma_points def unscented_weights ( n_features : int , kappa : Optional [ float ] = None , alpha : float = 1.0 , beta : float = 2.0 , ): \"\"\"Unscented transform weights. Parameters ---------- n_features : int Dimension of the input random variable. kappa : float, optional Scaling parameter. alpha : float, optional Parameter affecting covariance. Controls the spread of points (1e-3, 1) beta : float, optional Parameter affecting covariance. Used to encode additional (high-order) knowledge about the underlying Gaussian representation Returns ------- w : (num_points, ) ndarray Unscented weights for the transformed mean. wc : (num_points, ) ndarray Unscented weights for the transformed covariance. \"\"\" kappa = jnp . max ([ 3.0 - n_features , 0.0 ]) if kappa is None else kappa lam = alpha ** 2 * ( n_features + kappa ) - n_features wm = 1.0 / ( 2.0 * ( n_features + lam )) * jnp . ones ( 2 * n_features ) wc = wm . copy () wm0 = lam / ( n_features + lam ) wc0 = wm0 + ( 1 - alpha ** 2 + beta ) return jnp . hstack (( wm0 , wm )), jnp . hstack (( wc0 , wc )) X . shape , input_cov . shape logger . setLevel ( logging . DEBUG ) n_features = X . shape [ 1 ] n_mc_samples = 1000 # sigma point transform parameters alpha = 1. beta = 20.0 kappa = 1. Wm , Wc = unscented_weights ( n_features , kappa = kappa , alpha = alpha ) Wm = jnp . diag ( Wm ) Wc = jnp . diag ( Wc ) logging . debug ( f \"Wm, Wc: { Wm . shape } , { Wc . shape } \" ) # get unit sigma points key = jax . random . PRNGKey ( 0 ) unit_sigma_points = unscented_sigma_points ( n_features , kappa = kappa , alpha = alpha ) logging . debug ( f \"Sigma Points: { unit_sigma_points . shape } \" ) logging . debug ( f \"Xtest: { Xtest . shape } \" ) # form sigma-points from unit sigma-points x_ = Xtest + jnp . dot ( jnp . linalg . cholesky ( input_cov ), unit_sigma_points ) logging . debug ( f \"x_: { x_ . shape } \" ) # push sigma points through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) outputs = mu_f_batch ( x_ ) logging . debug ( f \"outputs: { outputs . shape } \" ) # output mean outputs_mean = jnp . mean ( outputs , axis = 1 ) . T logging . debug ( f \"outputs (mean): { outputs_mean . shape } \" ) # output covariance # logging.debug(f\"Cov: {outputs.shape}, {outputs_mean.shape}\") dfx = outputs - outputs_mean [:, None ] logging . debug ( f \"dfx: { dfx . shape } \" ) cov_f = jnp . dot ( jnp . dot ( dfx , Wc ), dfx . T ) logging . debug ( f \"cov_f: { cov_f . shape } \" ) 2020-05-04 13:00:00,582:DEBUG:Wm, Wc: (3, 3), (3, 3) 2020-05-04 13:00:00,587:DEBUG:Sigma Points: (1, 3) 2020-05-04 13:00:00,588:DEBUG:Xtest: (1000, 1) 2020-05-04 13:00:00,593:DEBUG:x_: (1000, 3) 2020-05-04 13:00:00,597:DEBUG:Compiling <unnamed function> for args (ShapedArray(float32[1000,3]),). 2020-05-04 13:00:00,658:DEBUG:outputs: (1000, 3) 2020-05-04 13:00:00,660:DEBUG:outputs (mean): (1000,) 2020-05-04 13:00:00,662:DEBUG:dfx: (1000, 3) 2020-05-04 13:00:00,663:DEBUG:cov_f: (1000, 1000) uncertainty = 1.96 * jnp . sqrt ( jnp . diag ( cov_f )) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/unscented/1d_gp_un.png\" )) # fig.savefig(\"figures/jaxgp/examples/mc/1d_gp_mc.png\") plt . show ()","title":"Uncsented Transformation"},{"location":"notebooks/3.0_unscented_gp/#more-points","text":"from typing import Optional def unscented_weights ( n_features : int , kappa : Optional [ float ] = None , alpha : float = 1.0 , beta : float = 2.0 , ): \"\"\"Unscented transform weights. Parameters ---------- n_features : int Dimension of the input random variable. kappa : float, optional Scaling parameter. alpha : float, optional Parameter affecting covariance. Controls the spread of points (1e-3, 1) beta : float, optional Parameter affecting covariance. Used to encode additional (high-order) knowledge about the underlying Gaussian representation Returns ------- w : (num_points, ) ndarray Unscented weights for the transformed mean. wc : (num_points, ) ndarray Unscented weights for the transformed covariance. \"\"\" tau = alpha ** 2 * ( n_features + kappa ) - n_features weight_mean_0 = tau / ( n_features + tau ) weight_cov_0 = tau / ( n_features + tau ) + ( 1 - alpha ** 2 + beta ) weight = 1. / ( 2. * ( n_features + tau ) ) gamma = jnp . sqrt ( n_features + tau ) return gamma , weight , weight_mean_0 , weight_cov_0 def generate_sigma_points ( x_mu , x_cov , gamma ): \"\"\"Unscented sigma points Parameters ---------- n_features : int, dimenion of the input random variables kappa : float, Optional scaling parameter alpha : float, Optional, default=1.0 parameter affecting the covariance Returns ------- sigma_points : np.ndarray, (n_features, n_sigma_points) Unscented sigma points \"\"\" term = gamma * np . sqrt ( x_cov . T ) ksigma_points = np . vstack (( x_mu , x_mu + term , x_mu - term )) return sigma_points gamma , weight , weight_mean_0 , weight_cov_0 = unscented_weights ( n_features = X [ 0 ] . shape [ 0 ], kappa = 0 , alpha = 1. , beta = 2. ) sigma_points = generate_sigma_points ( X [ 0 ], input_cov , gamma ) sigma_points . shape (1, 3)","title":"More Points"},{"location":"notebooks/4.0_mcmc_gp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); GP - MCMC Posterior \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations SSMToyBox GPyTorch PILCO Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import pathlib PATH = pathlib . Path ( here ()) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import numpyro import numpyro.distributions as dist import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:08<00:00, 58.48it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34] Predictions \u00b6 Standard Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/mc/1d_gp.png\" )) plt . show () Monte Carlo Samples \u00b6 X . shape , input_cov . shape logger . setLevel ( logging . INFO ) n_features = X . shape [ 1 ] n_mc_samples = 1000 Wc = 1.0 / ( n_mc_samples - 1 ) Wm = 1.0 / n_mc_samples # get unit sigma points key = jax . random . PRNGKey ( 0 ) unit_sigma_points = jax . random . multivariate_normal ( key , jnp . zeros ( n_features ), jnp . eye ( n_features ), shape = ( n_mc_samples ,) ) . T logging . debug ( f \"Sigma Points: { unit_sigma_points . shape } \" ) logging . debug ( f \"Xtest: { Xtest . shape } \" ) # form sigma-points from unit sigma-points x_ = Xtest + jnp . dot ( jnp . linalg . cholesky ( input_cov ), unit_sigma_points ) logging . debug ( f \"x_: { x_ . shape } \" ) # push sigma points through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) outputs = mu_f_batch ( x_ ) logging . debug ( f \"outputs: { outputs . shape } \" ) # output mean outputs_mean = jnp . mean ( outputs , axis = 1 ) . T logging . debug ( f \"outputs (mean): { outputs_mean . shape } \" ) # output covariance # logging.debug(f\"Cov: {outputs.shape}, {outputs_mean.shape}\") dfx = outputs - outputs_mean [:, None ] logging . debug ( f \"dfx: { dfx . shape } \" ) cov_f = Wc * jnp . dot ( dfx , dfx . T ) logging . debug ( f \"cov_f: { cov_f . shape } \" ) # output covariance # dfx = # mu_y_ = predictive_mean() uncertainty = 1.96 * jnp . sqrt ( jnp . diag ( cov_f )) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/mc/1d_gp_mc.png\" )) # fig.savefig(\"figures/jaxgp/examples/mc/1d_gp_mc.png\") plt . show () Refactoring w. Numpyro \u00b6 MC Sampling \u00b6 X . shape , input_cov . shape logger . setLevel ( logging . DEBUG ) n_features = X . shape [ 1 ] n_mc_samples = 1000 Wc = 1.0 / ( n_mc_samples - 1 ) Wm = 1.0 / n_mc_samples # get unit sigma points # create data distribution key = jax . random . PRNGKey ( 0 ) sigma_dist = dist . Normal ( loc = jnp . zeros ( shape = ( n_features ,)), scale = jnp . eye ( n_features )) # generate samples unit_sigma_samples = sigma_dist . sample ( key , sample_shape = ( n_mc_samples ,)) logging . debug ( f \"Sigma Points: { unit_sigma_points . shape } \" ) logging . debug ( f \"Xtest: { Xtest . shape } \" ) # form sigma-points from unit sigma-points x_ = Xtest [:, None ] + jnp . dot ( jnp . linalg . cholesky ( input_cov ), unit_sigma_samples ) logging . debug ( f \"X Noise: { x_ . shape } , \" ) # push sigma points through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) outputs = mu_f_batch ( x_ ) logging . debug ( f \"outputs: { outputs . shape } \" ) # output mean outputs_mean = jnp . mean ( outputs , axis = 1 ) . T logging . debug ( f \"outputs (mean): { outputs_mean . shape } \" ) # output covariance # logging.debug(f\"Cov: {outputs.shape}, {outputs_mean.shape}\") dfx = outputs - outputs_mean [:, None ] logging . debug ( f \"dfx: { dfx . shape } \" ) cov_f = Wc * jnp . dot ( dfx , dfx . T ) logging . debug ( f \"cov_f: { cov_f . shape } \" ) # output covariance # dfx = uncertainty = 1.96 * jnp . sqrt ( jnp . diag ( cov_f )) 2020-05-04 11:45:10,334:DEBUG:Sigma Points: (1, 1000) 2020-05-04 11:45:10,335:DEBUG:Xtest: (1000, 1) 2020-05-04 11:45:10,339:DEBUG:X Noise: (1000, 1000, 1), 2020-05-04 11:45:10,343:DEBUG:Compiling <unnamed function> for args (ShapedArray(float32[1000,1000,1]),). 2020-05-04 11:45:10,397:DEBUG:outputs: (1000, 1000) 2020-05-04 11:45:10,406:DEBUG:outputs (mean): (1000,) 2020-05-04 11:45:10,409:DEBUG:dfx: (1000, 1000) 2020-05-04 11:45:10,411:DEBUG:cov_f: (1000, 1000) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () Exact X Distribution \u00b6 logger . setLevel ( logging . INFO ) n_features = X . shape [ 1 ] n_mc_samples = 1000 # create data distribution key = jax . random . PRNGKey ( 0 ) x_noise_dist = dist . Normal ( loc = jnp . zeros (( n_features ,)), scale = jnp . sqrt ( jnp . diag ( input_cov ))) # generate samples x_noise_samples = x_noise_dist . sample ( key , sample_shape = ( n_mc_samples ,)) x_samples = Xtest [ None , :] + x_noise_samples [:, None ] # push samples through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) predictions = mu_f_batch ( x_samples ) mean_prediction = jnp . mean ( predictions , axis = 0 ) percentiles = jnp . percentile ( predictions , [ 5. , 95.0 ], axis = 0 ) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mean_prediction . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), percentiles [ 0 ,:], percentiles [ 1 ,:], alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (90% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"4.0 mcmc gp"},{"location":"notebooks/4.0_mcmc_gp/#gp-mcmc-posterior","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations SSMToyBox GPyTorch PILCO","title":"GP - MCMC Posterior"},{"location":"notebooks/4.0_mcmc_gp/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import pathlib PATH = pathlib . Path ( here ()) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import numpyro import numpyro.distributions as dist import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Imports"},{"location":"notebooks/4.0_mcmc_gp/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , )","title":"Data"},{"location":"notebooks/4.0_mcmc_gp/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"notebooks/4.0_mcmc_gp/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"notebooks/4.0_mcmc_gp/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:08<00:00, 58.48it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34]","title":"Training"},{"location":"notebooks/4.0_mcmc_gp/#predictions","text":"","title":"Predictions"},{"location":"notebooks/4.0_mcmc_gp/#standard-predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/mc/1d_gp.png\" )) plt . show ()","title":"Standard Predictions"},{"location":"notebooks/4.0_mcmc_gp/#monte-carlo-samples","text":"X . shape , input_cov . shape logger . setLevel ( logging . INFO ) n_features = X . shape [ 1 ] n_mc_samples = 1000 Wc = 1.0 / ( n_mc_samples - 1 ) Wm = 1.0 / n_mc_samples # get unit sigma points key = jax . random . PRNGKey ( 0 ) unit_sigma_points = jax . random . multivariate_normal ( key , jnp . zeros ( n_features ), jnp . eye ( n_features ), shape = ( n_mc_samples ,) ) . T logging . debug ( f \"Sigma Points: { unit_sigma_points . shape } \" ) logging . debug ( f \"Xtest: { Xtest . shape } \" ) # form sigma-points from unit sigma-points x_ = Xtest + jnp . dot ( jnp . linalg . cholesky ( input_cov ), unit_sigma_points ) logging . debug ( f \"x_: { x_ . shape } \" ) # push sigma points through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) outputs = mu_f_batch ( x_ ) logging . debug ( f \"outputs: { outputs . shape } \" ) # output mean outputs_mean = jnp . mean ( outputs , axis = 1 ) . T logging . debug ( f \"outputs (mean): { outputs_mean . shape } \" ) # output covariance # logging.debug(f\"Cov: {outputs.shape}, {outputs_mean.shape}\") dfx = outputs - outputs_mean [:, None ] logging . debug ( f \"dfx: { dfx . shape } \" ) cov_f = Wc * jnp . dot ( dfx , dfx . T ) logging . debug ( f \"cov_f: { cov_f . shape } \" ) # output covariance # dfx = # mu_y_ = predictive_mean() uncertainty = 1.96 * jnp . sqrt ( jnp . diag ( cov_f )) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( PATH . joinpath ( \"figures/jaxgp/examples/mc/1d_gp_mc.png\" )) # fig.savefig(\"figures/jaxgp/examples/mc/1d_gp_mc.png\") plt . show ()","title":"Monte Carlo Samples"},{"location":"notebooks/4.0_mcmc_gp/#refactoring-w-numpyro","text":"","title":"Refactoring w. Numpyro"},{"location":"notebooks/4.0_mcmc_gp/#mc-sampling","text":"X . shape , input_cov . shape logger . setLevel ( logging . DEBUG ) n_features = X . shape [ 1 ] n_mc_samples = 1000 Wc = 1.0 / ( n_mc_samples - 1 ) Wm = 1.0 / n_mc_samples # get unit sigma points # create data distribution key = jax . random . PRNGKey ( 0 ) sigma_dist = dist . Normal ( loc = jnp . zeros ( shape = ( n_features ,)), scale = jnp . eye ( n_features )) # generate samples unit_sigma_samples = sigma_dist . sample ( key , sample_shape = ( n_mc_samples ,)) logging . debug ( f \"Sigma Points: { unit_sigma_points . shape } \" ) logging . debug ( f \"Xtest: { Xtest . shape } \" ) # form sigma-points from unit sigma-points x_ = Xtest [:, None ] + jnp . dot ( jnp . linalg . cholesky ( input_cov ), unit_sigma_samples ) logging . debug ( f \"X Noise: { x_ . shape } , \" ) # push sigma points through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) outputs = mu_f_batch ( x_ ) logging . debug ( f \"outputs: { outputs . shape } \" ) # output mean outputs_mean = jnp . mean ( outputs , axis = 1 ) . T logging . debug ( f \"outputs (mean): { outputs_mean . shape } \" ) # output covariance # logging.debug(f\"Cov: {outputs.shape}, {outputs_mean.shape}\") dfx = outputs - outputs_mean [:, None ] logging . debug ( f \"dfx: { dfx . shape } \" ) cov_f = Wc * jnp . dot ( dfx , dfx . T ) logging . debug ( f \"cov_f: { cov_f . shape } \" ) # output covariance # dfx = uncertainty = 1.96 * jnp . sqrt ( jnp . diag ( cov_f )) 2020-05-04 11:45:10,334:DEBUG:Sigma Points: (1, 1000) 2020-05-04 11:45:10,335:DEBUG:Xtest: (1000, 1) 2020-05-04 11:45:10,339:DEBUG:X Noise: (1000, 1000, 1), 2020-05-04 11:45:10,343:DEBUG:Compiling <unnamed function> for args (ShapedArray(float32[1000,1000,1]),). 2020-05-04 11:45:10,397:DEBUG:outputs: (1000, 1000) 2020-05-04 11:45:10,406:DEBUG:outputs (mean): (1000,) 2020-05-04 11:45:10,409:DEBUG:dfx: (1000, 1000) 2020-05-04 11:45:10,411:DEBUG:cov_f: (1000, 1000) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"MC Sampling"},{"location":"notebooks/4.0_mcmc_gp/#exact-x-distribution","text":"logger . setLevel ( logging . INFO ) n_features = X . shape [ 1 ] n_mc_samples = 1000 # create data distribution key = jax . random . PRNGKey ( 0 ) x_noise_dist = dist . Normal ( loc = jnp . zeros (( n_features ,)), scale = jnp . sqrt ( jnp . diag ( input_cov ))) # generate samples x_noise_samples = x_noise_dist . sample ( key , sample_shape = ( n_mc_samples ,)) x_samples = Xtest [ None , :] + x_noise_samples [:, None ] # push samples through non-linearity mu_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_f_batch = jax . jit ( jax . vmap ( mu_f , in_axes = ( 0 ))) predictions = mu_f_batch ( x_samples ) mean_prediction = jnp . mean ( predictions , axis = 0 ) percentiles = jnp . percentile ( predictions , [ 5. , 95.0 ], axis = 0 ) logger . setLevel ( logging . INFO ) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mean_prediction . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), percentiles [ 0 ,:], percentiles [ 1 ,:], alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (90% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Exact X Distribution"},{"location":"notebooks/distances/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Distances \u00b6 Resources High-Performance Computation in Python | Numpy - Blog d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2} d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2} <span><span class=\"MathJax_Preview\">d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2}</span><script type=\"math/tex\">d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2} #@title Packages import functools import jax import jax.numpy as jnp import numpy as onp from sklearn.metrics.pairwise import euclidean_distances , haversine_distances from sklearn import datasets # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) #@title Plot Functions def plot_kernel_mat ( K ): # plot plt . figure () plt . imshow ( K , cmap = 'Reds' ) plt . title ( r '$K_ {ff} $, (rbf)' , fontsize = 20 , weight = 'bold' ) plt . tight_layout () plt . show () #@title Data def get_1d_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X_test = jnp . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) X = X [:, None ] X_test = X [:, None ] assert X . shape == ( N , 1 ) assert Y . shape == ( N ,) return X , Y , X_test def get_2d_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X1 = jnp . linspace ( - 10 , 10 , N ) X2 = jnp . linspace ( - 5 , 2 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X1 )) + jnp . exp ( X2 ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X1_test = jnp . linspace ( - 11 , 11 , N_test ) X2_test = jnp . linspace ( - 6 , 4 , N_test ) X = jnp . vstack (( X1 , X2 )) . T X_test = jnp . vstack (( X1_test , X2_test )) . T assert X . shape == ( N , 2 ) assert Y . shape == ( N ,) return X , Y , X_test # Get Data X , Y , X_test = get_1d_data ( 100 , sigma_inputs = 0.0 , sigma_obs = 0.1 , N_test = 100 ) /home/emmanuel/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Squared Euclidean Distance \u00b6 #@title Kernel Functions # Squared Euclidean Distance Formula # @jax.jit # def sqeuclidean_distance(x, y): # z = x - y # return jnp.einsum(\"i,i->\", z, z) @jax . jit def sqeuclidean_distance ( x , y ): return jnp . linalg . norm ( x - y ) ** 2 # return jnp.sum( (x - y) ** 2) # @jax.jit # def sqeuclidean_distance_vect(x, y): # return jnp.linalg.norm(x - y, axis=1)**2 # return jnp.sum( (x - y) ** 2) # @jax.jit # def sqeuclidean_distance(x, y): # return jnp.sum( (x - y) ** 2) # @jax.jit def gram ( func , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( x1 , y1 ))( x ))( y ) # X, Y, X_test = get_2d_data(1_000, sigma_obs=0.1) X , y = datasets . make_regression ( 1_000 , 10 ) X_ = jnp . array ( X ) type ( X_ ), X_ . shape (jax.interpreters.xla.DeviceArray, (1000, 10)) test_X = X . copy () #[:2, :] test_Y = X . copy () #[:2, :] dist_x_sk = euclidean_distances ( onp . array ( test_X ), onp . array ( test_Y ), squared = True ) euclidean_mat = jax . jit ( functools . partial ( gram , sqeuclidean_distance )) dist_x = euclidean_mat ( test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( dist_x ), dist_x_sk , decimal = 1e-5 ) plot_kernel_mat ( dist_x ) Speed Test \u00b6 % timeit _ = euclidean_distances ( X , X , squared = True ) % timeit _ = euclidean_mat ( X_ , X_ ) 6.75 ms \u00b1 58.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 491 \u00b5s \u00b1 3.16 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) Haversine Distance \u00b6","title":"Distances"},{"location":"notebooks/distances/#distances","text":"Resources High-Performance Computation in Python | Numpy - Blog d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2} d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2} <span><span class=\"MathJax_Preview\">d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2}</span><script type=\"math/tex\">d(u,v) = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2} #@title Packages import functools import jax import jax.numpy as jnp import numpy as onp from sklearn.metrics.pairwise import euclidean_distances , haversine_distances from sklearn import datasets # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) #@title Plot Functions def plot_kernel_mat ( K ): # plot plt . figure () plt . imshow ( K , cmap = 'Reds' ) plt . title ( r '$K_ {ff} $, (rbf)' , fontsize = 20 , weight = 'bold' ) plt . tight_layout () plt . show () #@title Data def get_1d_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X_test = jnp . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) X = X [:, None ] X_test = X [:, None ] assert X . shape == ( N , 1 ) assert Y . shape == ( N ,) return X , Y , X_test def get_2d_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X1 = jnp . linspace ( - 10 , 10 , N ) X2 = jnp . linspace ( - 5 , 2 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X1 )) + jnp . exp ( X2 ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X1_test = jnp . linspace ( - 11 , 11 , N_test ) X2_test = jnp . linspace ( - 6 , 4 , N_test ) X = jnp . vstack (( X1 , X2 )) . T X_test = jnp . vstack (( X1_test , X2_test )) . T assert X . shape == ( N , 2 ) assert Y . shape == ( N ,) return X , Y , X_test # Get Data X , Y , X_test = get_1d_data ( 100 , sigma_inputs = 0.0 , sigma_obs = 0.1 , N_test = 100 ) /home/emmanuel/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Distances"},{"location":"notebooks/distances/#squared-euclidean-distance","text":"#@title Kernel Functions # Squared Euclidean Distance Formula # @jax.jit # def sqeuclidean_distance(x, y): # z = x - y # return jnp.einsum(\"i,i->\", z, z) @jax . jit def sqeuclidean_distance ( x , y ): return jnp . linalg . norm ( x - y ) ** 2 # return jnp.sum( (x - y) ** 2) # @jax.jit # def sqeuclidean_distance_vect(x, y): # return jnp.linalg.norm(x - y, axis=1)**2 # return jnp.sum( (x - y) ** 2) # @jax.jit # def sqeuclidean_distance(x, y): # return jnp.sum( (x - y) ** 2) # @jax.jit def gram ( func , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( x1 , y1 ))( x ))( y ) # X, Y, X_test = get_2d_data(1_000, sigma_obs=0.1) X , y = datasets . make_regression ( 1_000 , 10 ) X_ = jnp . array ( X ) type ( X_ ), X_ . shape (jax.interpreters.xla.DeviceArray, (1000, 10)) test_X = X . copy () #[:2, :] test_Y = X . copy () #[:2, :] dist_x_sk = euclidean_distances ( onp . array ( test_X ), onp . array ( test_Y ), squared = True ) euclidean_mat = jax . jit ( functools . partial ( gram , sqeuclidean_distance )) dist_x = euclidean_mat ( test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( dist_x ), dist_x_sk , decimal = 1e-5 ) plot_kernel_mat ( dist_x )","title":"Squared Euclidean Distance"},{"location":"notebooks/distances/#speed-test","text":"% timeit _ = euclidean_distances ( X , X , squared = True ) % timeit _ = euclidean_mat ( X_ , X_ ) 6.75 ms \u00b1 58.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 491 \u00b5s \u00b1 3.16 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)","title":"Speed Test"},{"location":"notebooks/distances/#haversine-distance","text":"","title":"Haversine Distance"},{"location":"notebooks/egp_pyro_svgp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Preamble \u00b6 ! pip install pyro - ppl Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.2.1) Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.42.1) Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0) Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.5) Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0) Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1) import os import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-darkgrid' , 'seaborn-notebook' ]) import torch from torch.nn import Parameter import pyro import pyro.contrib.gp as gp import pyro.distributions as dist from pyro.nn import PyroSample , PyroParam from scipy.cluster.vq import kmeans2 smoke_test = ( 'CI' in os . environ ) # ignore; used to check code integrity in the Pyro repo # assert pyro.__version__.startswith('0.5.1') pyro . enable_validation ( True ) # can help with debugging pyro . set_rng_seed ( 0 ) Plot Utils \u00b6 # note that this helper function does three different things: # (i) plots the observed data; # (ii) plots the predictions from the learned GP after conditioning on data; # (iii) plots samples from the GP prior (with no conditioning on observed data) def plot ( plot_observed_data = False , plot_predictions = False , n_prior_samples = 0 , model = None , kernel = None , n_test = 500 ): plt . figure ( figsize = ( 12 , 6 )) if plot_observed_data : plt . plot ( X . numpy (), y . numpy (), 'kx' ) if plot_predictions : Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs # compute predictive mean and variance with torch . no_grad (): if type ( model ) == gp . models . VariationalSparseGP : mean , cov = model ( Xtest , full_cov = True ) else : try : mean , cov = model ( Xtest , full_cov = True , noiseless = False ) except : mean , cov = model ( Xtest ) sd = cov . diag () . sqrt () # standard deviation at each input point x plt . plot ( Xtest . numpy (), mean . numpy (), 'r' , lw = 2 ) # plot the mean plt . fill_between ( Xtest . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * sd ) . numpy (), ( mean + 2.0 * sd ) . numpy (), color = 'C0' , alpha = 0.3 ) if n_prior_samples > 0 : # plot samples from the GP prior Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs noise = ( model . noise if type ( model ) != gp . models . VariationalSparseGP else model . likelihood . variance ) cov = kernel . forward ( Xtest ) + noise . expand ( n_test ) . diag () samples = dist . MultivariateNormal ( torch . zeros ( n_test ), covariance_matrix = cov ) \\ . sample ( sample_shape = ( n_prior_samples ,)) plt . plot ( Xtest . numpy (), samples . numpy () . T , lw = 2 , alpha = 0.4 ) plt . xlim ( - 0.5 , 5.5 ) Data \u00b6 n_samples = 500 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda () Variational GP Regression \u00b6 Model \u00b6 # initialize the inducing inputs (kmeans) n_inducing = 20. Xu = kmeans2 ( X . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vsgp = gp . models . VariationalSparseGP ( X , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vsgp . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vsgp . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( vsgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vsgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational Sparse GP' , fontsize = 20 ) plt . show () Uncertain VSGP \u00b6 Method 0 - Standard Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites)) Losses \u00b6 plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figure plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Uncertain VSGP, Standard Prior' , fontsize = 20 ) plt . show () Method III - Bayesian Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # gplvm.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites)) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Bayesian Prior' , fontsize = 20 ) plt . show () Method I - Strong Conservative Prior \u00b6 In this method I will be imposing the following constraints: $$ \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} $$ Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.05 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Strong Conservative Prior' , fontsize = 20 ) plt . show () Method II - Strong Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, strong prior' , fontsize = 20 ) plt . show ()","title":"Egp pyro svgp"},{"location":"notebooks/egp_pyro_svgp/#preamble","text":"! pip install pyro - ppl Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.2.1) Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.42.1) Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0) Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.5) Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0) Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1) import os import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-darkgrid' , 'seaborn-notebook' ]) import torch from torch.nn import Parameter import pyro import pyro.contrib.gp as gp import pyro.distributions as dist from pyro.nn import PyroSample , PyroParam from scipy.cluster.vq import kmeans2 smoke_test = ( 'CI' in os . environ ) # ignore; used to check code integrity in the Pyro repo # assert pyro.__version__.startswith('0.5.1') pyro . enable_validation ( True ) # can help with debugging pyro . set_rng_seed ( 0 )","title":"Preamble"},{"location":"notebooks/egp_pyro_svgp/#plot-utils","text":"# note that this helper function does three different things: # (i) plots the observed data; # (ii) plots the predictions from the learned GP after conditioning on data; # (iii) plots samples from the GP prior (with no conditioning on observed data) def plot ( plot_observed_data = False , plot_predictions = False , n_prior_samples = 0 , model = None , kernel = None , n_test = 500 ): plt . figure ( figsize = ( 12 , 6 )) if plot_observed_data : plt . plot ( X . numpy (), y . numpy (), 'kx' ) if plot_predictions : Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs # compute predictive mean and variance with torch . no_grad (): if type ( model ) == gp . models . VariationalSparseGP : mean , cov = model ( Xtest , full_cov = True ) else : try : mean , cov = model ( Xtest , full_cov = True , noiseless = False ) except : mean , cov = model ( Xtest ) sd = cov . diag () . sqrt () # standard deviation at each input point x plt . plot ( Xtest . numpy (), mean . numpy (), 'r' , lw = 2 ) # plot the mean plt . fill_between ( Xtest . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * sd ) . numpy (), ( mean + 2.0 * sd ) . numpy (), color = 'C0' , alpha = 0.3 ) if n_prior_samples > 0 : # plot samples from the GP prior Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs noise = ( model . noise if type ( model ) != gp . models . VariationalSparseGP else model . likelihood . variance ) cov = kernel . forward ( Xtest ) + noise . expand ( n_test ) . diag () samples = dist . MultivariateNormal ( torch . zeros ( n_test ), covariance_matrix = cov ) \\ . sample ( sample_shape = ( n_prior_samples ,)) plt . plot ( Xtest . numpy (), samples . numpy () . T , lw = 2 , alpha = 0.4 ) plt . xlim ( - 0.5 , 5.5 )","title":"Plot Utils"},{"location":"notebooks/egp_pyro_svgp/#data","text":"n_samples = 500 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda ()","title":"Data"},{"location":"notebooks/egp_pyro_svgp/#variational-gp-regression","text":"","title":"Variational GP Regression"},{"location":"notebooks/egp_pyro_svgp/#model","text":"# initialize the inducing inputs (kmeans) n_inducing = 20. Xu = kmeans2 ( X . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vsgp = gp . models . VariationalSparseGP ( X , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vsgp . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"notebooks/egp_pyro_svgp/#inference","text":"# the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vsgp . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( vsgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer )","title":"Inference"},{"location":"notebooks/egp_pyro_svgp/#losses","text":"plt . plot ( losses );","title":"Losses"},{"location":"notebooks/egp_pyro_svgp/#predictions","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vsgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational Sparse GP' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"notebooks/egp_pyro_svgp/#uncertain-vsgp","text":"","title":"Uncertain VSGP"},{"location":"notebooks/egp_pyro_svgp/#method-0-standard-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter.","title":"Method 0 - Standard Prior"},{"location":"notebooks/egp_pyro_svgp/#model_1","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"notebooks/egp_pyro_svgp/#inference_1","text":"# the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites))","title":"Inference"},{"location":"notebooks/egp_pyro_svgp/#losses_1","text":"plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figure plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Uncertain VSGP, Standard Prior' , fontsize = 20 ) plt . show ()","title":"Losses"},{"location":"notebooks/egp_pyro_svgp/#method-iii-bayesian-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter.","title":"Method III - Bayesian Prior"},{"location":"notebooks/egp_pyro_svgp/#model_2","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # gplvm.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"notebooks/egp_pyro_svgp/#inference_2","text":"# the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites))","title":"Inference"},{"location":"notebooks/egp_pyro_svgp/#losses_2","text":"plt . plot ( losses );","title":"Losses"},{"location":"notebooks/egp_pyro_svgp/#predictions_1","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Bayesian Prior' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"notebooks/egp_pyro_svgp/#method-i-strong-conservative-prior","text":"In this method I will be imposing the following constraints: $$ \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} $$","title":"Method I - Strong Conservative Prior"},{"location":"notebooks/egp_pyro_svgp/#model_3","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.05 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"notebooks/egp_pyro_svgp/#inference_3","text":"# the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer )","title":"Inference"},{"location":"notebooks/egp_pyro_svgp/#losses_3","text":"plt . plot ( losses );","title":"Losses"},{"location":"notebooks/egp_pyro_svgp/#predictions_2","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Strong Conservative Prior' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"notebooks/egp_pyro_svgp/#method-ii-strong-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned}","title":"Method II - Strong Prior"},{"location":"notebooks/egp_pyro_svgp/#model_4","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"notebooks/egp_pyro_svgp/#inference_4","text":"# the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer )","title":"Inference"},{"location":"notebooks/egp_pyro_svgp/#losses_4","text":"plt . plot ( losses );","title":"Losses"},{"location":"notebooks/egp_pyro_svgp/#predictions_3","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, strong prior' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"notebooks/egp_pyro_vgp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Preamble \u00b6 #@title Package Install ! pip install pyro - ppl Collecting pyro-ppl Downloading https://files.pythonhosted.org/packages/c0/77/4db4946f6b5bf0601869c7b7594def42a7197729167484e1779fff5ca0d6/pyro_ppl-1.3.1-py3-none-any.whl (520kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 522kB 6.4MB/s eta 0:00:01 Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1) Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.3) Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.38.0) Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0) Collecting pyro-api>=0.1.1 Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl Installing collected packages: pyro-api, pyro-ppl Successfully installed pyro-api-0.1.1 pyro-ppl-1.3.1 #@title Import Packages import os import time import torch from torch.nn import Parameter import pyro import pyro.contrib.gp as gp import pyro.distributions as dist from pyro.nn import PyroSample , PyroParam from scipy.cluster.vq import kmeans2 smoke_test = ( 'CI' in os . environ ) # ignore; used to check code integrity in the Pyro repo # assert pyro.__version__.startswith('0.5.1') pyro . enable_validation ( True ) # can help with debugging pyro . set_rng_seed ( 0 ) # import matplotlib.pyplot as plt # plt.style.use(['seaborn-darkgrid', 'seaborn-notebook']) import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm #@title Plot Utils # note that this helper function does three different things: # (i) plots the observed data; # (ii) plots the predictions from the learned GP after conditioning on data; # (iii) plots samples from the GP prior (with no conditioning on observed data) def plot ( plot_observed_data = False , plot_predictions = False , n_prior_samples = 0 , model = None , kernel = None , n_test = 500 ): plt . figure ( figsize = ( 12 , 6 )) if plot_observed_data : plt . plot ( X . numpy (), y . numpy (), 'kx' ) if plot_predictions : Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs # compute predictive mean and variance with torch . no_grad (): if type ( model ) == gp . models . VariationalSparseGP : mean , cov = model ( Xtest , full_cov = True ) else : try : mean , cov = model ( Xtest , full_cov = True , noiseless = False ) except : mean , cov = model ( Xtest ) sd = cov . diag () . sqrt () # standard deviation at each input point x plt . plot ( Xtest . numpy (), mean . numpy (), 'r' , lw = 2 ) # plot the mean plt . fill_between ( Xtest . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * sd ) . numpy (), ( mean + 2.0 * sd ) . numpy (), color = 'C0' , alpha = 0.3 ) if n_prior_samples > 0 : # plot samples from the GP prior Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs noise = ( model . noise if type ( model ) != gp . models . VariationalSparseGP else model . likelihood . variance ) cov = kernel . forward ( Xtest ) + noise . expand ( n_test ) . diag () samples = dist . MultivariateNormal ( torch . zeros ( n_test ), covariance_matrix = cov ) \\ . sample ( sample_shape = ( n_prior_samples ,)) plt . plot ( Xtest . numpy (), samples . numpy () . T , lw = 2 , alpha = 0.4 ) plt . xlim ( - 0.5 , 5.5 ) #@title Data n_samples = 100 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () #@title Plot Noisy Data X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda () Variational GP Regression \u00b6 #@title Model # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vgp = gp . models . VariationalGP ( X , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( vgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 29.33 secs #@title Losses plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Plots plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' , zorder = 2 ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' , zorder = 2 ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP' , fontsize = 20 ) plt . show () So virtually no error bars. There have been reports that error bars in regression datasets is a problem. But this is a bit ridiculous. VGP w. Uncertain Inputs \u00b6 Method 0 - Standard Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # evgp.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 34.96 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Standard Prior' , fontsize = 20 ) plt . show () Method III - Bayesian Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 35.74 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Bayesian Prior' , fontsize = 20 ) plt . show ()","title":"Egp pyro vgp"},{"location":"notebooks/egp_pyro_vgp/#preamble","text":"#@title Package Install ! pip install pyro - ppl Collecting pyro-ppl Downloading https://files.pythonhosted.org/packages/c0/77/4db4946f6b5bf0601869c7b7594def42a7197729167484e1779fff5ca0d6/pyro_ppl-1.3.1-py3-none-any.whl (520kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 522kB 6.4MB/s eta 0:00:01 Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1) Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.3) Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.38.0) Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0) Collecting pyro-api>=0.1.1 Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl Installing collected packages: pyro-api, pyro-ppl Successfully installed pyro-api-0.1.1 pyro-ppl-1.3.1 #@title Import Packages import os import time import torch from torch.nn import Parameter import pyro import pyro.contrib.gp as gp import pyro.distributions as dist from pyro.nn import PyroSample , PyroParam from scipy.cluster.vq import kmeans2 smoke_test = ( 'CI' in os . environ ) # ignore; used to check code integrity in the Pyro repo # assert pyro.__version__.startswith('0.5.1') pyro . enable_validation ( True ) # can help with debugging pyro . set_rng_seed ( 0 ) # import matplotlib.pyplot as plt # plt.style.use(['seaborn-darkgrid', 'seaborn-notebook']) import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm #@title Plot Utils # note that this helper function does three different things: # (i) plots the observed data; # (ii) plots the predictions from the learned GP after conditioning on data; # (iii) plots samples from the GP prior (with no conditioning on observed data) def plot ( plot_observed_data = False , plot_predictions = False , n_prior_samples = 0 , model = None , kernel = None , n_test = 500 ): plt . figure ( figsize = ( 12 , 6 )) if plot_observed_data : plt . plot ( X . numpy (), y . numpy (), 'kx' ) if plot_predictions : Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs # compute predictive mean and variance with torch . no_grad (): if type ( model ) == gp . models . VariationalSparseGP : mean , cov = model ( Xtest , full_cov = True ) else : try : mean , cov = model ( Xtest , full_cov = True , noiseless = False ) except : mean , cov = model ( Xtest ) sd = cov . diag () . sqrt () # standard deviation at each input point x plt . plot ( Xtest . numpy (), mean . numpy (), 'r' , lw = 2 ) # plot the mean plt . fill_between ( Xtest . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * sd ) . numpy (), ( mean + 2.0 * sd ) . numpy (), color = 'C0' , alpha = 0.3 ) if n_prior_samples > 0 : # plot samples from the GP prior Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs noise = ( model . noise if type ( model ) != gp . models . VariationalSparseGP else model . likelihood . variance ) cov = kernel . forward ( Xtest ) + noise . expand ( n_test ) . diag () samples = dist . MultivariateNormal ( torch . zeros ( n_test ), covariance_matrix = cov ) \\ . sample ( sample_shape = ( n_prior_samples ,)) plt . plot ( Xtest . numpy (), samples . numpy () . T , lw = 2 , alpha = 0.4 ) plt . xlim ( - 0.5 , 5.5 ) #@title Data n_samples = 100 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () #@title Plot Noisy Data X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda ()","title":"Preamble"},{"location":"notebooks/egp_pyro_vgp/#variational-gp-regression","text":"#@title Model # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vgp = gp . models . VariationalGP ( X , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( vgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 29.33 secs #@title Losses plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Plots plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' , zorder = 2 ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' , zorder = 2 ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP' , fontsize = 20 ) plt . show () So virtually no error bars. There have been reports that error bars in regression datasets is a problem. But this is a bit ridiculous.","title":"Variational GP Regression"},{"location":"notebooks/egp_pyro_vgp/#vgp-w-uncertain-inputs","text":"","title":"VGP w. Uncertain Inputs"},{"location":"notebooks/egp_pyro_vgp/#method-0-standard-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # evgp.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 34.96 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Standard Prior' , fontsize = 20 ) plt . show ()","title":"Method 0 - Standard Prior"},{"location":"notebooks/egp_pyro_vgp/#method-iii-bayesian-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 35.74 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Bayesian Prior' , fontsize = 20 ) plt . show ()","title":"Method III - Bayesian Prior"},{"location":"notebooks/kernel_derivatives/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Kernel Derivatives \u00b6 Resources Differentating Gaussian Processes - Andrew McHutchen Paper Idea \u00b6 Linear Operators and Stochastic Partial Differential Equations in GPR - Simo S\u00e4rkk\u00e4 - PDF Expresses derivatives of GPs as operators Demo Colab Notebook He looks at ths special case where we have a GP with a mean function zero and a covariance matrix K K defined as: $$ \\mathbb{E}[f(\\mathbf{x})f^\\top(\\mathbf{x'})] = K_{ff}(\\mathbf{x,x'}) $$ So in GP terminology: $$ f(\\mathbf(x)) \\sim \\mathcal{GP}(\\mathbf{0}, K_{ff}(\\mathbf{x,x'})) $$ We use the rulse for linear transformations of GPs to obtain the different transformations of the kernel matrix. Let's define the notation for the derivative of a kernel matrix. Let g(\\cdot) g(\\cdot) be the derivative operator on a function f(\\cdot) f(\\cdot) . So: $$ g(\\mathbf{x}) = \\mathcal{L}_x f(\\mathbf{x}) $$ So now, we want to define the cross operators between the derivative g(\\cdot) g(\\cdot) and the function f(\\cdot) f(\\cdot) . Example : He draws a distinction between the two operators with an example of how this works in practice. So let's take the linear operator \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) . This operator: acts on a scalar GP f(x) f(x) a scalar input x x a covariance function k_{ff}(x,x') k_{ff}(x,x') outputs a scalar value y y We can get the following transformations: $$ \\begin{aligned} K_{gf}(\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x}) = \\mathcal{L}_xK (\\mathbf{x,x'}) \\ K_{fg}(\\mathbf{x,x'}) &= f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} {x'} = K (\\mathbf{x,x'})\\mathcal{L} {x'} \\ K (\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} = \\mathcal{L} xK (\\mathbf{x,x'})\\mathcal{L}_{x'}^\\top \\ \\end{aligned} $$ #@title Packages import functools import jax import jax.numpy as jnp import numpy as onp from sklearn.metrics.pairwise import rbf_kernel as rbf_sklearn # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) #@title Plot Functions def plot_kernel_mat ( K ): # plot plt . figure () plt . imshow ( K , cmap = 'Reds' ) plt . title ( r '$K_ {ff} $, (rbf)' , fontsize = 20 , weight = 'bold' ) plt . tight_layout () plt . show () #@title Data def get_1d_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X_test = jnp . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) X = X [:, None ] X_test = X [:, None ] assert X . shape == ( N , 1 ) assert Y . shape == ( N ,) return X , Y , X_test def get_2d_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X1 = jnp . linspace ( - 10 , 10 , N ) X2 = jnp . linspace ( - 5 , 2 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X1 )) + jnp . exp ( X2 ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X1_test = jnp . linspace ( - 11 , 11 , N_test ) X2_test = jnp . linspace ( - 6 , 4 , N_test ) X = jnp . vstack (( X1 , X2 )) . T X_test = jnp . vstack (( X1_test , X2_test )) . T assert X . shape == ( N , 2 ) assert Y . shape == ( N ,) return X , Y , X_test # Get Data X , Y , X_test = get_1d_data ( 100 , sigma_inputs = 0.0 , sigma_obs = 0.1 , N_test = 100 ) /home/emmanuel/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Kernel Function \u00b6 #@title Kernel Functions # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) # Covariance Matrix def covariance_matrix ( kernel_func , x , y ): mapx1 = jax . vmap ( lambda x , y : kernel_func ( x , y ), in_axes = ( 0 , None ), out_axes = 0 ) mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) return mapx2 ( x , y ) RBF Kernel \u00b6 X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X [: 1 , :] test_Y = X [: 1 , :] rbf_x_sk = rbf_sklearn ( onp . array ( test_X . reshape ( 1 , - 1 )), onp . array ( test_Y . reshape ( 1 , - 1 )), gamma = 1.0 ) print ( rbf_x_sk . shape , test_X . shape ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 } gamma = 1.0 rbf_k_ = functools . partial ( rbf_kernel , params ) rbf_x = rbf_k_ ( test_X . squeeze (), test_Y . squeeze () ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x ), rbf_x_sk ) (1, 1) (1, 2) Kernel Matrix \u00b6 We defined all of our functions above with only dimensions in mind, not the number of samples or the batch size. So we need to account for that. So if we wanted to calculate the kernel matrix, we would have to loop through all of the samples and calculate the products individually, which would take a long time; especially for large amounts of data. Avoid Loops at all cost in python... Fortunately, Jax has this incredible function vmap which handles batching automatically at apparently, no extra cost. So we can write our functions to account for vectors without having to care about the batch size and then use the vmap function to essentially \"vectorize\" our functions. It essentially allows us to take a product between a matrix and a sample or two vectors of multiple samples. Let's go through an example of how we can construct our kernel matrix. We need to map all points with one vector to another. We're going to take a single sample from X' X' and take the rbf kernel between it and all of X X . So: \\text{vmap}_f(\\mathbf{X}, \\mathbf{x}) \\text{vmap}_f(\\mathbf{X}, \\mathbf{x}) where X\\in \\mathbb{R}^{N \\times D} X\\in \\mathbb{R}^{N \\times D} is a matrix and \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} is a vector. # Gram Matrix def gram ( func , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( x1 , y1 ))( y ))( x ) # map function 1 mapx1 = jax . vmap ( lambda x , y : rbf_kernel ( params , x , y ), in_axes = ( 0 , None ), out_axes = 0 ) # test the mapping x1_mapped = mapx1 ( X , X [ 0 , :]) # Check output shapes, # of dimensions assert x1_mapped . shape [ 0 ] == X . shape [ 0 ] assert jnp . ndim ( x1_mapped ) == 1 This that's good: we have an array of size N N . So we've effectively mapped all points from one array to the other. So now we can do another vector mapping which allows us to take all samples of X' X' and map them against all samples of X X . So it'll be a vmap of a vmap . Then we'll get the N\\times N N\\times N kernel matrix. mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) K = mapx2 ( X , X ) # Check output shapes, # of dimensions assert K . shape [ 0 ] == X . shape [ 0 ], X . shape [ 0 ] assert jnp . ndim ( K ) == 2 rbf_x_sk = rbf_sklearn ( X , X , 1.0 ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x_sk ), K ) So great! We now have our kernel matrix. Let's plot it and check to see if it matches the manually constructed kernel matrix. Great! We have a vectorized kernel function and we were still able to construct our functions in terms of vectors only! This is nice for me personally because I've always struggled with understanding some of the coding when trying to deal with samples/batch-sizes. Most pseudo-code is written in vector format so paper \\rightarrow \\rightarrow has always been a painful transition for me. So now, let's wrap this in a nice function so that we can finish \"wrap up\" this model. X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X . copy () #[:2, :] test_Y = X . copy () #[:2, :] rbf_x_sk = rbf_sklearn ( onp . array ( test_X ), onp . array ( test_Y ), gamma = 1.0 ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 } rbf_k_ = functools . partial ( rbf_kernel , params ) rbf_x = covariance_matrix ( rbf_k_ , test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x ), rbf_x_sk ) plot_kernel_mat ( rbf_x ) #@title Tests kx = rbf_kernel ( params , X [ 0 ], X [ 0 ]) # check, the output should be 1.0 assert kx == 1.0 , f \"Output: { kx } \" kx = rbf_kernel ( params , X [ 0 ], X [ 1 ]) # check, the output should NOT be 1.0 assert kx != 1.0 , f \"Output: { kx } \" # dk_dx = drbf_kernel(gamma, X[0], X[0]) # # check, the output should be 0.0 # assert dk_dx == 0.0, f\"Output: {dk_dx}\" # dk_dx = drbf_kernel(gamma, X[0], X[1]) # # check, the output should NOT be 0.0 # assert dk_dx != 0.0, f\"Output: {dk_dx}\" #@title Speed Test # Covariance Matrix def covariance_matrix ( kernel_func , x , y ): mapx1 = jax . vmap ( lambda x , y : kernel_func ( x , y ), in_axes = ( 0 , None ), out_axes = 0 ) mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) return mapx2 ( x , y ) def gram ( func , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( x1 , y1 ))( x ))( y ) rbf_K = functools . partial ( rbf_kernel , params ) rbf_cov = jax . jit ( functools . partial ( covariance_matrix , rbf_K )) rbf_x = rbf_cov ( test_X , test_Y ) rbf_cov2 = jax . jit ( functools . partial ( gram , rbf_K )) rbf_x2 = rbf_cov2 ( test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x ), onp . array ( rbf_x2 )) % timeit _ = rbf_cov ( test_X , test_Y ) % timeit _ = rbf_cov2 ( test_X , test_Y ) 182 \u00b5s \u00b1 20.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 167 \u00b5s \u00b1 941 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) 1. Cross-Covariance Term - 1 st Derivative \u00b6 We can calculate the cross-covariance term K_{fg}(\\mathbf{x,x}) K_{fg}(\\mathbf{x,x}) . We apply the following operation K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} For the RBF Kernel, it's this: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) Single Sample \u00b6 X , Y , X_test = get_1d_data ( 10 , sigma_obs = 0.1 ) test_X = X [ 0 : 1 , :] test_Y = X [ 1 : 2 , :] From Scratch \u00b6 def drbf_kernel_scratch ( gamma , X , Y ): dK_fg_ = onp . empty ( X . shape [ - 1 ]) constant = - 2 * gamma k_val = rbf_sklearn ( onp . array ( X ), onp . array ( Y ), gamma = gamma ) for idim in range ( X . shape [ 1 ]): x_val = X [:, idim ] - Y [:, idim ] dK_fg_ [ idim ] = constant * k_val * x_val return dK_fg_ dK_fg_ = drbf_kernel_scratch ( gamma , test_X , test_Y ) print ( dK_fg_ ) [0.01392619] Jax \u00b6 # define the cross operator K_fg(x, y), dK wrt x drbf_kernel_fg = jax . jacobian ( rbf_kernel , argnums = ( 1 )) # calculate for a single sample dK_fg = drbf_kernel_fg ( params , test_X [ 0 ,:], test_Y [ 0 ,:]) print ( dK_fg ) [0.01392619] Multiple Dimensions \u00b6 X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X [ 0 : 1 , :] test_Y = X [ 1 : 2 , :] From Scratch \u00b6 dK_fg_ = drbf_kernel_scratch ( gamma , test_X , test_Y ) print ( dK_fg_ ) [0.0173953 0.00608835] Jax \u00b6 # define the cross operator K_fg(x, y), dK wrt x drbf_kernel_fg = jax . jacobian ( rbf_kernel , argnums = ( 1 )) # calculate for a single sample dK_fg = drbf_kernel_fg ( params , test_X [ 0 ,:], test_Y [ 0 ,:]) print ( dK_fg ) [0.0173953 0.00608835] Multiple Samples (Batches) \u00b6 X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X test_Y = X From Scratch \u00b6 dK_fg_ = onp . empty (( test_X . shape [ 0 ], test_X . shape [ 0 ], test_X . shape [ 1 ])) for i in range ( test_X . shape [ 0 ]): for j in range ( test_Y . shape [ 0 ]): dK_fg_ [ i , j , :] = drbf_kernel_scratch ( gamma , onp . array ( test_X [ i , :]) . reshape ( 1 , - 1 ), onp . array ( test_Y [ j , :]) . reshape ( 1 , - 1 )) Jax \u00b6 # define the cross operator K_fg(x, y), dK wrt x drbf_kernel_fg = jax . jacobian ( rbf_kernel , argnums = ( 1 )) K_func = functools . partial ( drbf_kernel_fg , params ) dK_fg = gram ( K_func , test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( dK_fg ), dK_fg_ ) 2. Cross-Covariance Term - 2 nd Derivative \u00b6 $$ \\frac{\\partial^2 k(x,y)}{\\partial x {j 2}} = 2 \\gamma \\left[ 2\\gamma(x^j - y j) 2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) $$ From Scratch \u00b6 def d2rbf_kernel_scratch_jac ( gamma , X , Y ): d2K_fg2_ = onp . empty ( X . shape [ - 1 ]) constant = 2 * gamma k_val = rbf_sklearn ( onp . array ( X ), onp . array ( Y ), gamma = gamma ) for idim in range ( X . shape [ 1 ]): x_val = constant * ( X [:, idim ] - Y [:, idim ]) ** 2 - 1 d2K_fg2_ [ idim ] = constant * k_val * x_val return d2K_fg2_ d2K_fg2_ = onp . empty (( test_X . shape [ 0 ], test_X . shape [ 0 ], test_X . shape [ 1 ])) for i in range ( test_X . shape [ 0 ]): for j in range ( test_Y . shape [ 0 ]): d2K_fg2_ [ i , j , :] = d2rbf_kernel_scratch_jac ( gamma , onp . array ( test_X [ i , :]) . reshape ( 1 , - 1 ), onp . array ( test_Y [ j , :]) . reshape ( 1 , - 1 )) Jax \u00b6 # define the cross operator K_fg(x, y), dK wrt x dK_fg_func = jax . hessian ( rbf_kernel , argnums = ( 1 )) K_func = functools . partial ( dK_fg_func , params ) d2K_fg2 = covariance_matrix ( K_func , test_X , test_Y ) d2K_fg2 = jnp . diagonal ( d2K_fg2 , axis1 = 2 , axis2 = 3 ) d2K_fg2 . shape (10, 10, 2) onp . testing . assert_array_almost_equal ( onp . array ( d2K_fg2 ), d2K_fg2_ ) 3. Cross-Covariance Term - 2 nd Derivative (Partial Derivatives) \u00b6 $$ \\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} = 4 \\gamma^2 (x^k - y k)(x j - y^j) k(\\mathbf{x}, \\mathbf{y}) $$ From Scratch \u00b6 def d2rbf_kernel_scratch_hessian ( gamma , X , Y ): d2K_fg2_ = onp . empty (( X . shape [ - 1 ], X . shape [ - 1 ])) constant = 2 * gamma constant_sq = constant ** 2 k_val = rbf_sklearn ( onp . array ( X ), onp . array ( Y ), gamma = gamma ) for idim in range ( X . shape [ 1 ]): for jdim in range ( X . shape [ 1 ]): x_val = constant * ( 1 - constant * ( X [:, idim ] - Y [:, idim ]) * ( X [:, jdim ] - Y [:, jdim ])) # - constant d2K_fg2_ [ idim , jdim ] = k_val * x_val return d2K_fg2_ d2K_fg2_ = onp . empty (( test_X . shape [ 0 ], test_X . shape [ 0 ], test_X . shape [ 1 ], test_X . shape [ 1 ])) for i in range ( test_X . shape [ 0 ]): for j in range ( test_Y . shape [ 0 ]): d2K_fg2_ [ i , j , ... ] = d2rbf_kernel_scratch_hessian ( gamma , onp . array ( test_X [ i , :]) . reshape ( 1 , - 1 ), onp . array ( test_Y [ j , :]) . reshape ( 1 , - 1 )) Jax \u00b6 # define the cross operator K_fg(x, y), dK wrt x dK_fg_func = jax . hessian ( rbf_kernel , argnums = ( 1 )) K_func = functools . partial ( dK_fg_func , params ) d2K_fg2 = covariance_matrix ( K_func , test_X , test_Y ) d2K_fg2 . shape (10, 10, 2, 2) onp . testing . assert_array_almost_equal ( onp . array ( onp . diagonal ( d2K_fg2 , axis1 = 2 , axis2 = 3 )), jnp . diagonal ( d2K_fg2 , axis1 = 2 , axis2 = 3 )) onp . testing . assert_array_almost_equal ( onp . array ( d2K_fg2 ), d2K_fg2_ ) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-67-a1b4fece15e3> in <module> ----> 1 onp . testing . assert_array_almost_equal ( onp . array ( d2K_fg2 ) , d2K_fg2_ ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/numpy/testing/_private/utils.py in assert_array_almost_equal (x, y, decimal, err_msg, verbose) 1043 return z < 1.5 * 10.0 ** ( - decimal ) 1044 -> 1045 assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose, 1046 header = ( 'Arrays are not almost equal to %d decimals' % decimal ) , 1047 precision=decimal) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/numpy/testing/_private/utils.py in assert_array_compare (comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf) 844 verbose = verbose , header = header , 845 names=('x', 'y'), precision=precision) --> 846 raise AssertionError ( msg ) 847 except ValueError : 848 import traceback AssertionError : Arrays are not almost equal to 6 decimals Mismatched elements: 112 / 400 (28%) Max absolute difference: 4. Max relative difference: 2.40703559 x: array([[[[-2.000000e+00, 0.000000e+00], [ 0.000000e+00, -2.000000e+00]], ... y: array([[[[ 2.000000e+00, 2.000000e+00], [ 2.000000e+00, 2.000000e+00]], ...","title":"Kernel derivatives"},{"location":"notebooks/kernel_derivatives/#kernel-derivatives","text":"Resources Differentating Gaussian Processes - Andrew McHutchen","title":"Kernel Derivatives"},{"location":"notebooks/kernel_derivatives/#paper-idea","text":"Linear Operators and Stochastic Partial Differential Equations in GPR - Simo S\u00e4rkk\u00e4 - PDF Expresses derivatives of GPs as operators Demo Colab Notebook He looks at ths special case where we have a GP with a mean function zero and a covariance matrix K K defined as: $$ \\mathbb{E}[f(\\mathbf{x})f^\\top(\\mathbf{x'})] = K_{ff}(\\mathbf{x,x'}) $$ So in GP terminology: $$ f(\\mathbf(x)) \\sim \\mathcal{GP}(\\mathbf{0}, K_{ff}(\\mathbf{x,x'})) $$ We use the rulse for linear transformations of GPs to obtain the different transformations of the kernel matrix. Let's define the notation for the derivative of a kernel matrix. Let g(\\cdot) g(\\cdot) be the derivative operator on a function f(\\cdot) f(\\cdot) . So: $$ g(\\mathbf{x}) = \\mathcal{L}_x f(\\mathbf{x}) $$ So now, we want to define the cross operators between the derivative g(\\cdot) g(\\cdot) and the function f(\\cdot) f(\\cdot) . Example : He draws a distinction between the two operators with an example of how this works in practice. So let's take the linear operator \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) . This operator: acts on a scalar GP f(x) f(x) a scalar input x x a covariance function k_{ff}(x,x') k_{ff}(x,x') outputs a scalar value y y We can get the following transformations: $$ \\begin{aligned} K_{gf}(\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x}) = \\mathcal{L}_xK (\\mathbf{x,x'}) \\ K_{fg}(\\mathbf{x,x'}) &= f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} {x'} = K (\\mathbf{x,x'})\\mathcal{L} {x'} \\ K (\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} = \\mathcal{L} xK (\\mathbf{x,x'})\\mathcal{L}_{x'}^\\top \\ \\end{aligned} $$ #@title Packages import functools import jax import jax.numpy as jnp import numpy as onp from sklearn.metrics.pairwise import rbf_kernel as rbf_sklearn # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) #@title Plot Functions def plot_kernel_mat ( K ): # plot plt . figure () plt . imshow ( K , cmap = 'Reds' ) plt . title ( r '$K_ {ff} $, (rbf)' , fontsize = 20 , weight = 'bold' ) plt . tight_layout () plt . show () #@title Data def get_1d_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X_test = jnp . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) X = X [:, None ] X_test = X [:, None ] assert X . shape == ( N , 1 ) assert Y . shape == ( N ,) return X , Y , X_test def get_2d_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X1 = jnp . linspace ( - 10 , 10 , N ) X2 = jnp . linspace ( - 5 , 2 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = jnp . sin ( 1.0 * jnp . pi / 1.6 * jnp . cos ( 5 + . 5 * X1 )) + jnp . exp ( X2 ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) X1_test = jnp . linspace ( - 11 , 11 , N_test ) X2_test = jnp . linspace ( - 6 , 4 , N_test ) X = jnp . vstack (( X1 , X2 )) . T X_test = jnp . vstack (( X1_test , X2_test )) . T assert X . shape == ( N , 2 ) assert Y . shape == ( N ,) return X , Y , X_test # Get Data X , Y , X_test = get_1d_data ( 100 , sigma_inputs = 0.0 , sigma_obs = 0.1 , N_test = 100 ) /home/emmanuel/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Paper Idea"},{"location":"notebooks/kernel_derivatives/#kernel-function","text":"#@title Kernel Functions # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) # Covariance Matrix def covariance_matrix ( kernel_func , x , y ): mapx1 = jax . vmap ( lambda x , y : kernel_func ( x , y ), in_axes = ( 0 , None ), out_axes = 0 ) mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) return mapx2 ( x , y )","title":"Kernel Function"},{"location":"notebooks/kernel_derivatives/#rbf-kernel","text":"X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X [: 1 , :] test_Y = X [: 1 , :] rbf_x_sk = rbf_sklearn ( onp . array ( test_X . reshape ( 1 , - 1 )), onp . array ( test_Y . reshape ( 1 , - 1 )), gamma = 1.0 ) print ( rbf_x_sk . shape , test_X . shape ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 } gamma = 1.0 rbf_k_ = functools . partial ( rbf_kernel , params ) rbf_x = rbf_k_ ( test_X . squeeze (), test_Y . squeeze () ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x ), rbf_x_sk ) (1, 1) (1, 2)","title":"RBF Kernel"},{"location":"notebooks/kernel_derivatives/#kernel-matrix","text":"We defined all of our functions above with only dimensions in mind, not the number of samples or the batch size. So we need to account for that. So if we wanted to calculate the kernel matrix, we would have to loop through all of the samples and calculate the products individually, which would take a long time; especially for large amounts of data. Avoid Loops at all cost in python... Fortunately, Jax has this incredible function vmap which handles batching automatically at apparently, no extra cost. So we can write our functions to account for vectors without having to care about the batch size and then use the vmap function to essentially \"vectorize\" our functions. It essentially allows us to take a product between a matrix and a sample or two vectors of multiple samples. Let's go through an example of how we can construct our kernel matrix. We need to map all points with one vector to another. We're going to take a single sample from X' X' and take the rbf kernel between it and all of X X . So: \\text{vmap}_f(\\mathbf{X}, \\mathbf{x}) \\text{vmap}_f(\\mathbf{X}, \\mathbf{x}) where X\\in \\mathbb{R}^{N \\times D} X\\in \\mathbb{R}^{N \\times D} is a matrix and \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} is a vector. # Gram Matrix def gram ( func , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( x1 , y1 ))( y ))( x ) # map function 1 mapx1 = jax . vmap ( lambda x , y : rbf_kernel ( params , x , y ), in_axes = ( 0 , None ), out_axes = 0 ) # test the mapping x1_mapped = mapx1 ( X , X [ 0 , :]) # Check output shapes, # of dimensions assert x1_mapped . shape [ 0 ] == X . shape [ 0 ] assert jnp . ndim ( x1_mapped ) == 1 This that's good: we have an array of size N N . So we've effectively mapped all points from one array to the other. So now we can do another vector mapping which allows us to take all samples of X' X' and map them against all samples of X X . So it'll be a vmap of a vmap . Then we'll get the N\\times N N\\times N kernel matrix. mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) K = mapx2 ( X , X ) # Check output shapes, # of dimensions assert K . shape [ 0 ] == X . shape [ 0 ], X . shape [ 0 ] assert jnp . ndim ( K ) == 2 rbf_x_sk = rbf_sklearn ( X , X , 1.0 ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x_sk ), K ) So great! We now have our kernel matrix. Let's plot it and check to see if it matches the manually constructed kernel matrix. Great! We have a vectorized kernel function and we were still able to construct our functions in terms of vectors only! This is nice for me personally because I've always struggled with understanding some of the coding when trying to deal with samples/batch-sizes. Most pseudo-code is written in vector format so paper \\rightarrow \\rightarrow has always been a painful transition for me. So now, let's wrap this in a nice function so that we can finish \"wrap up\" this model. X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X . copy () #[:2, :] test_Y = X . copy () #[:2, :] rbf_x_sk = rbf_sklearn ( onp . array ( test_X ), onp . array ( test_Y ), gamma = 1.0 ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 } rbf_k_ = functools . partial ( rbf_kernel , params ) rbf_x = covariance_matrix ( rbf_k_ , test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x ), rbf_x_sk ) plot_kernel_mat ( rbf_x ) #@title Tests kx = rbf_kernel ( params , X [ 0 ], X [ 0 ]) # check, the output should be 1.0 assert kx == 1.0 , f \"Output: { kx } \" kx = rbf_kernel ( params , X [ 0 ], X [ 1 ]) # check, the output should NOT be 1.0 assert kx != 1.0 , f \"Output: { kx } \" # dk_dx = drbf_kernel(gamma, X[0], X[0]) # # check, the output should be 0.0 # assert dk_dx == 0.0, f\"Output: {dk_dx}\" # dk_dx = drbf_kernel(gamma, X[0], X[1]) # # check, the output should NOT be 0.0 # assert dk_dx != 0.0, f\"Output: {dk_dx}\" #@title Speed Test # Covariance Matrix def covariance_matrix ( kernel_func , x , y ): mapx1 = jax . vmap ( lambda x , y : kernel_func ( x , y ), in_axes = ( 0 , None ), out_axes = 0 ) mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) return mapx2 ( x , y ) def gram ( func , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( x1 , y1 ))( x ))( y ) rbf_K = functools . partial ( rbf_kernel , params ) rbf_cov = jax . jit ( functools . partial ( covariance_matrix , rbf_K )) rbf_x = rbf_cov ( test_X , test_Y ) rbf_cov2 = jax . jit ( functools . partial ( gram , rbf_K )) rbf_x2 = rbf_cov2 ( test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( rbf_x ), onp . array ( rbf_x2 )) % timeit _ = rbf_cov ( test_X , test_Y ) % timeit _ = rbf_cov2 ( test_X , test_Y ) 182 \u00b5s \u00b1 20.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 167 \u00b5s \u00b1 941 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)","title":"Kernel Matrix"},{"location":"notebooks/kernel_derivatives/#1-cross-covariance-term-1st-derivative","text":"We can calculate the cross-covariance term K_{fg}(\\mathbf{x,x}) K_{fg}(\\mathbf{x,x}) . We apply the following operation K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} For the RBF Kernel, it's this: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y)","title":"1. Cross-Covariance Term - 1st Derivative"},{"location":"notebooks/kernel_derivatives/#single-sample","text":"X , Y , X_test = get_1d_data ( 10 , sigma_obs = 0.1 ) test_X = X [ 0 : 1 , :] test_Y = X [ 1 : 2 , :]","title":"Single Sample"},{"location":"notebooks/kernel_derivatives/#from-scratch","text":"def drbf_kernel_scratch ( gamma , X , Y ): dK_fg_ = onp . empty ( X . shape [ - 1 ]) constant = - 2 * gamma k_val = rbf_sklearn ( onp . array ( X ), onp . array ( Y ), gamma = gamma ) for idim in range ( X . shape [ 1 ]): x_val = X [:, idim ] - Y [:, idim ] dK_fg_ [ idim ] = constant * k_val * x_val return dK_fg_ dK_fg_ = drbf_kernel_scratch ( gamma , test_X , test_Y ) print ( dK_fg_ ) [0.01392619]","title":"From Scratch"},{"location":"notebooks/kernel_derivatives/#jax","text":"# define the cross operator K_fg(x, y), dK wrt x drbf_kernel_fg = jax . jacobian ( rbf_kernel , argnums = ( 1 )) # calculate for a single sample dK_fg = drbf_kernel_fg ( params , test_X [ 0 ,:], test_Y [ 0 ,:]) print ( dK_fg ) [0.01392619]","title":"Jax"},{"location":"notebooks/kernel_derivatives/#multiple-dimensions","text":"X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X [ 0 : 1 , :] test_Y = X [ 1 : 2 , :]","title":"Multiple Dimensions"},{"location":"notebooks/kernel_derivatives/#from-scratch_1","text":"dK_fg_ = drbf_kernel_scratch ( gamma , test_X , test_Y ) print ( dK_fg_ ) [0.0173953 0.00608835]","title":"From Scratch"},{"location":"notebooks/kernel_derivatives/#jax_1","text":"# define the cross operator K_fg(x, y), dK wrt x drbf_kernel_fg = jax . jacobian ( rbf_kernel , argnums = ( 1 )) # calculate for a single sample dK_fg = drbf_kernel_fg ( params , test_X [ 0 ,:], test_Y [ 0 ,:]) print ( dK_fg ) [0.0173953 0.00608835]","title":"Jax"},{"location":"notebooks/kernel_derivatives/#multiple-samples-batches","text":"X , Y , X_test = get_2d_data ( 10 , sigma_obs = 0.1 ) test_X = X test_Y = X","title":"Multiple Samples (Batches)"},{"location":"notebooks/kernel_derivatives/#from-scratch_2","text":"dK_fg_ = onp . empty (( test_X . shape [ 0 ], test_X . shape [ 0 ], test_X . shape [ 1 ])) for i in range ( test_X . shape [ 0 ]): for j in range ( test_Y . shape [ 0 ]): dK_fg_ [ i , j , :] = drbf_kernel_scratch ( gamma , onp . array ( test_X [ i , :]) . reshape ( 1 , - 1 ), onp . array ( test_Y [ j , :]) . reshape ( 1 , - 1 ))","title":"From Scratch"},{"location":"notebooks/kernel_derivatives/#jax_2","text":"# define the cross operator K_fg(x, y), dK wrt x drbf_kernel_fg = jax . jacobian ( rbf_kernel , argnums = ( 1 )) K_func = functools . partial ( drbf_kernel_fg , params ) dK_fg = gram ( K_func , test_X , test_Y ) onp . testing . assert_array_almost_equal ( onp . array ( dK_fg ), dK_fg_ )","title":"Jax"},{"location":"notebooks/kernel_derivatives/#2-cross-covariance-term-2nd-derivative","text":"$$ \\frac{\\partial^2 k(x,y)}{\\partial x {j 2}} = 2 \\gamma \\left[ 2\\gamma(x^j - y j) 2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) $$","title":"2. Cross-Covariance Term - 2nd Derivative"},{"location":"notebooks/kernel_derivatives/#from-scratch_3","text":"def d2rbf_kernel_scratch_jac ( gamma , X , Y ): d2K_fg2_ = onp . empty ( X . shape [ - 1 ]) constant = 2 * gamma k_val = rbf_sklearn ( onp . array ( X ), onp . array ( Y ), gamma = gamma ) for idim in range ( X . shape [ 1 ]): x_val = constant * ( X [:, idim ] - Y [:, idim ]) ** 2 - 1 d2K_fg2_ [ idim ] = constant * k_val * x_val return d2K_fg2_ d2K_fg2_ = onp . empty (( test_X . shape [ 0 ], test_X . shape [ 0 ], test_X . shape [ 1 ])) for i in range ( test_X . shape [ 0 ]): for j in range ( test_Y . shape [ 0 ]): d2K_fg2_ [ i , j , :] = d2rbf_kernel_scratch_jac ( gamma , onp . array ( test_X [ i , :]) . reshape ( 1 , - 1 ), onp . array ( test_Y [ j , :]) . reshape ( 1 , - 1 ))","title":"From Scratch"},{"location":"notebooks/kernel_derivatives/#jax_3","text":"# define the cross operator K_fg(x, y), dK wrt x dK_fg_func = jax . hessian ( rbf_kernel , argnums = ( 1 )) K_func = functools . partial ( dK_fg_func , params ) d2K_fg2 = covariance_matrix ( K_func , test_X , test_Y ) d2K_fg2 = jnp . diagonal ( d2K_fg2 , axis1 = 2 , axis2 = 3 ) d2K_fg2 . shape (10, 10, 2) onp . testing . assert_array_almost_equal ( onp . array ( d2K_fg2 ), d2K_fg2_ )","title":"Jax"},{"location":"notebooks/kernel_derivatives/#3-cross-covariance-term-2nd-derivative-partial-derivatives","text":"$$ \\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} = 4 \\gamma^2 (x^k - y k)(x j - y^j) k(\\mathbf{x}, \\mathbf{y}) $$","title":"3. Cross-Covariance Term - 2nd Derivative (Partial Derivatives)"},{"location":"notebooks/kernel_derivatives/#from-scratch_4","text":"def d2rbf_kernel_scratch_hessian ( gamma , X , Y ): d2K_fg2_ = onp . empty (( X . shape [ - 1 ], X . shape [ - 1 ])) constant = 2 * gamma constant_sq = constant ** 2 k_val = rbf_sklearn ( onp . array ( X ), onp . array ( Y ), gamma = gamma ) for idim in range ( X . shape [ 1 ]): for jdim in range ( X . shape [ 1 ]): x_val = constant * ( 1 - constant * ( X [:, idim ] - Y [:, idim ]) * ( X [:, jdim ] - Y [:, jdim ])) # - constant d2K_fg2_ [ idim , jdim ] = k_val * x_val return d2K_fg2_ d2K_fg2_ = onp . empty (( test_X . shape [ 0 ], test_X . shape [ 0 ], test_X . shape [ 1 ], test_X . shape [ 1 ])) for i in range ( test_X . shape [ 0 ]): for j in range ( test_Y . shape [ 0 ]): d2K_fg2_ [ i , j , ... ] = d2rbf_kernel_scratch_hessian ( gamma , onp . array ( test_X [ i , :]) . reshape ( 1 , - 1 ), onp . array ( test_Y [ j , :]) . reshape ( 1 , - 1 ))","title":"From Scratch"},{"location":"notebooks/kernel_derivatives/#jax_4","text":"# define the cross operator K_fg(x, y), dK wrt x dK_fg_func = jax . hessian ( rbf_kernel , argnums = ( 1 )) K_func = functools . partial ( dK_fg_func , params ) d2K_fg2 = covariance_matrix ( K_func , test_X , test_Y ) d2K_fg2 . shape (10, 10, 2, 2) onp . testing . assert_array_almost_equal ( onp . array ( onp . diagonal ( d2K_fg2 , axis1 = 2 , axis2 = 3 )), jnp . diagonal ( d2K_fg2 , axis1 = 2 , axis2 = 3 )) onp . testing . assert_array_almost_equal ( onp . array ( d2K_fg2 ), d2K_fg2_ ) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-67-a1b4fece15e3> in <module> ----> 1 onp . testing . assert_array_almost_equal ( onp . array ( d2K_fg2 ) , d2K_fg2_ ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/numpy/testing/_private/utils.py in assert_array_almost_equal (x, y, decimal, err_msg, verbose) 1043 return z < 1.5 * 10.0 ** ( - decimal ) 1044 -> 1045 assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose, 1046 header = ( 'Arrays are not almost equal to %d decimals' % decimal ) , 1047 precision=decimal) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/numpy/testing/_private/utils.py in assert_array_compare (comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf) 844 verbose = verbose , header = header , 845 names=('x', 'y'), precision=precision) --> 846 raise AssertionError ( msg ) 847 except ValueError : 848 import traceback AssertionError : Arrays are not almost equal to 6 decimals Mismatched elements: 112 / 400 (28%) Max absolute difference: 4. Max relative difference: 2.40703559 x: array([[[[-2.000000e+00, 0.000000e+00], [ 0.000000e+00, -2.000000e+00]], ... y: array([[[[ 2.000000e+00, 2.000000e+00], [ 2.000000e+00, 2.000000e+00]], ...","title":"Jax"},{"location":"notebooks/numpyro_egp_mcmc/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exact GP w. MCMC \u00b6 In this example we show how to use NUTS to sample from the posterior over the hyperparameters of a gaussian process. Source : Numpyro Example import sys from pyprojroot import here sys . path . append ( str ( here ())) from dataclasses import dataclass import time import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers import jax from jax import vmap import jax.numpy as np import jax.random as random from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 num_chains = 1 num_warmup = 1_000 num_samples = 1_000 device = 'cpu' numpyro . set_platform ( args . device ) numpyro . set_host_device_count ( args . num_chains ) # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) GP Model \u00b6 # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = jnp . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * jnp . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * jnp . eye ( X . shape [ 0 ]) return k @jax . jit def model ( X , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = jnp . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y , ) Inference \u00b6 # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True , ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( \" \\n MCMC elapsed time:\" , time . time () - start ) return mcmc . get_samples () Training \u00b6 # do inference rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) samples = run_inference ( model , args , rng_key , X , y ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [42:33<00:00, 1.28s/it, 1023 steps of size 4.98e-06. acc. prob=0.95] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1960118272.00 256.13 1960118528.00 1960118528.00 1960118528.00 0.50 1.00 kernel_noise 0.00 0.00 0.00 0.00 0.00 nan nan kernel_var 2.63 0.00 2.63 2.63 2.63 6.13 1.00 Number of divergences: 0 MCMC elapsed time: 2557.5514323711395 /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/numpyro/diagnostics.py:172: RuntimeWarning: invalid value encountered in true_divide rho_k = 1. - (var_within - gamma_k_c.mean(axis=0)) / var_estimator Predictions \u00b6 def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = jnp . linalg . inv ( k_XX ) K = k_pp - jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , jnp . transpose ( k_pX ))) sigma_noise = jnp . sqrt ( jnp . clip ( jnp . diag ( K ), a_min = 0.0 )) * jax . random . normal ( rng_key , X_test . shape [: 1 ] ) mean = jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ \"kernel_var\" ], samples [ \"kernel_length\" ], samples [ \"kernel_noise\" ], ) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , y , X_test , var , length , noise ) )( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) Results \u00b6 # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , y , \"kx\" ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = \"lightblue\" ) # plot mean prediction ax . plot ( X_test , mean_prediction , \"blue\" , ls = \"solid\" , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) # plt.savefig(\"numpyro_gp_plot.png\") plt . tight_layout () Experiment \u00b6 GP Model - Uncertain Inputs \u00b6 def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"Numpyro egp mcmc"},{"location":"notebooks/numpyro_egp_mcmc/#exact-gp-w-mcmc","text":"In this example we show how to use NUTS to sample from the posterior over the hyperparameters of a gaussian process. Source : Numpyro Example import sys from pyprojroot import here sys . path . append ( str ( here ())) from dataclasses import dataclass import time import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers import jax from jax import vmap import jax.numpy as np import jax.random as random from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Exact GP w. MCMC"},{"location":"notebooks/numpyro_egp_mcmc/#data","text":"@dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 num_chains = 1 num_warmup = 1_000 num_samples = 1_000 device = 'cpu' numpyro . set_platform ( args . device ) numpyro . set_host_device_count ( args . num_chains ) # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , )","title":"Data"},{"location":"notebooks/numpyro_egp_mcmc/#gp-model","text":"# squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = jnp . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * jnp . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * jnp . eye ( X . shape [ 0 ]) return k @jax . jit def model ( X , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = jnp . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y , )","title":"GP Model"},{"location":"notebooks/numpyro_egp_mcmc/#inference","text":"# helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True , ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( \" \\n MCMC elapsed time:\" , time . time () - start ) return mcmc . get_samples ()","title":"Inference"},{"location":"notebooks/numpyro_egp_mcmc/#training","text":"# do inference rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) samples = run_inference ( model , args , rng_key , X , y ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [42:33<00:00, 1.28s/it, 1023 steps of size 4.98e-06. acc. prob=0.95] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1960118272.00 256.13 1960118528.00 1960118528.00 1960118528.00 0.50 1.00 kernel_noise 0.00 0.00 0.00 0.00 0.00 nan nan kernel_var 2.63 0.00 2.63 2.63 2.63 6.13 1.00 Number of divergences: 0 MCMC elapsed time: 2557.5514323711395 /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/numpyro/diagnostics.py:172: RuntimeWarning: invalid value encountered in true_divide rho_k = 1. - (var_within - gamma_k_c.mean(axis=0)) / var_estimator","title":"Training"},{"location":"notebooks/numpyro_egp_mcmc/#predictions","text":"def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = jnp . linalg . inv ( k_XX ) K = k_pp - jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , jnp . transpose ( k_pX ))) sigma_noise = jnp . sqrt ( jnp . clip ( jnp . diag ( K ), a_min = 0.0 )) * jax . random . normal ( rng_key , X_test . shape [: 1 ] ) mean = jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ \"kernel_var\" ], samples [ \"kernel_length\" ], samples [ \"kernel_noise\" ], ) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , y , X_test , var , length , noise ) )( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 )","title":"Predictions"},{"location":"notebooks/numpyro_egp_mcmc/#results","text":"# make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , y , \"kx\" ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = \"lightblue\" ) # plot mean prediction ax . plot ( X_test , mean_prediction , \"blue\" , ls = \"solid\" , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) # plt.savefig(\"numpyro_gp_plot.png\") plt . tight_layout ()","title":"Results"},{"location":"notebooks/numpyro_egp_mcmc/#experiment","text":"","title":"Experiment"},{"location":"notebooks/numpyro_egp_mcmc/#gp-model-uncertain-inputs","text":"def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"GP Model - Uncertain Inputs"},{"location":"talks/2020_kermes/","text":"KERMES Meetup 2020 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020 Input Uncertainty Propagation in Gaussian Process Regression Models \u00b6 Resources \u00b6 Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website . Slides \u00b6","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes/#kermes-meetup-2020","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes/#input-uncertainty-propagation-in-gaussian-process-regression-models","text":"","title":"Input Uncertainty Propagation in Gaussian Process Regression Models"},{"location":"talks/2020_kermes/#resources","text":"Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website .","title":"Resources"},{"location":"talks/2020_kermes/#slides","text":"","title":"Slides"}]}