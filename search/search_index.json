{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Input Uncertainty for Gaussian Processes \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Documentation: jejjohnson.github.io/uncertain_gps Repo: github.com/jejjohnson/uncertain_gps A graphical model of a GP algorithm with the addition of uncertainty component for the input. This repository is home to my studies on uncertain inputs for Gaussian processes. Gaussian processes are a kernel Bayesian framework that is known to generalize well for small datasets and also offers predictive mean and predictive variance estimates. It is one of the most complete models that model uncertainty. Demo showing the error bars for a standard GP predictive variance and an augmented predictive variance estimate using Taylor expansions. In this repository, I am interested in exploring the capabilities and limits with Gaussian process regression algorithms when handling noisy inputs. Input uncertainty is often not talked about in the machine learning literature, so I will be exploring this in great detail for my thesis. Taylor Approximation \u00b6 \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} We can approximate the predictive mean and variance equations via a Taylor expansion which adds a corrective term w.r.t. the derivative of the function and the known variance. This assumes we know the variance and we don't modify the predictive mean of the learned GP function. 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation. Unscented Transforms \u00b6 The predictive mean and variance using the Unscented transformation. Variational Inference \u00b6 TODO Monte Carlo Estimation \u00b6 1D Example Posterior Approximation Training Exact Prior Known Input Error My Resources \u00b6 Literature Review \u00b6 I have gone through most of the relevant works related to noisy inputs in the context of Gaussian processes. It is very extensive and it also offers some literature that is relevant but may not explicitly mentioned uncertain inputs in the paper. Documentation \u00b6 I have some documentation which has all of my personal notes and derivations related to GPs and noisy inputs. Some highlights include the Taylor approximation, moment matching and variational inference. GP Model Zoo \u00b6 I have documented and try to keep up with some of the latest Gaussian process literature in my repository.","title":"Home"},{"location":"#input-uncertainty-for-gaussian-processes","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Documentation: jejjohnson.github.io/uncertain_gps Repo: github.com/jejjohnson/uncertain_gps A graphical model of a GP algorithm with the addition of uncertainty component for the input. This repository is home to my studies on uncertain inputs for Gaussian processes. Gaussian processes are a kernel Bayesian framework that is known to generalize well for small datasets and also offers predictive mean and predictive variance estimates. It is one of the most complete models that model uncertainty. Demo showing the error bars for a standard GP predictive variance and an augmented predictive variance estimate using Taylor expansions. In this repository, I am interested in exploring the capabilities and limits with Gaussian process regression algorithms when handling noisy inputs. Input uncertainty is often not talked about in the machine learning literature, so I will be exploring this in great detail for my thesis.","title":"Input Uncertainty for Gaussian Processes"},{"location":"#taylor-approximation","text":"\\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} We can approximate the predictive mean and variance equations via a Taylor expansion which adds a corrective term w.r.t. the derivative of the function and the known variance. This assumes we know the variance and we don't modify the predictive mean of the learned GP function. 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation.","title":"Taylor Approximation"},{"location":"#unscented-transforms","text":"The predictive mean and variance using the Unscented transformation.","title":"Unscented Transforms"},{"location":"#variational-inference","text":"TODO","title":"Variational Inference"},{"location":"#monte-carlo-estimation","text":"1D Example Posterior Approximation Training Exact Prior Known Input Error","title":"Monte Carlo Estimation"},{"location":"#my-resources","text":"","title":"My Resources"},{"location":"#literature-review","text":"I have gone through most of the relevant works related to noisy inputs in the context of Gaussian processes. It is very extensive and it also offers some literature that is relevant but may not explicitly mentioned uncertain inputs in the paper.","title":"Literature Review"},{"location":"#documentation","text":"I have some documentation which has all of my personal notes and derivations related to GPs and noisy inputs. Some highlights include the Taylor approximation, moment matching and variational inference.","title":"Documentation"},{"location":"#gp-model-zoo","text":"I have documented and try to keep up with some of the latest Gaussian process literature in my repository.","title":"GP Model Zoo"},{"location":"MonteCarlo/demo/","text":"MCMC eGP \u00b6 TLDR I did a quick experiment where I look at how we can impact the error bars when doing a fully Bayesian GP (i.e. GP with MCMC inference). I have 3 cases where I use no prior on the inputs, where I use a modest prior on the inputs, and one where I use the exact known prior on the inputs. The results are definitely different than what I'm used to because I actually trained the GP knowing the priors. The error bars were reduced which I guess makes sense. TODO : Do the MCMC where we approximate the posterior when we trained the GP with uncertain inputs. Posterior Approximation Training Exact Prior Known Input Error Experiment \u00b6 Code Blocks Install ! pip install jax jaxlib numpyro Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.62) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.42) Collecting numpyro \u001b[?25l Downloading https://files.pythonhosted.org/packages/b8/58/54e914bb6d8ee9196f8dbf28b81057fea81871fc171dbee03b790336d0c5/numpyro-0.2.4-py3-none-any.whl (159kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.5MB/s \u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.2.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.3) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.38.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.12.0) \u001b[31mERROR: numpyro 0.2.4 has requirement jax==0.1.57, but you'll have jax 0.1.62 which is incompatible.\u001b[0m \u001b[31mERROR: numpyro 0.2.4 has requirement jaxlib==0.1.37, but you'll have jaxlib 0.1.42 which is incompatible.\u001b[0m Installing collected packages: numpyro Successfully installed numpyro-0.2.4 Imports #@title packages import time import numpy as onp from dataclasses import dataclass import jax from jax import vmap import jax.numpy as np import jax.random as random import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS import matplotlib import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline Data #@title Data def get_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = np . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = np . sin ( 1.0 * np . pi / 1.6 * np . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= np . mean ( Y ) Y /= np . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = np . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) return X , Y , X_test GP Model \u00b6 #@title GP Model # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = np . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * np . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * np . eye ( X . shape [ 0 ]) return k def model ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15 * np.ones((Xmu.shape[0],)))) X = Xmu # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( ' \\n MCMC elapsed time:' , time . time () - start ) return mcmc . get_samples () # do GP prediction for a given set of hyperparameters. this makes use of the well-known # formula for gaussian process predictions def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = np . linalg . inv ( k_XX ) K = k_pp - np . matmul ( k_pX , np . matmul ( K_xx_inv , np . transpose ( k_pX ))) sigma_noise = np . sqrt ( np . clip ( np . diag ( K ), a_min = 0. )) * jax . random . normal ( rng_key , X_test . shape [: 1 ]) mean = np . matmul ( k_pX , np . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise Experiment \u00b6 @dataclass class args : num_data = 60 num_warmup = 100 num_chains = 1 num_samples = 1_000 device = 'cpu' sigma_inputs = 0.3 sigma_obs = 0.05 numpyro . set_platform ( args . device ) X , Y , X_test = get_data ( args . num_data , sigma_inputs = args . sigma_inputs , sigma_obs = args . sigma_obs ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( model , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:11<00:00, 96.81it/s, 7 steps of size 6.48e-01. acc. prob=0.94] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1.97 0.23 1.97 1.58 2.34 650.87 1.00 kernel_noise 0.04 0.01 0.04 0.02 0.05 637.46 1.00 kernel_var 1.15 0.65 0.98 0.34 1.97 563.69 1.00 Number of divergences: 0 MCMC elapsed time: 14.073462963104248 Predictions \u00b6 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] GP Model - Uncertain Inputs \u00b6 def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] Results \u00b6 Exact Known Input Error Prior","title":"MCMC eGP"},{"location":"MonteCarlo/demo/#mcmc-egp","text":"TLDR I did a quick experiment where I look at how we can impact the error bars when doing a fully Bayesian GP (i.e. GP with MCMC inference). I have 3 cases where I use no prior on the inputs, where I use a modest prior on the inputs, and one where I use the exact known prior on the inputs. The results are definitely different than what I'm used to because I actually trained the GP knowing the priors. The error bars were reduced which I guess makes sense. TODO : Do the MCMC where we approximate the posterior when we trained the GP with uncertain inputs. Posterior Approximation Training Exact Prior Known Input Error","title":"MCMC eGP"},{"location":"MonteCarlo/demo/#experiment","text":"Code Blocks Install ! pip install jax jaxlib numpyro Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.62) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.42) Collecting numpyro \u001b[?25l Downloading https://files.pythonhosted.org/packages/b8/58/54e914bb6d8ee9196f8dbf28b81057fea81871fc171dbee03b790336d0c5/numpyro-0.2.4-py3-none-any.whl (159kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.5MB/s \u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.2.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.3) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.38.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.12.0) \u001b[31mERROR: numpyro 0.2.4 has requirement jax==0.1.57, but you'll have jax 0.1.62 which is incompatible.\u001b[0m \u001b[31mERROR: numpyro 0.2.4 has requirement jaxlib==0.1.37, but you'll have jaxlib 0.1.42 which is incompatible.\u001b[0m Installing collected packages: numpyro Successfully installed numpyro-0.2.4 Imports #@title packages import time import numpy as onp from dataclasses import dataclass import jax from jax import vmap import jax.numpy as np import jax.random as random import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS import matplotlib import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline Data #@title Data def get_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = np . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = np . sin ( 1.0 * np . pi / 1.6 * np . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= np . mean ( Y ) Y /= np . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = np . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) return X , Y , X_test","title":"Experiment"},{"location":"MonteCarlo/demo/#gp-model","text":"#@title GP Model # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = np . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * np . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * np . eye ( X . shape [ 0 ]) return k def model ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15 * np.ones((Xmu.shape[0],)))) X = Xmu # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( ' \\n MCMC elapsed time:' , time . time () - start ) return mcmc . get_samples () # do GP prediction for a given set of hyperparameters. this makes use of the well-known # formula for gaussian process predictions def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = np . linalg . inv ( k_XX ) K = k_pp - np . matmul ( k_pX , np . matmul ( K_xx_inv , np . transpose ( k_pX ))) sigma_noise = np . sqrt ( np . clip ( np . diag ( K ), a_min = 0. )) * jax . random . normal ( rng_key , X_test . shape [: 1 ]) mean = np . matmul ( k_pX , np . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise","title":"GP Model"},{"location":"MonteCarlo/demo/#experiment_1","text":"@dataclass class args : num_data = 60 num_warmup = 100 num_chains = 1 num_samples = 1_000 device = 'cpu' sigma_inputs = 0.3 sigma_obs = 0.05 numpyro . set_platform ( args . device ) X , Y , X_test = get_data ( args . num_data , sigma_inputs = args . sigma_inputs , sigma_obs = args . sigma_obs ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( model , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:11<00:00, 96.81it/s, 7 steps of size 6.48e-01. acc. prob=0.94] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1.97 0.23 1.97 1.58 2.34 650.87 1.00 kernel_noise 0.04 0.01 0.04 0.02 0.05 637.46 1.00 kernel_var 1.15 0.65 0.98 0.34 1.97 563.69 1.00 Number of divergences: 0 MCMC elapsed time: 14.073462963104248","title":"Experiment"},{"location":"MonteCarlo/demo/#predictions","text":"# do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"Predictions"},{"location":"MonteCarlo/demo/#gp-model-uncertain-inputs","text":"def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"GP Model - Uncertain Inputs"},{"location":"MonteCarlo/demo/#results","text":"Exact Known Input Error Prior","title":"Results"},{"location":"Notes/approximate/","text":"Error Propagation in Gaussian Transformations \u00b6 We're in the setting where we have some inputs that come from a distribution \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we have some outputs y\\in\\mathbb{R} y\\in\\mathbb{R} . Typically we have some function f(\\mathbf) f(\\mathbf) that maps the points f:\\mathbb{R}^D \\rightarrow \\mathbb{R} f:\\mathbb{R}^D \\rightarrow \\mathbb{R} . So like we do with GPs, we have the following function\" y=f(\\mathbf{x}) y=f(\\mathbf{x}) Change of Variables \u00b6 To have this full mapping, we would need to use the change of variables method because we are doing a transformation between of probability distributions. \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} This is very expensive and can be very difficult depending upon the function. Conditional Gaussian Distribution \u00b6 Alternatively, if we know that f() f() is described by a Gaussian distribution, then we can find the joint distribution between \\mathbf{x},y \\mathbf{x},y . This can be described as: \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) Taylor Expansions \u00b6 by using a Taylor series expansion. Let \\mu_x \\mu_x be the true inputs and we perturb these by some noise \\delta_x \\delta_x which is described by a normal distribution \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) . So we can write an expression for the function f(\\mathbf{x}) f(\\mathbf{x}) . \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} where \\nabla_x f(\\mu_x) \\nabla_x f(\\mu_x) is the jacobian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x , \\nabla_{xx}f(\\mu_x) \\nabla_{xx}f(\\mu_x) is the hessian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x and e_i e_i is a ones vector (essentially the trace of a full matrix). Joint Distribution \u00b6 So, we want \\tilde{f}(\\mathbf{x}) \\tilde{f}(\\mathbf{x}) which is a joint distribution of \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} . But as we said, this is difficult to compute so we do a Taylor approximation to this function \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} . But we still want the full joint distribution , ideally Gaussian. So that would mean we at least need the expectation and the covariance. \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] Derivation Mean Function The mean function is the easiest to derive. We can just take the expectation of the first two terms and we'll see why all the higher order terms disappear. \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} Covariance Function The covariance function is a bit harder. But again, the final expression is quite simple. \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} So now we can have a full expression for the joint distribution with the Taylor expansion. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) where: \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} Joint Distribution for Additive Noise \u00b6 So in this setting, we are closer to what we typically use for a GP model. y = f(\\mathbf{x}) + \\epsilon y = f(\\mathbf{x}) + \\epsilon where we have noisy inputs with additive Gaussian noise \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we assume additive Gaussian noise for the outputs \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) . We want a joint distribution of \\mathbf{x},y \\mathbf{x},y . So using the same sequences of steps as above, we actually get a very similar joint distribution, just with an additional term. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) where \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} So if we want to make predictions with our new model, we will have the final equation as: \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"Gaussian Approximations"},{"location":"Notes/approximate/#error-propagation-in-gaussian-transformations","text":"We're in the setting where we have some inputs that come from a distribution \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we have some outputs y\\in\\mathbb{R} y\\in\\mathbb{R} . Typically we have some function f(\\mathbf) f(\\mathbf) that maps the points f:\\mathbb{R}^D \\rightarrow \\mathbb{R} f:\\mathbb{R}^D \\rightarrow \\mathbb{R} . So like we do with GPs, we have the following function\" y=f(\\mathbf{x}) y=f(\\mathbf{x})","title":"Error Propagation in Gaussian Transformations"},{"location":"Notes/approximate/#change-of-variables","text":"To have this full mapping, we would need to use the change of variables method because we are doing a transformation between of probability distributions. \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} This is very expensive and can be very difficult depending upon the function.","title":"Change of Variables"},{"location":"Notes/approximate/#conditional-gaussian-distribution","text":"Alternatively, if we know that f() f() is described by a Gaussian distribution, then we can find the joint distribution between \\mathbf{x},y \\mathbf{x},y . This can be described as: \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Conditional Gaussian Distribution"},{"location":"Notes/approximate/#taylor-expansions","text":"by using a Taylor series expansion. Let \\mu_x \\mu_x be the true inputs and we perturb these by some noise \\delta_x \\delta_x which is described by a normal distribution \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) . So we can write an expression for the function f(\\mathbf{x}) f(\\mathbf{x}) . \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} where \\nabla_x f(\\mu_x) \\nabla_x f(\\mu_x) is the jacobian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x , \\nabla_{xx}f(\\mu_x) \\nabla_{xx}f(\\mu_x) is the hessian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x and e_i e_i is a ones vector (essentially the trace of a full matrix).","title":"Taylor Expansions"},{"location":"Notes/approximate/#joint-distribution","text":"So, we want \\tilde{f}(\\mathbf{x}) \\tilde{f}(\\mathbf{x}) which is a joint distribution of \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} . But as we said, this is difficult to compute so we do a Taylor approximation to this function \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} . But we still want the full joint distribution , ideally Gaussian. So that would mean we at least need the expectation and the covariance. \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] Derivation Mean Function The mean function is the easiest to derive. We can just take the expectation of the first two terms and we'll see why all the higher order terms disappear. \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} Covariance Function The covariance function is a bit harder. But again, the final expression is quite simple. \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} So now we can have a full expression for the joint distribution with the Taylor expansion. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) where: \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned}","title":"Joint Distribution"},{"location":"Notes/approximate/#joint-distribution-for-additive-noise","text":"So in this setting, we are closer to what we typically use for a GP model. y = f(\\mathbf{x}) + \\epsilon y = f(\\mathbf{x}) + \\epsilon where we have noisy inputs with additive Gaussian noise \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we assume additive Gaussian noise for the outputs \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) . We want a joint distribution of \\mathbf{x},y \\mathbf{x},y . So using the same sequences of steps as above, we actually get a very similar joint distribution, just with an additional term. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) where \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} So if we want to make predictions with our new model, we will have the final equation as: \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"Joint Distribution for Additive Noise"},{"location":"Notes/basics/","text":"Gaussian Process Basics \u00b6 Data \u00b6 Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise. Model \u00b6 Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} We specify a mean function, m m and a covariance function k k . Likelihood , {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} which describes the dataset Marginal Likelihood , {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} . Posterior \u00b6 First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F}) Deterministic Inputs \u00b6 In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) Probabilistic Inputs \u00b6 In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} Variational GP Models \u00b6 Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Sparse GP Models \u00b6 Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Basics"},{"location":"Notes/basics/#gaussian-process-basics","text":"","title":"Gaussian Process Basics"},{"location":"Notes/basics/#data","text":"Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise.","title":"Data"},{"location":"Notes/basics/#model","text":"Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{{\\color{blue}{ p(y| f, \\mathbf{X}) } \\,\\color{darkgreen} {p(f)}}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} {\\color{darkgreen} {p(f) = \\mathcal{GP}(m, k)}} We specify a mean function, m m and a covariance function k k . Likelihood , {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} {\\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) }} which describes the dataset Marginal Likelihood , {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} {\\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df }} Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} .","title":"Model"},{"location":"Notes/basics/#posterior","text":"First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F})","title":"Posterior"},{"location":"Notes/basics/#deterministic-inputs","text":"In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I})","title":"Deterministic Inputs"},{"location":"Notes/basics/#probabilistic-inputs","text":"In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned}","title":"Probabilistic Inputs"},{"location":"Notes/basics/#variational-gp-models","text":"Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right)","title":"Variational GP Models"},{"location":"Notes/basics/#sparse-gp-models","text":"Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Sparse GP Models"},{"location":"Notes/error_propagation/","text":"Error Propagation \u00b6 Taylor Series Expansion Law of Error Propagation Proof: Mean Function Proof: Variance Function Resources Taylor Series Expansion \u00b6 A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) Law of Error Propagation \u00b6 This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} Proof: Mean Function \u00b6 Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} Proof: Variance Function \u00b6 Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. Resources \u00b6 Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Error Propagation"},{"location":"Notes/error_propagation/#error-propagation","text":"Taylor Series Expansion Law of Error Propagation Proof: Mean Function Proof: Variance Function Resources","title":"Error Propagation"},{"location":"Notes/error_propagation/#taylor-series-expansion","text":"A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right)","title":"Taylor Series Expansion"},{"location":"Notes/error_propagation/#law-of-error-propagation","text":"This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top}","title":"Law of Error Propagation"},{"location":"Notes/error_propagation/#proof-mean-function","text":"Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned}","title":"Proof: Mean Function"},{"location":"Notes/error_propagation/#proof-variance-function","text":"Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself.","title":"Proof: Variance Function"},{"location":"Notes/error_propagation/#resources","text":"Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Resources"},{"location":"Notes/extrapolation/","text":"Extrapolation \u00b6 One interesting problem that is related to uncertainty is how well this extrapolates for unseen regions (whether it is spatially or temporally). Smart Combination of Kernels \u00b6 Related to what @Gus and I were talking about earlier. I think a GP would be better at extrapolating if we use kernel that is better defined for extrapolation. For example, we can just use a combination of well defined kernels with actual thought into the trends we expect like they did for the classic Mauna dataset: ( sklearn demo ) , ( tensorflow demo ). They ended up using a combination of RBF + RBF * ExpSinSquared + RQ + RBF + White with arbitrary scalers in front of all of the terms. I have no clue how in the world they came up with that\u2026 Fourier Basis Functions \u00b6 In general any GP that is approximated with Fourier Basis functions will be good at finding periodic trends. For example the Sparse Spectrum GP (SSGP) (as you mentioned in your paper Gustau, pg68) (original paper for SSGP) is related to the Fourier features method but GP-ed. Resources Gustau Paper Gustaus paper summarizing GPs in the context of Earth science. Briefly mentions SSGPs. Presentation A lab that does a lot of work on spectral kernels did a nice presentation with a decent summary of the literature. SSGP Paper original paper on SSGP by Titsias. SSGP w. Uncertain Inputs Paper Paper detailing how one can do the Taylor expansion to propagate the errors. VSSGP Paper | Poster | ICML Presentation Original paper by Yarin Gal and presentation about the variational approach to the SSGP method. The original author did the code in Theano so I don't recommend you use it nor try to understand it. It's ugly... Variational Fourier Features for GPs - Paper Original paper by James Hensman et al. using variational fourier features for GPs. To be honest, I'm still not 100% sure what the difference is between this method and the method by Yarin Gal... According to the paper they say: \"Gal and Turner proposed variation inference in a sparse spectrum model that is derived form a GP model. Our work aims to directly approximate the posterior of the try models using a variational representation.\" I still don't get it. Spectral Mixture Kernel \u00b6 Or a kernel that was designed to include all the parameters necessary to find patterns like the spectral mixture kernel ( paper , pg 2, eq. 12). Lastly, just use a neural network and slap a GP layer at the end and let the data tell you the pattern. Resources Original Paper original paper with the derivation of the spectral mixture kernel. Paper some extenions to Multi-Task / Multi-Output / Multi-Fidelity problems Thesis of Vincent Dutordoi (chapter 6, pg. 67) Does the derivation for the spectral mixture kernel extension to incorporate uncertain inputs. Uses exact moment matching (which is an expensive operation...) but in theory, it should better propagate the uncertain inputs. It's also a really good thesis that explains how to propagate uncertain inputs through Gaussian processes (chapter 4, pg. 42). Warning : the equations are nasty... GPyTorch Demo The fastest implementation you'll find on the internet. Large Scale \u00b6 All of the above methods can be trained using sparse and variational methods to make them scale to really large amounts of data. Although I think they kind of fail when you have a lot of dimensions. No more than 3-5 is what I\u2019ve seen in the literature. And typically they only do toy examples with 1D data. Error Propagation \u00b6 Now to couple them with the Taylor expansion would be interesting. As long as the methods have a kernel that is differentiable (wrt inputs), then it should be possible.","title":"Extrapolation"},{"location":"Notes/extrapolation/#extrapolation","text":"One interesting problem that is related to uncertainty is how well this extrapolates for unseen regions (whether it is spatially or temporally).","title":"Extrapolation"},{"location":"Notes/extrapolation/#smart-combination-of-kernels","text":"Related to what @Gus and I were talking about earlier. I think a GP would be better at extrapolating if we use kernel that is better defined for extrapolation. For example, we can just use a combination of well defined kernels with actual thought into the trends we expect like they did for the classic Mauna dataset: ( sklearn demo ) , ( tensorflow demo ). They ended up using a combination of RBF + RBF * ExpSinSquared + RQ + RBF + White with arbitrary scalers in front of all of the terms. I have no clue how in the world they came up with that\u2026","title":"Smart Combination of Kernels"},{"location":"Notes/extrapolation/#fourier-basis-functions","text":"In general any GP that is approximated with Fourier Basis functions will be good at finding periodic trends. For example the Sparse Spectrum GP (SSGP) (as you mentioned in your paper Gustau, pg68) (original paper for SSGP) is related to the Fourier features method but GP-ed. Resources Gustau Paper Gustaus paper summarizing GPs in the context of Earth science. Briefly mentions SSGPs. Presentation A lab that does a lot of work on spectral kernels did a nice presentation with a decent summary of the literature. SSGP Paper original paper on SSGP by Titsias. SSGP w. Uncertain Inputs Paper Paper detailing how one can do the Taylor expansion to propagate the errors. VSSGP Paper | Poster | ICML Presentation Original paper by Yarin Gal and presentation about the variational approach to the SSGP method. The original author did the code in Theano so I don't recommend you use it nor try to understand it. It's ugly... Variational Fourier Features for GPs - Paper Original paper by James Hensman et al. using variational fourier features for GPs. To be honest, I'm still not 100% sure what the difference is between this method and the method by Yarin Gal... According to the paper they say: \"Gal and Turner proposed variation inference in a sparse spectrum model that is derived form a GP model. Our work aims to directly approximate the posterior of the try models using a variational representation.\" I still don't get it.","title":"Fourier Basis Functions"},{"location":"Notes/extrapolation/#spectral-mixture-kernel","text":"Or a kernel that was designed to include all the parameters necessary to find patterns like the spectral mixture kernel ( paper , pg 2, eq. 12). Lastly, just use a neural network and slap a GP layer at the end and let the data tell you the pattern. Resources Original Paper original paper with the derivation of the spectral mixture kernel. Paper some extenions to Multi-Task / Multi-Output / Multi-Fidelity problems Thesis of Vincent Dutordoi (chapter 6, pg. 67) Does the derivation for the spectral mixture kernel extension to incorporate uncertain inputs. Uses exact moment matching (which is an expensive operation...) but in theory, it should better propagate the uncertain inputs. It's also a really good thesis that explains how to propagate uncertain inputs through Gaussian processes (chapter 4, pg. 42). Warning : the equations are nasty... GPyTorch Demo The fastest implementation you'll find on the internet.","title":"Spectral Mixture Kernel"},{"location":"Notes/extrapolation/#large-scale","text":"All of the above methods can be trained using sparse and variational methods to make them scale to really large amounts of data. Although I think they kind of fail when you have a lot of dimensions. No more than 3-5 is what I\u2019ve seen in the literature. And typically they only do toy examples with 1D data.","title":"Large Scale"},{"location":"Notes/extrapolation/#error-propagation","text":"Now to couple them with the Taylor expansion would be interesting. As long as the methods have a kernel that is differentiable (wrt inputs), then it should be possible.","title":"Error Propagation"},{"location":"Notes/literature/","text":"Uncertain Inputs in Gaussian Processe \u00b6 Motivation \u00b6 This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs. Algorithms \u00b6 Error-In-Variables Regression \u00b6 This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ] Monte Carlo Sampling \u00b6 So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this. Taylor Expansion \u00b6 Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ] Moment Matching \u00b6 This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end) Covariance Functions \u00b6 Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code Iterative \u00b6 Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR Linearized (Unscented) Approximation \u00b6 This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach. Heteroscedastic Likelihood Models \u00b6 Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code Latent Variable Models \u00b6 Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016) Latent Covariates \u00b6 Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) Variational Strategies \u00b6 Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Appendix \u00b6 Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Literature \u00b6 Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform SigmaPointTransform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel Toolboxes \u00b6 Emukit Connecting Concepts \u00b6 Moment Matching \u00b6 Derivatives of GPs \u00b6 Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018) Extended Kalman Filter \u00b6 This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code Uncertain Inputs in other ML fields \u00b6 Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation Key Equations \u00b6 Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Literature"},{"location":"Notes/literature/#uncertain-inputs-in-gaussian-processe","text":"","title":"Uncertain Inputs in Gaussian Processe"},{"location":"Notes/literature/#motivation","text":"This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs.","title":"Motivation"},{"location":"Notes/literature/#algorithms","text":"","title":"Algorithms"},{"location":"Notes/literature/#error-in-variables-regression","text":"This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ]","title":"Error-In-Variables Regression"},{"location":"Notes/literature/#monte-carlo-sampling","text":"So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this.","title":"Monte Carlo Sampling"},{"location":"Notes/literature/#taylor-expansion","text":"Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ]","title":"Taylor Expansion"},{"location":"Notes/literature/#moment-matching","text":"This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end)","title":"Moment Matching"},{"location":"Notes/literature/#covariance-functions","text":"Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code","title":"Covariance Functions"},{"location":"Notes/literature/#iterative","text":"Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR","title":"Iterative"},{"location":"Notes/literature/#linearized-unscented-approximation","text":"This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach.","title":"Linearized (Unscented) Approximation"},{"location":"Notes/literature/#heteroscedastic-likelihood-models","text":"Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code","title":"Heteroscedastic Likelihood Models"},{"location":"Notes/literature/#latent-variable-models","text":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016)","title":"Latent Variable Models"},{"location":"Notes/literature/#latent-covariates","text":"Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019)","title":"Latent Covariates"},{"location":"Notes/literature/#variational-strategies","text":"Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Variational Strategies"},{"location":"Notes/literature/#appendix","text":"","title":"Appendix"},{"location":"Notes/literature/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"Notes/literature/#literature","text":"Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform SigmaPointTransform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel","title":"Literature"},{"location":"Notes/literature/#toolboxes","text":"Emukit","title":"Toolboxes"},{"location":"Notes/literature/#connecting-concepts","text":"","title":"Connecting Concepts"},{"location":"Notes/literature/#moment-matching_1","text":"","title":"Moment Matching"},{"location":"Notes/literature/#derivatives-of-gps","text":"Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018)","title":"Derivatives of GPs"},{"location":"Notes/literature/#extended-kalman-filter","text":"This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code","title":"Extended Kalman Filter"},{"location":"Notes/literature/#uncertain-inputs-in-other-ml-fields","text":"Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation","title":"Uncertain Inputs in other ML fields"},{"location":"Notes/literature/#key-equations","text":"Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Key Equations"},{"location":"Notes/mm/","text":"Moment-Matching \u00b6","title":"Moment-Matching"},{"location":"Notes/mm/#moment-matching","text":"","title":"Moment-Matching"},{"location":"Notes/next/","text":"Next Steps \u00b6 So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen: 1. Apply these algorithms to different problems (other than dynamical systems) \u00b6 It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it? 2. Improve the Kernel Expectation Calculations \u00b6 So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there. 3. Think about the problem differently \u00b6 An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too! 4. Think about pragmatic solutions \u00b6 Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free. 5. Figure Out how to extend it to Deep GPs \u00b6 So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"Next Steps"},{"location":"Notes/next/#next-steps","text":"So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen:","title":"Next Steps"},{"location":"Notes/next/#1-apply-these-algorithms-to-different-problems-other-than-dynamical-systems","text":"It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it?","title":"1. Apply these algorithms to different problems (other than dynamical systems)"},{"location":"Notes/next/#2-improve-the-kernel-expectation-calculations","text":"So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there.","title":"2. Improve the Kernel Expectation Calculations"},{"location":"Notes/next/#3-think-about-the-problem-differently","text":"An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too!","title":"3. Think about the problem differently"},{"location":"Notes/next/#4-think-about-pragmatic-solutions","text":"Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free.","title":"4. Think about pragmatic solutions"},{"location":"Notes/next/#5-figure-out-how-to-extend-it-to-deep-gps","text":"So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"5. Figure Out how to extend it to Deep GPs"},{"location":"Notes/software/","text":"Software \u00b6 GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch. GPy \u00b6 This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems. My Model Zoo \u00b6 Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github GPFlow \u00b6 This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting. Pyro \u00b6 This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research. My Model Zoo \u00b6 Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab GPyTorch \u00b6 This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications. Summary \u00b6 Algorithms Implemented \u00b6 Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Software"},{"location":"Notes/software/#software","text":"GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch.","title":"Software"},{"location":"Notes/software/#gpy","text":"This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems.","title":"GPy"},{"location":"Notes/software/#my-model-zoo","text":"Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github","title":"My Model Zoo"},{"location":"Notes/software/#gpflow","text":"This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting.","title":"GPFlow"},{"location":"Notes/software/#pyro","text":"This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research.","title":"Pyro"},{"location":"Notes/software/#my-model-zoo_1","text":"Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab","title":"My Model Zoo"},{"location":"Notes/software/#gpytorch","text":"This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications.","title":"GPyTorch"},{"location":"Notes/software/#summary","text":"","title":"Summary"},{"location":"Notes/software/#algorithms-implemented","text":"Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Algorithms Implemented"},{"location":"Notes/stochastic/","text":"","title":"Stochastic"},{"location":"Notes/taylor/","text":"Linearization (Taylor Expansions) \u00b6 Conditional Gaussian Distributions I: Additive Noise Model ( x,f x,f ) Other GP Methods II: Non-Additive Noise Model III: Quadratic Approximation Literature Supplementary Error Propagation Fubini's Theorem Law of Iterated Expecations Conditional Variance Analytical Moments \u00b6 The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the Mean Function \u00b6 \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function \u00b6 The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} Taylor Approximation \u00b6 We will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Linearized Predictive Mean and Variance \u00b6 \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term. Conditional Gaussian Distributions \u00b6 I: Additive Noise Model ( x,f x,f ) \u00b6 This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP} {-1}K_{*} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top . Other GP Methods \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{ z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{ } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. II: Non-Additive Noise Model \u00b6 III: Quadratic Approximation \u00b6 Literature \u00b6 Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference. Supplementary \u00b6 Error Propagation \u00b6 To see more about error propagation and the relation to the mean and variance, see here . Fubini's Theorem \u00b6 Law of Iterated Expecations \u00b6 Conditional Variance \u00b6","title":"Linearization (Taylor Expansions)"},{"location":"Notes/taylor/#linearization-taylor-expansions","text":"Conditional Gaussian Distributions I: Additive Noise Model ( x,f x,f ) Other GP Methods II: Non-Additive Noise Model III: Quadratic Approximation Literature Supplementary Error Propagation Fubini's Theorem Law of Iterated Expecations Conditional Variance","title":"Linearization (Taylor Expansions)"},{"location":"Notes/taylor/#analytical-moments","text":"The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the","title":"Analytical Moments"},{"location":"Notes/taylor/#mean-function","text":"\\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned}","title":"Mean Function"},{"location":"Notes/taylor/#variance-function","text":"The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned}","title":"Variance Function"},{"location":"Notes/taylor/#taylor-approximation","text":"We will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned}","title":"Taylor Approximation"},{"location":"Notes/taylor/#linearized-predictive-mean-and-variance","text":"\\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.","title":"Linearized Predictive Mean and Variance"},{"location":"Notes/taylor/#conditional-gaussian-distributions","text":"","title":"Conditional Gaussian Distributions"},{"location":"Notes/taylor/#i-additive-noise-model-xfxf","text":"This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP} {-1}K_{*} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"I: Additive Noise Model (x,fx,f)"},{"location":"Notes/taylor/#other-gp-methods","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{ z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{ } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Other GP Methods"},{"location":"Notes/taylor/#ii-non-additive-noise-model","text":"","title":"II: Non-Additive Noise Model"},{"location":"Notes/taylor/#iii-quadratic-approximation","text":"","title":"III: Quadratic Approximation"},{"location":"Notes/taylor/#literature","text":"Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Literature"},{"location":"Notes/taylor/#supplementary","text":"","title":"Supplementary"},{"location":"Notes/taylor/#error-propagation","text":"To see more about error propagation and the relation to the mean and variance, see here .","title":"Error Propagation"},{"location":"Notes/taylor/#fubinis-theorem","text":"","title":"Fubini's Theorem"},{"location":"Notes/taylor/#law-of-iterated-expecations","text":"","title":"Law of Iterated Expecations"},{"location":"Notes/taylor/#conditional-variance","text":"","title":"Conditional Variance"},{"location":"Notes/uncertainty/","text":"What is Uncertainty? \u00b6","title":"What is Uncertainty?"},{"location":"Notes/uncertainty/#what-is-uncertainty","text":"","title":"What is Uncertainty?"},{"location":"Notes/vi/","text":"Uncertain Inputs GPs - Variational Strategies \u00b6 This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Posterior Approximations Variational GP Model with Latent Inputs Evidence Lower Bound (ELBO) Uncertain Inputs Case I - Strong Prior Case II - Regularized Strong Prior Case III - Prior with Openness Case IV - Bonus, Conservative Freedom Resources Important Papers Summary Thesis Talks Posterior Approximations \u00b6 What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. Variational GP Model with Latent Inputs \u00b6 Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) Evidence Lower Bound (ELBO) \u00b6 In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. Uncertain Inputs \u00b6 So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties. Case I - Strong Prior \u00b6 Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes. Case II - Regularized Strong Prior \u00b6 This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) Case III - Prior with Openness \u00b6 The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. Case IV - Bonus, Conservative Freedom \u00b6 Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options Resources \u00b6 Important Papers \u00b6 These are the important papers that helped me understand what was going on throughout the learning process. Summary Thesis \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Talks \u00b6 Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Uncertain Inputs GPs - Variational Strategies"},{"location":"Notes/vi/#uncertain-inputs-gps-variational-strategies","text":"This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Posterior Approximations Variational GP Model with Latent Inputs Evidence Lower Bound (ELBO) Uncertain Inputs Case I - Strong Prior Case II - Regularized Strong Prior Case III - Prior with Openness Case IV - Bonus, Conservative Freedom Resources Important Papers Summary Thesis Talks","title":"Uncertain Inputs GPs - Variational Strategies"},{"location":"Notes/vi/#posterior-approximations","text":"What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more.","title":"Posterior Approximations"},{"location":"Notes/vi/#variational-gp-model-with-latent-inputs","text":"Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right)","title":"Variational GP Model with Latent Inputs"},{"location":"Notes/vi/#evidence-lower-bound-elbo","text":"In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.","title":"Evidence Lower Bound (ELBO)"},{"location":"Notes/vi/#uncertain-inputs","text":"So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.","title":"Uncertain Inputs"},{"location":"Notes/vi/#case-i-strong-prior","text":"Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.","title":"Case I - Strong Prior"},{"location":"Notes/vi/#case-ii-regularized-strong-prior","text":"This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right)","title":"Case II - Regularized Strong Prior"},{"location":"Notes/vi/#case-iii-prior-with-openness","text":"The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise.","title":"Case III - Prior with Openness"},{"location":"Notes/vi/#case-iv-bonus-conservative-freedom","text":"Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options","title":"Case IV - Bonus, Conservative Freedom"},{"location":"Notes/vi/#resources","text":"","title":"Resources"},{"location":"Notes/vi/#important-papers","text":"These are the important papers that helped me understand what was going on throughout the learning process.","title":"Important Papers"},{"location":"Notes/vi/#summary-thesis","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Summary Thesis"},{"location":"Notes/vi/#talks","text":"Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Talks"},{"location":"Taylor/2.0_linearized_gp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linearized GP \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 73.05it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34] Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () 1 st Order Taylor Expansion \u00b6 # =========================== # 1st Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Predictive Mean mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) pred_grad_f = jax . jit ( jax . vmap ( jax . grad ( mean_f , argnums = ( 0 )), in_axes = ( 0 ))) dmu_y = pred_grad_f ( Xtest ) # Predictive Variance var_correction_o1 = jnp . diag ( jnp . dot ( jnp . dot ( dmu_y , input_cov ), dmu_y . T )) # Uncertainty mu_y = mu_y_o1 uncertainty_t1 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt . show () 2 nd Order Taylor Expansion \u00b6 # =========================== # 2nd Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Mean function correction mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_df2 = jax . jit ( jax . vmap ( jax . hessian ( mean_f , argnums = ( 0 )), in_axes = ( 0 )) ) d2mu_y = mu_df2 ( Xtest ) d2mu_y = jnp . dot ( d2mu_y , input_cov ) mu_y_o2 = 0.5 * jnp . trace ( d2mu_y , axis1 = 1 , axis2 = 2 ) mu_y = mu_y_o1 + mu_y_o2 # Variance Function Correction var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_f = functools . partial ( predictive_variance , params , gp_priors , X , y ) pred_var_f = jax . jit ( jax . vmap ( jax . hessian ( var_f , argnums = ( 0 )), in_axes = ( 0 , None , None )) ) d2var_y2 = pred_var_f ( Xtest , True , False ) d2var_y2 = jnp . dot ( d2var_y2 , input_cov ) var_correction_o2 = jnp . trace ( d2var_y2 , axis1 = 1 , axis2 = 2 ) # Uncertainty uncertainty_t2 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze () + var_correction_o2 . squeeze () ) plt . plot ( mu_y_o1 + mu_y_o2 ) plt . plot ( mu_y_o1 ) plt . show () fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_2o.png\") plt . show () Results \u00b6 Standard GP Error Bars \u00b6 Taylor Expansion (1 st Order) \u00b6 Taylor Expansion (2 nd Order) \u00b6 Difference in 1 st and 2 nd Order \u00b6 fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), # label=r\"Predictive Mean\", color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"blue\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.4 , color = \"yellow\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_diff.png\") plt . show ()","title":"2.0 linearized gp"},{"location":"Taylor/2.0_linearized_gp/#linearized-gp","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Linearized GP"},{"location":"Taylor/2.0_linearized_gp/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2","title":"Imports"},{"location":"Taylor/2.0_linearized_gp/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Data"},{"location":"Taylor/2.0_linearized_gp/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"Taylor/2.0_linearized_gp/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"Taylor/2.0_linearized_gp/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:06<00:00, 73.05it/s, length_scale=1.20, likelihood_noise=0.16, var_f=1.07, Loss=21.34]","title":"Training"},{"location":"Taylor/2.0_linearized_gp/#predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Predictions"},{"location":"Taylor/2.0_linearized_gp/#1st-order-taylor-expansion","text":"# =========================== # 1st Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Predictive Mean mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) pred_grad_f = jax . jit ( jax . vmap ( jax . grad ( mean_f , argnums = ( 0 )), in_axes = ( 0 ))) dmu_y = pred_grad_f ( Xtest ) # Predictive Variance var_correction_o1 = jnp . diag ( jnp . dot ( jnp . dot ( dmu_y , input_cov ), dmu_y . T )) # Uncertainty mu_y = mu_y_o1 uncertainty_t1 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt . show ()","title":"1st Order Taylor Expansion"},{"location":"Taylor/2.0_linearized_gp/#2nd-order-taylor-expansion","text":"# =========================== # 2nd Order Taylor Expansion # =========================== mu_y_o1 = predictive_mean ( params , gp_priors , X , y , Xtest ) # Mean function correction mean_f = functools . partial ( predictive_mean , params , gp_priors , X , y ) mu_df2 = jax . jit ( jax . vmap ( jax . hessian ( mean_f , argnums = ( 0 )), in_axes = ( 0 )) ) d2mu_y = mu_df2 ( Xtest ) d2mu_y = jnp . dot ( d2mu_y , input_cov ) mu_y_o2 = 0.5 * jnp . trace ( d2mu_y , axis1 = 1 , axis2 = 2 ) mu_y = mu_y_o1 + mu_y_o2 # Variance Function Correction var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_f = functools . partial ( predictive_variance , params , gp_priors , X , y ) pred_var_f = jax . jit ( jax . vmap ( jax . hessian ( var_f , argnums = ( 0 )), in_axes = ( 0 , None , None )) ) d2var_y2 = pred_var_f ( Xtest , True , False ) d2var_y2 = jnp . dot ( d2var_y2 , input_cov ) var_correction_o2 = jnp . trace ( d2var_y2 , axis1 = 1 , axis2 = 2 ) # Uncertainty uncertainty_t2 = 1.96 * jnp . sqrt ( var_y . squeeze () + var_correction_o1 . squeeze () + var_correction_o2 . squeeze () ) plt . plot ( mu_y_o1 + mu_y_o2 ) plt . plot ( mu_y_o1 ) plt . show () fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_2o.png\") plt . show ()","title":"2nd Order Taylor Expansion"},{"location":"Taylor/2.0_linearized_gp/#results","text":"","title":"Results"},{"location":"Taylor/2.0_linearized_gp/#standard-gp-error-bars","text":"","title":"Standard GP Error Bars"},{"location":"Taylor/2.0_linearized_gp/#taylor-expansion-1st-order","text":"","title":"Taylor Expansion (1st Order)"},{"location":"Taylor/2.0_linearized_gp/#taylor-expansion-2nd-order","text":"","title":"Taylor Expansion (2nd Order)"},{"location":"Taylor/2.0_linearized_gp/#difference-in-1st-and-2nd-order","text":"fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), # label=r\"Predictive Mean\", color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t1 , mu_y . squeeze () - uncertainty_t1 , alpha = 0.3 , color = \"blue\" , label = f \"Predictive Std Taylor 1st Order\" , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty_t2 , mu_y . squeeze () - uncertainty_t2 , alpha = 0.4 , color = \"yellow\" , label = f \"Predictive Std Taylor 2nd Order\" , ) ax . set_ylim ([ - 3.5 , 3.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_diff.png\") plt . show ()","title":"Difference in 1st and 2nd Order"},{"location":"Taylor/error_propagation/","text":"Error Propagation \u00b6 Taylor Series Expansion \u00b6 A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) Law of Error Propagation \u00b6 This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} Proof: Mean Function \u00b6 Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} Proof: Variance Function \u00b6 Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. Resources \u00b6 Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Error Propagation"},{"location":"Taylor/error_propagation/#error-propagation","text":"","title":"Error Propagation"},{"location":"Taylor/error_propagation/#taylor-series-expansion","text":"A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right)","title":"Taylor Series Expansion"},{"location":"Taylor/error_propagation/#law-of-error-propagation","text":"This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top}","title":"Law of Error Propagation"},{"location":"Taylor/error_propagation/#proof-mean-function","text":"Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned}","title":"Proof: Mean Function"},{"location":"Taylor/error_propagation/#proof-variance-function","text":"Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself.","title":"Proof: Variance Function"},{"location":"Taylor/error_propagation/#resources","text":"Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Resources"},{"location":"Taylor/scratch/","text":"","title":"Scratch"},{"location":"Taylor/taylor/","text":"Linearized GP \u00b6 Recall the GP formulation: y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) Recall the posterior formulas: \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} In the case where we have uncertain inputs \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) , this function needs to be modified in order to accommodate the uncertainty The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the posterior distribution wrt the inputs. Mean Function \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} Taylor Approximation \u00b6 Taking complete expectations can be very expensive because we need to take the expectation wrt to the inputs through nonlinear terms such as the kernel functions and their inverses. So, we will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Note To see more about error propagation and the relation to the mean and variance, see here . So expanding these equations gives us the following: \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term. Examples \u00b6 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation. Sparse GPs \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} So the new predictive functions will be: \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. Literature \u00b6 Theory Bayesian Filtering and Smoothing - Smio Sarkka ()- Book Modelling and Control of Dynamic Systems Using GP Models - Jus Kocijan () - Book Applied to Gaussian Processes Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Linearized GP"},{"location":"Taylor/taylor/#linearized-gp","text":"Recall the GP formulation: y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) Recall the posterior formulas: \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} In the case where we have uncertain inputs \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) , this function needs to be modified in order to accommodate the uncertainty The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the posterior distribution wrt the inputs. Mean Function \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned}","title":"Linearized GP"},{"location":"Taylor/taylor/#taylor-approximation","text":"Taking complete expectations can be very expensive because we need to take the expectation wrt to the inputs through nonlinear terms such as the kernel functions and their inverses. So, we will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Note To see more about error propagation and the relation to the mean and variance, see here . So expanding these equations gives us the following: \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.","title":"Taylor Approximation"},{"location":"Taylor/taylor/#examples","text":"1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation.","title":"Examples"},{"location":"Taylor/taylor/#sparse-gps","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} So the new predictive functions will be: \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Sparse GPs"},{"location":"Taylor/taylor/#literature","text":"Theory Bayesian Filtering and Smoothing - Smio Sarkka ()- Book Modelling and Control of Dynamic Systems Using GP Models - Jus Kocijan () - Book Applied to Gaussian Processes Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Literature"},{"location":"notebooks/1.0_gp_basics/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Gaussian Process Regression \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) Data \u00b6 def get_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 1 , 1 , N ) Y = X + 0.2 * jnp . power ( X , 3.0 ) + 0.5 * jnp . power ( 0.5 + X , 2.0 ) * jnp . sin ( 4.0 * X ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = jnp . linspace ( - 1.2 , 1.2 , N_test ) return X [:, None ], Y [:, None ], X_test [:, None ], None logger . setLevel ( logging . INFO ) , y , Xtest , ytest = get_data () print ( X . shape , y . shape ) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . show () (30, 1) (30, 1) Gaussian Process \u00b6 Model: GP Prior \u00b6 Parameters : X, Y, $\\theta= $ (Likelihood Parameters, Kernel Parameters) Compute the Kernel Matrix Compute the Mean function Sample from the Multivariate Normal Distribution Kernel Function \u00b6 k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) <span><span class=\"MathJax_Preview\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right)</span><script type=\"math/tex\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) # ARD Kernel @jax . jit def ard_kernel ( params , x , y ): # divide by the length scale x = x / params [ 'length_scale' ] y = y / params [ 'length_scale' ] # return the ard kernel return params [ 'var_f' ] * jnp . exp ( - sqeuclidean_distance ( x , y ) ) params = { 'var_f' : 1.0 , 'sigma' : 1.0 } Kernel Matrix \u00b6 # Gram Matrix def gram ( func , params , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ))( y ))( x ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] # test_X = x_plot[0, :] cov_f = functools . partial ( gram , rbf_kernel ) K_ = cov_f ( params , Xtest , X ) print ( K_ . shape ) K_ = cov_f ( params , X , Xtest ) print ( K_ . shape ) (400, 30) (30, 400) Mean Function \u00b6 Honestly, I never work with mean functions. I always assume a zero-mean function and that's it. I don't really know anyone who works with mean functions either. I've seen it used in deep Gaussian processes but I have no expertise in which mean functions to use. So, we'll follow the community standard for now: zero mean function def zero_mean ( x ): return jnp . zeros ( x . shape [ 0 ]) 3. Compute Model \u00b6 Now we have all of the components to make our GP prior function. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) # define mean function mu_f = zero_mean # define covariance function params = { 'gamma' : 1.0 , 'var_f' : 1.0 } cov_f = functools . partial ( gram , rbf_kernel ) mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = X [ 0 , :]) Checks \u00b6 So I'm still getting used to the vmap . So in theory, this function should work for a vector \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} and for a batch of samples X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} # checks - 1 vector (D) test_X = X [ 0 , :] . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) print ( mu_x . shape , cov_x . shape ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 # checks - 1 vector with batch size (NxD) test_X = X . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (1,) (1, 1) Woot! Success! So now we can technically sample from this GP prior distribution. 4. Sampling from GP Prior \u00b6 from scipy.stats import multivariate_normal as scio_mvn Scipy \u00b6 # checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # check outputs assert mu_x . shape == ( n_samples ,) assert cov_x . shape == ( n_samples , n_samples ) # draw random samples from distribution n_functions = 10 y_samples = stats . multivariate_normal . rvs ( mean = mu_x , cov = cov_x , size = n_functions ) assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-182-7c0d1026349a> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # check outputs <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' Note - The positive semi-definite error \u00b6 I believe that's due to the diagonals being off. Normally we add something called jitter. This allows the matrix to be positive semi-definite. mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ]) # draw random samples from distribution n_functions = 10 y_samples = scio_mvn . rvs ( mean = mu_x , cov = cov_x_ , size = n_functions ) print ( y_samples . shape ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-183-12ab200a8e55> in <module> ----> 1 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 2 3 # make it semi-positive definite with jitter 4 jitter = 1e-6 5 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ] ) <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' And now we don't have that message. This is a small thing but it's super important and can lead to errors in the optimization if not addressed. Jax \u00b6 # checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 key = jax . random . PRNGKey ( 0 ) y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) # check assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-184-9b06421fbf35> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # make it semi-positive definite with jitter <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' 4. Posterior \u00b6 Conditioned on the observations, can we make predictions. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def cholesky_factorization ( K , Y ): # cho factor the cholesky logger . debug ( f \"ChoFactor: K { K . shape } \" ) L = jax . scipy . linalg . cho_factor ( K , lower = True ) logger . debug ( f \"Output, L: { L [ 0 ] . shape } , { L [ 1 ] } \" ) # weights logger . debug ( f \"Input, ChoSolve(L, Y): { L [ 0 ] . shape , Y . shape } \" ) weights = jax . scipy . linalg . cho_solve ( L , Y ) logger . debug ( f \"Output, alpha: { weights . shape } \" ) return L , weights jitter = 1e-6 def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] test_X = Xtest [ 0 , :] prior_funcs = ( mu_f , cov_f ) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , X_new = test_X ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (1,) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (1,),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (1, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (1,), KxX @ alpha: (1, 30) @ (30, 1) DEBUG:root:Output, mu_y: (1, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 1) DEBUG:root:Output, v: (30, 1) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[1])). DEBUG:root:Output, Kxx: (1, 1) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (1, 1), v:(30, 1) DEBUG:root:Output: cov(x*, x*) - (1, 1) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 1) DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling _where for args (ShapedArray(bool[1,1]), ShapedArray(float32[1,1]), ShapedArray(float32[1,1])). DEBUG:root:Output, diag(Kxx): (1,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (1, 30), (30, 30), (1, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[1,30]), ShapedArray(float32[1,30])). DEBUG:root:Output, var_y: (1,), 0.03,0.03 (1, 1) (1, 1) (1,) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,400])). DEBUG:root:Output, v: (30, 400) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling _where for args (ShapedArray(bool[400,400]), ShapedArray(float32[400,400]), ShapedArray(float32[400,400])). DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.00,0.03 (400, 1) (400, 400) (400,) plt . plot ( var_y . squeeze ()) [<matplotlib.lines.Line2D at 0x7f06943d8a30>] test_X . shape , mu_y . shape ((1,), (400, 1)) uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) [<matplotlib.lines.Line2D at 0x7f06741adfd0>] 5. Loss - Log-Likelihood \u00b6 From Scratch \u00b6 # @jax.jit def cholesky_factorization ( K , Y ): # cho factor the cholesky L = jax . scipy . linalg . cho_factor ( K ) # weights weights = jax . scipy . linalg . cho_solve ( L , Y ) return L , weights def nll_scratch ( gp_priors , params , X , Y ) -> float : ( mu_func , cov_func ) = gp_priors # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # y_mean = jnp.mean(Y, axis=1) # Y -= y_mean # print(mu_x.shape, Kxx.shape) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== # print(f\"Problem:\", X.shape, Y.shape, Kxx.shape) # print(f\"Y: {Y.shape}, Kxx: {Kxx.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ 'likelihood_noise' ] + 1e-5 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) # L = jax.scipy.linalg.cholesky(Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]), lower=True) # alpha = jax.scipy.linalg.solve_triangular(L.T, jax.scipy.linalg.solve_triangular(L, y, lower=True)) # print(f\"Y: {Y.shape}, alpha: {alpha.shape}\") logging . debug ( f \"Y: { Y . shape } ,alpha: { alpha . shape } \" ) log_likelihood = - 0.5 * jnp . einsum ( \"ik,ik->k\" , Y , alpha ) #* jnp.dot(Y.T, alpha) # log_likelihood -= jnp . sum ( jnp . log ( jnp . diag ( L ))) log_likelihood -= ( Kxx . shape [ 0 ] / 2 ) * jnp . log ( 2 * jnp . pi ) # log_likelihood -= jnp.sum(-0.5 * np.log(2 * 3.1415) - params['var_f']**2) return - jnp . sum ( log_likelihood ) # # print(L.shape, alpha.shape) # # cho factor the cholesky # K_gp = Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]) # # L = jax.scipy.linalg.cholesky(K_gp) # # assert np.testing.assert_array_almost_equal(K_gp, L @ L.T), # return jax.scipy.stats.multivariate_normal.logpdf(Y, mean=mu_x, cov=K_gp) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = nll_scratch ( prior_funcs , params , X , y ) print ( nll ) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Y: (30, 1),alpha:(30, 1) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _where for args (ShapedArray(bool[30,30]), ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). 54.878708 Auto-Batching with VMAP \u00b6 nll_scratch_vec = jax . vmap ( nll_scratch , in_axes = ( None , None , 0 , 0 )) nll = nll_scratch_vec ( params , prior_funcs , X , y [:, None ]) print ( nll . sum ()) (1,) (1, 1) Y: (1,), alpha: (1,) -209.98637 Refactor - Built-in Function \u00b6 It turns out that the jax library already has the logpdf for the multivariate_normal already implemented. So we can just use that. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def marginal_likelihood ( prior_params , params , Xtrain , Ytrain ): # unpack params ( mu_func , cov_func ) = prior_params # ========================== # 1. GP Prior # ========================== mu_x = mu_f ( Xtrain ) logging . debug ( f \"mu: { mu_x . shape } \" ) Kxx = cov_f ( params , Xtrain , Xtrain ) logging . debug ( f \"Kxx: { Kxx . shape } \" ) # print(\"MLL (GPPR):\", Xtrain.shape, Ytrain.shape) # mu_x, Kxx = gp_prior(params, mu_f=mu_func, cov_f=cov_func , x=Xtrain) # =========================== # 2. GP Likelihood # =========================== K_gp = Kxx + ( params [ 'likelihood_noise' ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]) logging . debug ( f \"K_gp: { K_gp . shape } \" ) # print(\"MLL (GPLL):\", Xtrain.shape, Ytrain.shape) # =========================== # 3. Built-in GP Likelihood # =========================== logging . debug ( f \"Input: { Ytrain . squeeze () . shape } , mu: { mu_x . shape } , K: { K_gp . shape } \" ) log_prob = jax . scipy . stats . multivariate_normal . logpdf ( Ytrain . squeeze (), mean = jnp . zeros ( Ytrain . shape [ 0 ]), cov = K_gp ) logging . debug ( f \"LogProb: { log_prob . shape } \" ) nll = jnp . sum ( log_prob ) return - nll logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = marginal_likelihood ( prior_funcs , params , X , y ) print ( nll ) DEBUG:root:mu: (30,) DEBUG:root:Kxx: (30, 30) DEBUG:root:K_gp: (30, 30) DEBUG:root:Input: (30,), mu: (30,), K: (30, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30]), ShapedArray(float32[30])). DEBUG:root:LogProb: () 54.937344 logger . setLevel ( logging . INFO ) % timeit _ = nll_scratch ( prior_funcs , params , X , y ) % timeit _ = marginal_likelihood ( prior_funcs , params , X , y ) 18.3 ms \u00b1 2.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 26 ms \u00b1 906 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 6. Training \u00b6 def softplus ( x ): return np . logaddexp ( x , 0. ) logger . setLevel ( logging . INFO ) X , y , Xtest , ytest = get_data ( 30 ) params = { 'gamma' : 10. , # 'length_scale': 1.0, # 'var_f': 1.0, 'likelihood_noise' : 1e-3 , } # Nice Trick for better training of params def saturate ( params ): return { ikey : softplus ( ivalue ) for ( ikey , ivalue ) in params . items ()} params = saturate ( params ) cov_f = functools . partial ( gram , rbf_kernel ) gp_priors = ( mu_f , cov_f ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( nll_scratch , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) # MEAN FUNCTION mu_f = zero_mean # l_val = mll_loss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) l_vals = mll_loss ( saturate ( params ), X , y ) # print('MLL (vector):', l_val) # print('MLL (samples):', l_vals) # dl_val = dloss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) dl_vals = dloss ( saturate ( params ), X , y ) # print('dMLL (vector):', dl_val)| # print('dMLL (samples):', dl_vals) # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # print(\"BEOFRE!\") # print(X.shape, y.shape) # print(\"PARAMS\", params) # print(opt_state) # value and gradient of loss function loss = mll_loss ( params , X , y ) grads = dloss ( params , X , y ) # # print(f\"VALUE:\", value) # print(\"During! v\", value) # print(\"During! p\", params) # print(\"During! g\", grads) # update parameter state opt_state = opt_update ( 0 , grads , opt_state ) # get new params params = get_params ( opt_state ) # print(\"AFTER! v\", value) # print(\"AFTER! p\", params) # print(\"AFTER! g\", grads) return params , opt_state , loss # initialize optimizer opt_init , opt_update , get_params = optimizers . adam ( step_size = 1e-2 ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) # print(\"PARAMS!\", params) n_epochs = 2_000 learning_rate = 0.01 losses = list () import tqdm with tqdm . trange ( n_epochs ) as bar : for i in bar : postfix = {} # params = saturate(params) # get nll and grads # nll, grads = dloss(params, X, y) params , opt_state , value = step ( params , X , y , opt_state ) # update params # params, momentums, scales, nll = train_step(params, momentums, scales, X, y) for ikey in params . keys (): postfix [ ikey ] = f \" { params [ ikey ] : .2f } \" # params[ikey] += learning_rate * grads[ikey].mean() losses . append ( value . mean ()) postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) params = saturate ( params ) # params = log_params(params) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:05<00:00, 335.90it/s, gamma=1.58, likelihood_noise=-2.76, Loss=10.66] plt . plot ( losses ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977} 7. Predictions \u00b6 def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) logging . debug ( f \"Output, Kxx: { Kxx . shape } , { Kxx . min () } , { Kxx . max () } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") L = jax . scipy . linalg . cholesky ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-7 ) * jnp . eye ( Kxx . shape [ 0 ]), lower = True ) logging . debug ( f \"Output, L: { L . shape } , { L . min () } , { L . max () } \" ) alpha = jax . scipy . linalg . solve_triangular ( L . T , jax . scipy . linalg . solve_triangular ( L , Y , lower = True ) ) # (L, lower), alpha = cholesky_factorization( # , Y # ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } , { alpha . min () } , { alpha . max () } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } , { v . min () : .2f } , { v . max () : .2f } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977} # print(X.shape, y.shape, test_X.shape) logger . setLevel ( logging . DEBUG ) # x_plot = jnp.linspace(X.min(), X.max(), 1_000)[:, None] print ( X . shape , y . shape , Xtest . shape ) mu_y , cov_y , var_y = posterior ( params , gp_priors , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) # onp.testing.assert_array_almost_equal(jncov_y, var_y) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (30, 1) (30, 1) (400, 1) DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Output, Kxx: (30, 30), 0.000838853360619396, 1.0 DEBUG:root:Solving Cholesky Factorization... DEBUG:root:Output, L: (30, 30),0.0,1.0303192138671875 DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, L: (30, 30), alpha: (30, 1),-8.470149993896484,10.187272071838379 DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:root:Output, v: (30, 400), -0.17,0.75 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -2.54,3.05 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -5.07,13.96 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.01,0.12 (400, 1) (400, 400) (400,) print ( var_y . min (), var_y . max (), cov_y . min (), cov_y . max ()) plt . plot ( var_y . squeeze ()) 0.07021278 0.18080568 0.05092573 0.18080568 [<matplotlib.lines.Line2D at 0x7f065c01e5b0>] uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . show ()","title":"1.0 gp basics"},{"location":"notebooks/1.0_gp_basics/#gaussian-process-regression","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Gaussian Process Regression"},{"location":"notebooks/1.0_gp_basics/#imports","text":"import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ])","title":"Imports"},{"location":"notebooks/1.0_gp_basics/#data","text":"def get_data ( N = 30 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = jnp . linspace ( - 1 , 1 , N ) Y = X + 0.2 * jnp . power ( X , 3.0 ) + 0.5 * jnp . power ( 0.5 + X , 2.0 ) * jnp . sin ( 4.0 * X ) Y += sigma_obs * onp . random . randn ( N ) Y -= jnp . mean ( Y ) Y /= jnp . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = jnp . linspace ( - 1.2 , 1.2 , N_test ) return X [:, None ], Y [:, None ], X_test [:, None ], None logger . setLevel ( logging . INFO ) , y , Xtest , ytest = get_data () print ( X . shape , y . shape ) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . show () (30, 1) (30, 1)","title":"Data"},{"location":"notebooks/1.0_gp_basics/#gaussian-process","text":"","title":"Gaussian Process"},{"location":"notebooks/1.0_gp_basics/#model-gp-prior","text":"Parameters : X, Y, $\\theta= $ (Likelihood Parameters, Kernel Parameters) Compute the Kernel Matrix Compute the Mean function Sample from the Multivariate Normal Distribution","title":"Model: GP Prior"},{"location":"notebooks/1.0_gp_basics/#kernel-function","text":"k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) <span><span class=\"MathJax_Preview\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right)</span><script type=\"math/tex\">k(x,y) = \\sigma_f \\exp \\left( - \\frac{1}{2\\sigma_\\lambda^2}|| x - y||^2_2 \\right) # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) # ARD Kernel @jax . jit def ard_kernel ( params , x , y ): # divide by the length scale x = x / params [ 'length_scale' ] y = y / params [ 'length_scale' ] # return the ard kernel return params [ 'var_f' ] * jnp . exp ( - sqeuclidean_distance ( x , y ) ) params = { 'var_f' : 1.0 , 'sigma' : 1.0 }","title":"Kernel Function"},{"location":"notebooks/1.0_gp_basics/#kernel-matrix","text":"# Gram Matrix def gram ( func , params , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ))( y ))( x ) params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] # test_X = x_plot[0, :] cov_f = functools . partial ( gram , rbf_kernel ) K_ = cov_f ( params , Xtest , X ) print ( K_ . shape ) K_ = cov_f ( params , X , Xtest ) print ( K_ . shape ) (400, 30) (30, 400)","title":"Kernel Matrix"},{"location":"notebooks/1.0_gp_basics/#mean-function","text":"Honestly, I never work with mean functions. I always assume a zero-mean function and that's it. I don't really know anyone who works with mean functions either. I've seen it used in deep Gaussian processes but I have no expertise in which mean functions to use. So, we'll follow the community standard for now: zero mean function def zero_mean ( x ): return jnp . zeros ( x . shape [ 0 ])","title":"Mean Function"},{"location":"notebooks/1.0_gp_basics/#3-compute-model","text":"Now we have all of the components to make our GP prior function. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) # define mean function mu_f = zero_mean # define covariance function params = { 'gamma' : 1.0 , 'var_f' : 1.0 } cov_f = functools . partial ( gram , rbf_kernel ) mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = X [ 0 , :])","title":"3. Compute Model"},{"location":"notebooks/1.0_gp_basics/#checks","text":"So I'm still getting used to the vmap . So in theory, this function should work for a vector \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} and for a batch of samples X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} # checks - 1 vector (D) test_X = X [ 0 , :] . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) print ( mu_x . shape , cov_x . shape ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 # checks - 1 vector with batch size (NxD) test_X = X . copy () mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) assert mu_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( mu_x ) == 1 # Check output shapes, # of dimensions assert cov_x . shape [ 0 ] == test_X . shape [ 0 ] assert jnp . ndim ( cov_x ) == 2 DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (1,) (1, 1) Woot! Success! So now we can technically sample from this GP prior distribution.","title":"Checks"},{"location":"notebooks/1.0_gp_basics/#4-sampling-from-gp-prior","text":"from scipy.stats import multivariate_normal as scio_mvn","title":"4. Sampling from GP Prior"},{"location":"notebooks/1.0_gp_basics/#scipy","text":"# checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # check outputs assert mu_x . shape == ( n_samples ,) assert cov_x . shape == ( n_samples , n_samples ) # draw random samples from distribution n_functions = 10 y_samples = stats . multivariate_normal . rvs ( mean = mu_x , cov = cov_x , size = n_functions ) assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-182-7c0d1026349a> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # check outputs <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma'","title":"Scipy"},{"location":"notebooks/1.0_gp_basics/#note-the-positive-semi-definite-error","text":"I believe that's due to the diagonals being off. Normally we add something called jitter. This allows the matrix to be positive semi-definite. mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ]) # draw random samples from distribution n_functions = 10 y_samples = scio_mvn . rvs ( mean = mu_x , cov = cov_x_ , size = n_functions ) print ( y_samples . shape ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-183-12ab200a8e55> in <module> ----> 1 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 2 3 # make it semi-positive definite with jitter 4 jitter = 1e-6 5 cov_x_ = cov_x + jitter * np . eye ( cov_x . shape [ 0 ] ) <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma' And now we don't have that message. This is a small thing but it's super important and can lead to errors in the optimization if not addressed.","title":"Note - The positive semi-definite error"},{"location":"notebooks/1.0_gp_basics/#jax","text":"# checks - 1 vector (D) params = { 'length_scale' : 0.1 , 'var_f' : 1.0 , } n_samples = 10 # condition on 3 samples test_X = X [: n_samples , :] . copy () # random samples from distribution mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 key = jax . random . PRNGKey ( 0 ) y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) # check assert y_samples . shape == ( n_functions , n_samples ) for isample in y_samples : plt . plot ( isample ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-184-9b06421fbf35> in <module> 9 test_X = X [ : n_samples , : ] . copy ( ) # random samples from distribution 10 ---> 11 mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) 12 13 # make it semi-positive definite with jitter <ipython-input-178-5213747816ca> in gp_prior (params, mu_f, cov_f, x) 1 def gp_prior ( params , mu_f , cov_f , x ) : ----> 2 return mu_f ( x ) , cov_f ( params , x , x ) <ipython-input-175-6d9cf91bb78f> in gram (func, params, x, y) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (x1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/api.py in batched_fun (*args) 756 in_axes_flat = _flatten_axes ( in_tree , in_axes ) 757 _check_axis_sizes ( in_tree , args_flat , in_axes_flat ) --> 758 out_flat = batching.batch(flat_fun, args_flat, in_axes_flat, 759 lambda: _flatten_axes(out_tree(), out_axes)) 760 return tree_unflatten ( out_tree ( ) , out_flat ) ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/interpreters/batching.py in batch (fun, in_vals, in_dims, out_dim_dests) 32 # executes a batched version of `fun` following out_dim_dests 33 batched_fun = batch_fun ( fun , in_dims , out_dim_dests ) ---> 34 return batched_fun . call_wrapped ( * in_vals ) 35 36 @ lu . transformation_with_aux ~/.conda/envs/jax_py38/lib/python3.8/site-packages/jax/linear_util.py in call_wrapped (self, *args, **kwargs) 148 gen = None 149 --> 150 ans = self . f ( * args , ** dict ( self . params , ** kwargs ) ) 151 del args 152 while stack : <ipython-input-175-6d9cf91bb78f> in <lambda> (y1) 1 # Gram Matrix 2 def gram ( func , params , x , y ) : ----> 3 return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ) ) ( y ) ) ( x ) <ipython-input-173-a1a5400824ce> in rbf_kernel (params, x, y) 7 # @jax.jit 8 def rbf_kernel ( params , x , y ) : ----> 9 return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y ) ) 10 11 # ARD Kernel KeyError : 'gamma'","title":"Jax"},{"location":"notebooks/1.0_gp_basics/#4-posterior","text":"Conditioned on the observations, can we make predictions. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def cholesky_factorization ( K , Y ): # cho factor the cholesky logger . debug ( f \"ChoFactor: K { K . shape } \" ) L = jax . scipy . linalg . cho_factor ( K , lower = True ) logger . debug ( f \"Output, L: { L [ 0 ] . shape } , { L [ 1 ] } \" ) # weights logger . debug ( f \"Input, ChoSolve(L, Y): { L [ 0 ] . shape , Y . shape } \" ) weights = jax . scipy . linalg . cho_solve ( L , Y ) logger . debug ( f \"Output, alpha: { weights . shape } \" ) return L , weights jitter = 1e-6 def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) # input vector # x_plot = jnp.linspace(X.min(), X.max(), 100)[:, None] test_X = Xtest [ 0 , :] prior_funcs = ( mu_f , cov_f ) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , X_new = test_X ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (1,) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (1,),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (1, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (1,), KxX @ alpha: (1, 30) @ (30, 1) DEBUG:root:Output, mu_y: (1, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 1) DEBUG:root:Output, v: (30, 1) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[1]), ShapedArray(float32[1])). DEBUG:root:Output, Kxx: (1, 1) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (1, 1), v:(30, 1) DEBUG:root:Output: cov(x*, x*) - (1, 1) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 1) DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (1,),(1,) DEBUG:absl:Compiling _where for args (ShapedArray(bool[1,1]), ShapedArray(float32[1,1]), ShapedArray(float32[1,1])). DEBUG:root:Output, diag(Kxx): (1,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (1, 30), (30, 30), (1, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[1,30]), ShapedArray(float32[1,30])). DEBUG:root:Output, var_y: (1,), 0.03,0.03 (1, 1) (1, 1) (1,) mu_y , cov_y , var_y = posterior ( params , prior_funcs , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Solving Cholesky Factorization... DEBUG:root:ChoFactor: K(30, 30) DEBUG:root:Output, L: (30, 30), True DEBUG:root:Input, ChoSolve(L, Y): ((30, 30), (30, 1)) DEBUG:root:Output, alpha: (30, 1) DEBUG:root:Output, L: (30, 30), alpha: (30, 1) DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,400])). DEBUG:root:Output, v: (30, 400) DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -6.00,7.19 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling _where for args (ShapedArray(bool[400,400]), ShapedArray(float32[400,400]), ShapedArray(float32[400,400])). DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -33.54,86.93 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.00,0.03 (400, 1) (400, 400) (400,) plt . plot ( var_y . squeeze ()) [<matplotlib.lines.Line2D at 0x7f06943d8a30>] test_X . shape , mu_y . shape ((1,), (400, 1)) uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) [<matplotlib.lines.Line2D at 0x7f06741adfd0>]","title":"4. Posterior"},{"location":"notebooks/1.0_gp_basics/#5-loss-log-likelihood","text":"","title":"5. Loss - Log-Likelihood"},{"location":"notebooks/1.0_gp_basics/#from-scratch","text":"# @jax.jit def cholesky_factorization ( K , Y ): # cho factor the cholesky L = jax . scipy . linalg . cho_factor ( K ) # weights weights = jax . scipy . linalg . cho_solve ( L , Y ) return L , weights def nll_scratch ( gp_priors , params , X , Y ) -> float : ( mu_func , cov_func ) = gp_priors # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # y_mean = jnp.mean(Y, axis=1) # Y -= y_mean # print(mu_x.shape, Kxx.shape) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== # print(f\"Problem:\", X.shape, Y.shape, Kxx.shape) # print(f\"Y: {Y.shape}, Kxx: {Kxx.shape}\") ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ 'likelihood_noise' ] + 1e-5 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) # L = jax.scipy.linalg.cholesky(Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]), lower=True) # alpha = jax.scipy.linalg.solve_triangular(L.T, jax.scipy.linalg.solve_triangular(L, y, lower=True)) # print(f\"Y: {Y.shape}, alpha: {alpha.shape}\") logging . debug ( f \"Y: { Y . shape } ,alpha: { alpha . shape } \" ) log_likelihood = - 0.5 * jnp . einsum ( \"ik,ik->k\" , Y , alpha ) #* jnp.dot(Y.T, alpha) # log_likelihood -= jnp . sum ( jnp . log ( jnp . diag ( L ))) log_likelihood -= ( Kxx . shape [ 0 ] / 2 ) * jnp . log ( 2 * jnp . pi ) # log_likelihood -= jnp.sum(-0.5 * np.log(2 * 3.1415) - params['var_f']**2) return - jnp . sum ( log_likelihood ) # # print(L.shape, alpha.shape) # # cho factor the cholesky # K_gp = Kxx + ( params['likelihood_noise'] + 1e-6 ) * jnp.eye(Kxx.shape[0]) # # L = jax.scipy.linalg.cholesky(K_gp) # # assert np.testing.assert_array_almost_equal(K_gp, L @ L.T), # return jax.scipy.stats.multivariate_normal.logpdf(Y, mean=mu_x, cov=K_gp) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = nll_scratch ( prior_funcs , params , X , y ) print ( nll ) DEBUG:absl:Compiling _cholesky for args (ShapedArray(float32[30,30]),). DEBUG:absl:Compiling _cho_solve for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Y: (30, 1),alpha:(30, 1) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _where for args (ShapedArray(bool[30,30]), ShapedArray(float32[30,30]), ShapedArray(float32[30,30])). 54.878708","title":"From Scratch"},{"location":"notebooks/1.0_gp_basics/#auto-batching-with-vmap","text":"nll_scratch_vec = jax . vmap ( nll_scratch , in_axes = ( None , None , 0 , 0 )) nll = nll_scratch_vec ( params , prior_funcs , X , y [:, None ]) print ( nll . sum ()) (1,) (1, 1) Y: (1,), alpha: (1,) -209.98637","title":"Auto-Batching with VMAP"},{"location":"notebooks/1.0_gp_basics/#refactor-built-in-function","text":"It turns out that the jax library already has the logpdf for the multivariate_normal already implemented. So we can just use that. def gp_prior ( params , mu_f , cov_f , x ): return mu_f ( x ) , cov_f ( params , x , x ) def marginal_likelihood ( prior_params , params , Xtrain , Ytrain ): # unpack params ( mu_func , cov_func ) = prior_params # ========================== # 1. GP Prior # ========================== mu_x = mu_f ( Xtrain ) logging . debug ( f \"mu: { mu_x . shape } \" ) Kxx = cov_f ( params , Xtrain , Xtrain ) logging . debug ( f \"Kxx: { Kxx . shape } \" ) # print(\"MLL (GPPR):\", Xtrain.shape, Ytrain.shape) # mu_x, Kxx = gp_prior(params, mu_f=mu_func, cov_f=cov_func , x=Xtrain) # =========================== # 2. GP Likelihood # =========================== K_gp = Kxx + ( params [ 'likelihood_noise' ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]) logging . debug ( f \"K_gp: { K_gp . shape } \" ) # print(\"MLL (GPLL):\", Xtrain.shape, Ytrain.shape) # =========================== # 3. Built-in GP Likelihood # =========================== logging . debug ( f \"Input: { Ytrain . squeeze () . shape } , mu: { mu_x . shape } , K: { K_gp . shape } \" ) log_prob = jax . scipy . stats . multivariate_normal . logpdf ( Ytrain . squeeze (), mean = jnp . zeros ( Ytrain . shape [ 0 ]), cov = K_gp ) logging . debug ( f \"LogProb: { log_prob . shape } \" ) nll = jnp . sum ( log_prob ) return - nll logger . setLevel ( logging . DEBUG ) # MEAN FUNCTION mu_f = zero_mean # COVARIANCE FUNCTION params = { 'gamma' : 1.0 , 'var_f' : 1.0 , 'likelihood_noise' : 0.01 , } cov_f = functools . partial ( gram , rbf_kernel ) prior_funcs = ( mu_f , cov_f ) # print(X.shape, y.shape, test_X.shape) nll = marginal_likelihood ( prior_funcs , params , X , y ) print ( nll ) DEBUG:root:mu: (30,) DEBUG:root:Kxx: (30, 30) DEBUG:root:K_gp: (30, 30) DEBUG:root:Input: (30,), mu: (30,), K: (30, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[30]), ShapedArray(float32[30])). DEBUG:root:LogProb: () 54.937344 logger . setLevel ( logging . INFO ) % timeit _ = nll_scratch ( prior_funcs , params , X , y ) % timeit _ = marginal_likelihood ( prior_funcs , params , X , y ) 18.3 ms \u00b1 2.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 26 ms \u00b1 906 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)","title":"Refactor - Built-in Function"},{"location":"notebooks/1.0_gp_basics/#6-training","text":"def softplus ( x ): return np . logaddexp ( x , 0. ) logger . setLevel ( logging . INFO ) X , y , Xtest , ytest = get_data ( 30 ) params = { 'gamma' : 10. , # 'length_scale': 1.0, # 'var_f': 1.0, 'likelihood_noise' : 1e-3 , } # Nice Trick for better training of params def saturate ( params ): return { ikey : softplus ( ivalue ) for ( ikey , ivalue ) in params . items ()} params = saturate ( params ) cov_f = functools . partial ( gram , rbf_kernel ) gp_priors = ( mu_f , cov_f ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( nll_scratch , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) # MEAN FUNCTION mu_f = zero_mean # l_val = mll_loss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) l_vals = mll_loss ( saturate ( params ), X , y ) # print('MLL (vector):', l_val) # print('MLL (samples):', l_vals) # dl_val = dloss(saturate(params), X[0,:], y[0, :].reshape(-1, 1)) dl_vals = dloss ( saturate ( params ), X , y ) # print('dMLL (vector):', dl_val)| # print('dMLL (samples):', dl_vals) # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # print(\"BEOFRE!\") # print(X.shape, y.shape) # print(\"PARAMS\", params) # print(opt_state) # value and gradient of loss function loss = mll_loss ( params , X , y ) grads = dloss ( params , X , y ) # # print(f\"VALUE:\", value) # print(\"During! v\", value) # print(\"During! p\", params) # print(\"During! g\", grads) # update parameter state opt_state = opt_update ( 0 , grads , opt_state ) # get new params params = get_params ( opt_state ) # print(\"AFTER! v\", value) # print(\"AFTER! p\", params) # print(\"AFTER! g\", grads) return params , opt_state , loss # initialize optimizer opt_init , opt_update , get_params = optimizers . adam ( step_size = 1e-2 ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) # print(\"PARAMS!\", params) n_epochs = 2_000 learning_rate = 0.01 losses = list () import tqdm with tqdm . trange ( n_epochs ) as bar : for i in bar : postfix = {} # params = saturate(params) # get nll and grads # nll, grads = dloss(params, X, y) params , opt_state , value = step ( params , X , y , opt_state ) # update params # params, momentums, scales, nll = train_step(params, momentums, scales, X, y) for ikey in params . keys (): postfix [ ikey ] = f \" { params [ ikey ] : .2f } \" # params[ikey] += learning_rate * grads[ikey].mean() losses . append ( value . mean ()) postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) params = saturate ( params ) # params = log_params(params) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:05<00:00, 335.90it/s, gamma=1.58, likelihood_noise=-2.76, Loss=10.66] plt . plot ( losses ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977}","title":"6. Training"},{"location":"notebooks/1.0_gp_basics/#7-predictions","text":"def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False ): logging . debug ( f \"Inputs, X: { X . shape } , Y: { Y . shape } , X*: { X_new . shape } \" ) ( mu_func , cov_func ) = prior_params logging . debug ( \"Loaded mean and cov functions\" ) # ========================== # 1. GP PRIOR # ========================== logging . debug ( f \"Getting GP Priors...\" ) mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) logging . debug ( f \"Output, mu_x: { mu_x . shape } , Kxx: { Kxx . shape } \" ) logging . debug ( f \"Output, Kxx: { Kxx . shape } , { Kxx . min () } , { Kxx . max () } \" ) # check outputs assert mu_x . shape == ( X . shape [ 0 ],), f \" { mu_x . shape } =/= { ( X . shape [ 0 ],) } \" assert Kxx . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \" { Kxx . shape } =/= { ( X . shape [ 0 ], X . shape [ 0 ]) } \" # =========================== # 2. CHOLESKY FACTORIZATION # =========================== logging . debug ( f \"Solving Cholesky Factorization...\" ) # 1 STEP # print(f\"Problem: {Kxx.shape},{Y.shape}\") L = jax . scipy . linalg . cholesky ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-7 ) * jnp . eye ( Kxx . shape [ 0 ]), lower = True ) logging . debug ( f \"Output, L: { L . shape } , { L . min () } , { L . max () } \" ) alpha = jax . scipy . linalg . solve_triangular ( L . T , jax . scipy . linalg . solve_triangular ( L , Y , lower = True ) ) # (L, lower), alpha = cholesky_factorization( # , Y # ) logging . debug ( f \"Output, L: { L . shape } , alpha: { alpha . shape } , { alpha . min () } , { alpha . max () } \" ) assert L . shape == ( X . shape [ 0 ], X . shape [ 0 ], ), f \"L: { L . shape } =/= X..: { ( X . shape [ 0 ], X . shape [ 0 ]) } \" assert alpha . shape == ( X . shape [ 0 ], 1 ), f \"alpha: { alpha . shape } =/= X: { X . shape [ 0 ], 1 } \" # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ logging . debug ( f \"Getting Projection Kernel...\" ) logging . debug ( f \"Input, cov(x*, X): { X_new . shape } , { X . shape } \" ) # calculate transform kernel KxX = cov_func ( params , X_new , X ) logging . debug ( f \"Output, KxX: { KxX . shape } \" ) assert KxX . shape == ( X_new . shape [ 0 ], X . shape [ 0 ], ), f \" { KxX . shape } =/= { ( X_new . shape [ 0 ], X . shape [ 0 ]) } \" # Project data logging . debug ( f \"Getting Predictive Mean Distribution...\" ) logging . debug ( f \"Input, mu(x*): { X_new . shape } , KxX @ alpha: { KxX . shape } @ { alpha . shape } \" ) mu_y = jnp . dot ( KxX , alpha ) logging . debug ( f \"Output, mu_y: { mu_y . shape } \" ) assert mu_y . shape == ( X_new . shape [ 0 ], 1 ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Covariance matrix...\" ) logging . debug ( f \"Input, L @ KxX.T: { L . shape } @ { KxX . T . shape } \" ) # print(f\"K_xX: {KXx.T.shape}, L: {L.shape}\") v = jax . scipy . linalg . cho_solve (( L , True ), KxX . T ) logging . debug ( f \"Output, v: { v . shape } , { v . min () : .2f } , { v . max () : .2f } \" ) assert v . shape == ( X . shape [ 0 ], X_new . shape [ 0 ], ), f \"v: { v . shape } =/= { ( X_new . shape [ 0 ]) } \" logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) Kxx = cov_func ( params , X_new , X_new ) logging . debug ( f \"Output, Kxx: { Kxx . shape } \" ) assert Kxx . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) logging . debug ( f \"Calculating final covariance matrix...\" ) logging . debug ( f \"Inputs, Kxx: { Kxx . shape } , v: { v . shape } \" ) cov_y = Kxx - jnp . dot ( KxX , v ) logging . debug ( f \"Output: cov(x*, x*) - { cov_y . shape } \" ) assert cov_y . shape == ( X_new . shape [ 0 ], X_new . shape [ 0 ]) if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # TODO: Bug here for vmap... # ===================================== # 6. PREDICTIVE VARIANCE DISTRIBUTION # ===================================== logging . debug ( f \"Getting Predictive Variance...\" ) logging . debug ( f \"Input, L.T, I: { L . T . shape } , { KxX . T . shape } \" ) Linv = jax . scipy . linalg . solve_triangular ( L . T , jnp . eye ( L . shape [ 0 ])) logging . debug ( f \"Output, Linv: { Linv . shape } , { Linv . min () : .2f } , { Linv . max () : .2f } \" ) logging . debug ( f \"Covariance matrix tests...cov(x*, x*)\" ) logging . debug ( f \"Inputs, cov(x*, x*) - { X_new . shape } , { X_new . shape } \" ) var_y = jnp . diag ( cov_func ( params , X_new , X_new )) logging . debug ( f \"Output, diag(Kxx): { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) logging . debug ( f \"Inputs, Linv @ Linv.T - { Linv . shape } , { Linv . T . shape } \" ) Kinv = jnp . dot ( Linv , Linv . T ) logging . debug ( f \"Output, Kinv: { Kinv . shape } , { Kinv . min () : .2f } , { Kinv . max () : .2f } \" ) logging . debug ( f \"Final Variance...\" ) logging . debug ( f \"Inputs, KxX: { KxX . shape } , { Kinv . shape } , { KxX . shape } \" ) var_y -= jnp . einsum ( \"ij,ij->i\" , jnp . dot ( KxX , Kinv ), KxX ) #jnp.dot(jnp.dot(KxX, Kinv), KxX.T) logging . debug ( f \"Output, var_y: { var_y . shape } , { var_y . min () : .2f } , { var_y . max () : .2f } \" ) #jnp.einsum(\"ij, jk, ki->i\", KxX, jnp.dot(Linv, Linv.T), KxX.T) return mu_y , cov_y , jnp . diag ( cov_y ) params {'gamma': 1.770868627016222, 'likelihood_noise': 0.06155753105145977} # print(X.shape, y.shape, test_X.shape) logger . setLevel ( logging . DEBUG ) # x_plot = jnp.linspace(X.min(), X.max(), 1_000)[:, None] print ( X . shape , y . shape , Xtest . shape ) mu_y , cov_y , var_y = posterior ( params , gp_priors , X , y , Xtest , True ) print ( mu_y . shape , cov_y . shape , var_y . shape ) # onp.testing.assert_array_almost_equal(jncov_y, var_y) DEBUG:root:Inputs, X: (30, 1), Y: (30, 1), X*: (400, 1) DEBUG:root:Loaded mean and cov functions DEBUG:root:Getting GP Priors... DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[30,1]), ShapedArray(float32[30,1])). (30, 1) (30, 1) (400, 1) DEBUG:root:Output, mu_x: (30,), Kxx: (30, 30) DEBUG:root:Output, Kxx: (30, 30), 0.000838853360619396, 1.0 DEBUG:root:Solving Cholesky Factorization... DEBUG:root:Output, L: (30, 30),0.0,1.0303192138671875 DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:absl:Compiling _solve_triangular for args (ShapedArray(float32[30,30]), ShapedArray(float32[30,1])). DEBUG:root:Output, L: (30, 30), alpha: (30, 1),-8.470149993896484,10.187272071838379 DEBUG:root:Getting Projection Kernel... DEBUG:root:Input, cov(x*, X): (400, 1),(30, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[30,1])). DEBUG:root:Output, KxX: (400, 30) DEBUG:root:Getting Predictive Mean Distribution... DEBUG:root:Input, mu(x*): (400, 1), KxX @ alpha: (400, 30) @ (30, 1) DEBUG:root:Output, mu_y: (400, 1) DEBUG:root:Getting Predictive Covariance matrix... DEBUG:root:Input, L @ KxX.T: (30, 30) @ (30, 400) DEBUG:root:Output, v: (30, 400), -0.17,0.75 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:absl:Compiling rbf_kernel for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[400,1]), ShapedArray(float32[400,1])). DEBUG:root:Output, Kxx: (400, 400) DEBUG:root:Calculating final covariance matrix... DEBUG:root:Inputs, Kxx: (400, 400), v:(30, 400) DEBUG:root:Output: cov(x*, x*) - (400, 400) DEBUG:root:Getting Predictive Variance... DEBUG:root:Input, L.T, I: (30, 30), (30, 400) DEBUG:root:Output, Linv: (30, 30), -2.54,3.05 DEBUG:root:Covariance matrix tests...cov(x*, x*) DEBUG:root:Inputs, cov(x*, x*) - (400, 1),(400, 1) DEBUG:root:Output, diag(Kxx): (400,), 1.00,1.00 DEBUG:root:Inputs, Linv @ Linv.T - (30, 30),(30, 30) DEBUG:root:Output, Kinv: (30, 30), -5.07,13.96 DEBUG:root:Final Variance... DEBUG:root:Inputs, KxX: (400, 30), (30, 30), (400, 30) DEBUG:absl:Compiling _einsum for args (ShapedArray(float32[400,30]), ShapedArray(float32[400,30])). DEBUG:root:Output, var_y: (400,), 0.01,0.12 (400, 1) (400, 400) (400,) print ( var_y . min (), var_y . max (), cov_y . min (), cov_y . max ()) plt . plot ( var_y . squeeze ()) 0.07021278 0.18080568 0.05092573 0.18080568 [<matplotlib.lines.Line2D at 0x7f065c01e5b0>] uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = 'red' ) plt . plot ( Xtest . squeeze (), mu_y . squeeze (), label = 'Mean' ) plt . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.1 ) plt . show ()","title":"7. Predictions"},{"location":"notebooks/1.1_gp_refactored/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Gaussian Process Regression \u00b6 This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow. Imports \u00b6 import sys from pyprojroot import here sys . path . append ( str ( here ())) import pathlib FIG_PATH = pathlib . Path ( str ( here ())) . joinpath ( 'figures/jaxgp/examples/standard/' ) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data , near_square_wave from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . INFO ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ]) Data \u00b6 from dataclasses import dataclass @dataclass class args : num_train = 80 num_test = 1_000 smoke_test = False input_noise = 0.3 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = near_square_wave ( n_train = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , n_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Model \u00b6 # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Optimizer \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Training \u00b6 # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 0%| | 0/500 [00:00<?, ?it/s]DEBUG:absl:Compiling gram for args (AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit()). DEBUG:absl:Compiling marginal_likelihood for args (AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit()). DEBUG:absl:Compiling <unnamed wrapped function> for args (AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit()). DEBUG:absl:Compiling step for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[80,1]), ShapedArray(float32[80,1]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[])). 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:11<00:00, 45.09it/s, length_scale=2.56, likelihood_noise=0.05, var_f=0.70, Loss=17.55] Predictions \u00b6 # Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_f = predictive_variance ( params , gp_priors , X , y , Xtest , False , False ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_likelihood = var_y - var_f # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) Results \u00b6 Full GP w. Uncertainty \u00b6 # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 2.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( FIG_PATH . joinpath ( \"1d_gp.png\" )) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () Likelihood Variance \u00b6 # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_likelihood . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (Outputs)\" , ) ax . set_ylim ([ - 3.5 , 2.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( FIG_PATH . joinpath ( \"1d_gp_nout.png\" )) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show () Model Variance \u00b6 # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_f . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (Model)\" , ) ax . set_ylim ([ - 3.5 , 2.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( FIG_PATH . joinpath ( \"1d_gp_nmodel.png\" )) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"1.1 gp refactored"},{"location":"notebooks/1.1_gp_refactored/#gaussian-process-regression","text":"This notebook, I will go over how we can implement the Gaussian process (GP) regression algorithm using Jax. This isn't a new algorithm or anything but I would like to get accustomed to using Jax because it will be useful later when I implement the GPs to handle uncertain inputs. Inspirations Github Code - Lucas Broke down the GP function very nicely. Nice enough for me to follow.","title":"Gaussian Process Regression"},{"location":"notebooks/1.1_gp_refactored/#imports","text":"import sys from pyprojroot import here sys . path . append ( str ( here ())) import pathlib FIG_PATH = pathlib . Path ( str ( here ())) . joinpath ( 'figures/jaxgp/examples/standard/' ) import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers from src.models.jaxgp.data import get_data , near_square_wave from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload import functools import jax import jax.numpy as jnp from jax.experimental import optimizers import numpy as np import numpy as onp import logging logger = logging . getLogger () logger . setLevel ( logging . INFO ) # Plotting libraries import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-paper' ])","title":"Imports"},{"location":"notebooks/1.1_gp_refactored/#data","text":"from dataclasses import dataclass @dataclass class args : num_train = 80 num_test = 1_000 smoke_test = False input_noise = 0.3 output_noise = 0.15 # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = near_square_wave ( n_train = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , n_test = args . num_test , ) /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Data"},{"location":"notebooks/1.1_gp_refactored/#model","text":"# PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , ard_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { # \"gamma\": 2.0, \"length_scale\" : 1.0 , \"var_f\" : 1.0 , \"likelihood_noise\" : 1.0 , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss ))","title":"Model"},{"location":"notebooks/1.1_gp_refactored/#optimizer","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss","title":"Optimizer"},{"location":"notebooks/1.1_gp_refactored/#training","text":"# TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) 0%| | 0/500 [00:00<?, ?it/s]DEBUG:absl:Compiling gram for args (AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit()). DEBUG:absl:Compiling marginal_likelihood for args (AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit()). DEBUG:absl:Compiling <unnamed wrapped function> for args (AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit(), AbstractUnit()). DEBUG:absl:Compiling step for args (ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[80,1]), ShapedArray(float32[80,1]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[]), ShapedArray(float32[])). 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:11<00:00, 45.09it/s, length_scale=2.56, likelihood_noise=0.05, var_f=0.70, Loss=17.55]","title":"Training"},{"location":"notebooks/1.1_gp_refactored/#predictions","text":"# Posterior Predictions mu_y = predictive_mean ( params , gp_priors , X , y , Xtest ) var_f = predictive_variance ( params , gp_priors , X , y , Xtest , False , False ) var_y = predictive_variance ( params , gp_priors , X , y , Xtest , True , False ) var_likelihood = var_y - var_f # Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ())","title":"Predictions"},{"location":"notebooks/1.1_gp_refactored/#results","text":"","title":"Results"},{"location":"notebooks/1.1_gp_refactored/#full-gp-w-uncertainty","text":"# Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_y . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (95% Confidence)\" , ) ax . set_ylim ([ - 3.5 , 2.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( FIG_PATH . joinpath ( \"1d_gp.png\" )) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Full GP w. Uncertainty"},{"location":"notebooks/1.1_gp_refactored/#likelihood-variance","text":"# Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_likelihood . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (Outputs)\" , ) ax . set_ylim ([ - 3.5 , 2.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( FIG_PATH . joinpath ( \"1d_gp_nout.png\" )) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Likelihood Variance"},{"location":"notebooks/1.1_gp_refactored/#model-variance","text":"# Uncertainty uncertainty = 1.96 * jnp . sqrt ( var_f . squeeze ()) fig , ax = plt . subplots () ax . scatter ( X , y , c = \"red\" , label = \"Training Data\" ) ax . plot ( Xtest . squeeze (), mu_y . squeeze (), label = r \"Predictive Mean\" , color = \"black\" , linewidth = 3 , ) ax . fill_between ( Xtest . squeeze (), mu_y . squeeze () + uncertainty , mu_y . squeeze () - uncertainty , alpha = 0.3 , color = \"darkorange\" , label = f \"Predictive Std (Model)\" , ) ax . set_ylim ([ - 3.5 , 2.5 ]) ax . legend ( fontsize = 12 ) plt . tight_layout () fig . savefig ( FIG_PATH . joinpath ( \"1d_gp_nmodel.png\" )) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp.png\") plt . show ()","title":"Model Variance"},{"location":"notebooks/gpytorch_gp_uncertain/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); GPyTorch - Uncertain Inputs \u00b6 This is my notebook where I play around with all things PyTorch. I use the following packages: PyTorch Pyro GPyTorch PyTorch Lightning #@title Install Packages !pip install --upgrade pyro-ppl gpytorch pytorch-lightning tqdm Requirement already up-to-date: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.4.0) Requirement already up-to-date: gpytorch in /usr/local/lib/python3.6/dist-packages (1.2.0) Requirement already up-to-date: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (0.9.0) Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.50.0) Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.5) Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.3.0) Requirement already satisfied, skipping upgrade: torch>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.6.0+cu101) Requirement already satisfied, skipping upgrade: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.2) Requirement already satisfied, skipping upgrade: tensorboard==2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.2.0) Requirement already satisfied, skipping upgrade: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1) Requirement already satisfied, skipping upgrade: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2) Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (20.4) Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.7.0) Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.35.1) Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (2.23.0) Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.15.0) Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.4.1) Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.10.0) Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.2.2) Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.17.2) Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.32.0) Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.12.4) Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (50.3.0) Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.0.1) Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytorch-lightning) (2.4.7) Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (2020.6.20) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (1.24.3) Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (3.0.4) Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (2.10) Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (1.3.0) Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (2.0.0) Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.2.8) Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.6) Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.1.1) Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (3.1.0) Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (3.2.0) Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.4.8) #@title Load Packages # TYPE HINTS from typing import Tuple, Optional, Dict, Callable, Union # PyTorch Settings import torch # Pyro Settings # GPyTorch Settings import gpytorch # PyTorch Lightning Settings import pytorch_lightning as pl import tqdm # NUMPY SETTINGS import numpy as np np.set_printoptions(precision=3, suppress=True) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns.set_context(context='talk',font_scale=0.7) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd.set_option(\"display.max_rows\", 120) pd.set_option(\"display.max_columns\", 120) # LOGGING SETTINGS import sys import logging logging.basicConfig( level=logging.INFO, stream=sys.stdout, format='%(asctime)s:%(levelname)s:%(message)s' ) logger = logging.getLogger() #logger.setLevel(logging.INFO) %load_ext autoreload %autoreload 2 Data \u00b6 from sklearn.utils import check_random_state def near_square_wave( n_train: int=80, input_noise: float=0.3, output_noise: float=0.15, n_test: int=400, random_state: int=123, ): # function f = lambda x: np.sin(1.0 * np.pi / 1.6 * np.cos(5 + .5 * x)) # create clean inputs x_mu = np.linspace(-10, 10, n_train) # clean outputs y = f(x_mu) # generate noise x_rng = check_random_state(random_state) y_rng = check_random_state(random_state + 1) # noisy inputs x = x_mu + input_noise * x_rng.randn(x_mu.shape[0]) # noisy outputs y = f(x_mu) + output_noise * y_rng.randn(x_mu.shape[0]) # test points x_test = np.linspace(-12, 12, n_test) + x_rng.randn(n_test) y_test = f(np.linspace(-12, 12, n_test)) x_test = np.sort(x_test) return x[:, None], y[:, None], x_test[:, None], y_test Example I - Constant Input Error \u00b6 import math # function f = lambda x: torch.sin(1.0 * math.pi / 1.6 * torch.cos(5 + .5 * x)) # training inputs train_x_mean = torch.linspace(-10, 10, 80) # noise for outputs train_x_stdv = 0.3 * torch.ones(train_x_mean.shape[0]) # training points train_x = train_x_mean + train_x_stdv # true function (near squared sine wave) y_rng = check_random_state(123 + 1) output_noise = 0.15 real_y = f(train_x) train_y = real_y + output_noise * torch.randn(train_x_mean.size()) fig, ax = plt.subplots() ax.plot(train_x, real_y) ax.errorbar(train_x, train_y, xerr=1.96 * train_x_stdv, fmt='k.') plt.show() train_x_stdv.shape torch.Size([80]) #@title GP Model class ExactGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood): super(ExactGPModel, self).__init__(train_x, train_y, likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) def forward(self, x): mean_x = self.mean_module(x) covar_x = self.covar_module(x) return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) # initialize likelihood and model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(train_x, train_y, likelihood) training_iter = 100 # Find optimal model hyperparameters model.train() likelihood.train() # Use the adam optimizer optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # Includes GaussianLikelihood parameters # \"Loss\" for GPs - the marginal log likelihood mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) iterator = tqdm.notebook.tqdm(range(training_iter)) for i in iterator: # Zero gradients from previous iteration optimizer.zero_grad() # Output from model output = model(train_x) # Calc loss and backprop gradients loss = -mll(output, train_y) loss.backward() iterator.set_postfix( loss=loss.item(), length_scale=model.covar_module.base_kernel.lengthscale.item(), noise=model.likelihood.noise.item() ) # print('Iter %d/%d - Loss: %.3f lengthscale: %.3f noise: %.3f' % ( # i + 1, training_iter, loss.item(), # model.covar_module.base_kernel.lengthscale.item(), # model.likelihood.noise.item() # )) optimizer.step() var element = $('#b4488314-68b9-4e03-b6c5-31806f7dbff7'); {\"model_id\": \"a27034fa663d4f309c52cd9adc92fabf\", \"version_minor\": 0, \"version_major\": 2} #@title Predictions # Get into evaluation (predictive posterior) mode model.eval() likelihood.eval() # Test points are regularly spaced along [0,1] # Make predictions by feeding model through likelihood with torch.no_grad(), gpytorch.settings.fast_pred_var(): test_x = torch.linspace(-12, 12, 100) observed_pred = likelihood(model(test_x)) with torch.no_grad(): # Initialize plot # f, ax = plt.subplots(1, 1, figsize=(8, 3)) # Get upper and lower confidence bounds lower, upper = observed_pred.confidence_region() # # Plot training data as black stars # ax.errorbar(train_x_mean.numpy(), train_y.numpy(), xerr=train_x_stdv, fmt='k*') # # Plot predictive means as blue line # ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b') # # Shade between the lower and upper confidence bounds # ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5) # ax.set_ylim([-3, 3]) # ax.legend(['Observed Data', 'Mean', 'Confidence']) fig, ax = plt.subplots() ax.scatter(train_x_mean.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") ax.plot( test_x.numpy().squeeze(), observed_pred.mean.numpy().squeeze(), label=r\"Predictive Mean\", color=\"black\", linewidth=3, ) ax.fill_between( test_x.numpy().squeeze(), lower.numpy(), upper.numpy(), alpha=0.3, color=\"darkorange\", label=f\"Predictive Std\", ) ax.set_ylim([-3.5, 2.5]) ax.legend(fontsize=12) plt.tight_layout() # fig.savefig(FIG_PATH.joinpath(\"1d_gp_taylor_1o.png\")) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt.show() Linearized GP \u00b6 test_x = torch.linspace(-12, 12, 100) test_y = f(test_x) v = torch.ones(100) def predict_mean(test_x): return likelihood(model(test_x)).mean def predict_var(test_x): return likelihood(model(test_x)).variance def predict_covar(test_x): return likelihood(model(test_x)).covariance_matrix def predict_mean_sum(test_x): return likelihood(model(test_x)).mean.sum() mu = predict_mean(test_x) var = predict_var(test_x) covar = predict_covar(test_x) _ , dx = torch.autograd.functional.vjp(predict_mean, test_x, v) _, dx2 = torch.autograd.functional.vhp(predict_mean_sum, test_x, v) # f, ax = plt.subplots(1, 1, figsize=(8, 5)) plt.figure(figsize=(8, 5)) plt.plot(mu.detach().numpy(), label=\"Mean\") plt.plot(dx.detach().numpy(), label=\"Jacobian\") plt.plot(dx2.detach().numpy(), label=\"Hessian\") plt.legend() plt.show() Predictive Mean & Variance \u00b6 var_e = var.detach().numpy()[:, np.newaxis] cov_e = covar.detach().numpy() var_x = train_x_stdv ** 2 cov_x = np.array([0.3])[:, np.newaxis] # 1st order approximation to1 = dx.detach().numpy()[:, np.newaxis].dot(cov_x.dot(dx.detach().numpy()[:, np.newaxis].T)) var_to1 = var_e + np.diag(to1)[:, np.newaxis] # 2nd order approximation to2 = np.dot(dx2.detach().numpy().squeeze()[:, np.newaxis, np.newaxis], cov_x) to2 = np.trace(to2, axis1=1, axis2=2) mu_2 = mu.detach().numpy() + 0.5 * to2.squeeze() var_to2 = var_to1 + to2[:, np.newaxis] # standard deviation std_e = np.sqrt(var_e) std_to1 = np.sqrt(var_to1) std_to2 = np.sqrt(var_to2) /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in sqrt plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy(), label=\"Predictive Mean\") plt.plot(test_x.detach().numpy().squeeze(), mu_2, label=\"Predictive Mean (2nd Order)\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"red\", label=\"Real Function\", color='black', linewidth=3) plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.legend() plt.show() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Saw kwargs ['c', 'color'] which are all aliases for 'color'. Kept value from 'color'. Passing multiple aliases for the same property will raise a TypeError in 3.3. after removing the cwd from sys.path. # f, ax = plt.subplots(1, 1, figsize=(8, 5)) plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy(), label=\"Predictive Mean\") plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"black\", label=\"Real Function\") plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() - 1.96 * std_e.squeeze(), label='Upper Limit') plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() + 1.96 * std_e.squeeze(), label='Lower Limit') plt.legend() plt.show() plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy(), label=\"Predictive Mean\") plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"black\", label=\"Real Function\") plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() - 1.96 * std_to1.squeeze(), label='Upper Limit') plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() + 1.96 * std_to1.squeeze(), label='Lower Limit') plt.legend() plt.show() plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu_2, label=\"Predictive Mean\") plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"black\", label=\"Real Function\") plt.plot(test_x.detach().numpy().squeeze(), mu_2.squeeze() - 1.96 * std_to2.squeeze(), label='Upper Limit') plt.plot(test_x.detach().numpy().squeeze(), mu_2.squeeze() + 1.96 * std_to2.squeeze(), label='Lower Limit') plt.legend() plt.show() from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error scores = {} scores['mae_o1'] = mean_absolute_error( test_y.detach().numpy(), mu.detach().numpy() ) scores['mae_o2'] = mean_absolute_error( test_y.detach().numpy(), mu_2 ) print(f\"MAE (o1): {scores['mae_o1']:.4f}\") print(f\"MAE (o2): {scores['mae_o2']:.4f}\") scores['mse_o1'] = mean_squared_error( test_y.detach().numpy(), mu.detach().numpy() ) scores['mse_o2'] = mean_squared_error( test_y.detach().numpy(), mu_2 ) print(f\"MSE (o1): {scores['mse_o1']:.4f}\") print(f\"MSE (o2): {scores['mse_o2']:.4f}\") scores['rmse_o1'] = np.sqrt(scores['mse_o1']) scores['rmse_o2'] = np.sqrt(scores['mse_o2']) print(f\"RMSE (o1): {scores['rmse_o1']:.4f}\") print(f\"RMSE (o2): {scores['rmse_o2']:.4f}\") scores['r2_o1'] = r2_score( test_y.detach().numpy(), mu.detach().numpy() ) scores['r2_o2'] = r2_score( test_y.detach().numpy(), mu_2 ) print(f\"R2 (o1): {scores['r2_o1']:.4f}\") print(f\"R2 (o2): {scores['r2_o2']:.4f}\") MAE (o1): 0.0964 MAE (o2): 0.1047 MSE (o1): 0.0254 MSE (o2): 0.0261 RMSE (o1): 0.1594 RMSE (o2): 0.1616 R2 (o1): 0.9644 R2 (o2): 0.9635 {\"a27034fa663d4f309c52cd9adc92fabf\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"HBoxModel\", \"state\": {\"_view_name\": \"HBoxView\", \"_dom_classes\": [], \"_model_name\": \"HBoxModel\", \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"box_style\": \"\", \"layout\": \"IPY_MODEL_d74d1c4bfb704b06a839c2da92ba3b5c\", \"_model_module\": \"@jupyter-widgets/controls\", \"children\": [\"IPY_MODEL_85fe61b44f2843e39acd1781822e81da\", \"IPY_MODEL_b0e47dcde45f4da8b2d6b3838eb29ccb\", \"IPY_MODEL_6dc50433782f4658a04aa328c14bd8fa\"]}}, \"d74d1c4bfb704b06a839c2da92ba3b5c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}, \"85fe61b44f2843e39acd1781822e81da\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"HTMLModel\", \"state\": {\"_view_name\": \"HTMLView\", \"style\": \"IPY_MODEL_3ccd989ab4d94666b68ea3612d91e5ae\", \"_dom_classes\": [], \"description\": \"\", \"_model_name\": \"HTMLModel\", \"placeholder\": \"\\u200b\", \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"value\": \"100%\", \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"description_tooltip\": null, \"_model_module\": \"@jupyter-widgets/controls\", \"layout\": \"IPY_MODEL_752b2a0ba2cf4bf2afd2eb53d5709cbd\"}}, \"b0e47dcde45f4da8b2d6b3838eb29ccb\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"FloatProgressModel\", \"state\": {\"_view_name\": \"ProgressView\", \"style\": \"IPY_MODEL_fa0775971f8348aeab138f65e697c138\", \"_dom_classes\": [], \"description\": \"\", \"_model_name\": \"FloatProgressModel\", \"bar_style\": \"success\", \"max\": 100, \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"value\": 100, \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"orientation\": \"horizontal\", \"min\": 0, \"description_tooltip\": null, \"_model_module\": \"@jupyter-widgets/controls\", \"layout\": \"IPY_MODEL_061184ba7947494e9be4fadc9e3a8a83\"}}, \"6dc50433782f4658a04aa328c14bd8fa\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"HTMLModel\", \"state\": {\"_view_name\": \"HTMLView\", \"style\": \"IPY_MODEL_753149e3dfc24270b644b213cbf1fac8\", \"_dom_classes\": [], \"description\": \"\", \"_model_name\": \"HTMLModel\", \"placeholder\": \"\\u200b\", \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"value\": \" 100/100 [00:00&lt;00:00, 150.36it/s, length_scale=1.7, loss=0.014, noise=0.0287]\", \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"description_tooltip\": null, \"_model_module\": \"@jupyter-widgets/controls\", \"layout\": \"IPY_MODEL_ead21bae4e1243feb1da3f14b164a05f\"}}, \"3ccd989ab4d94666b68ea3612d91e5ae\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"_view_name\": \"StyleView\", \"_model_name\": \"DescriptionStyleModel\", \"description_width\": \"\", \"_view_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.2.0\", \"_model_module\": \"@jupyter-widgets/controls\"}}, \"752b2a0ba2cf4bf2afd2eb53d5709cbd\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}, \"fa0775971f8348aeab138f65e697c138\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"_view_name\": \"StyleView\", \"_model_name\": \"ProgressStyleModel\", \"description_width\": \"\", \"_view_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.2.0\", \"bar_color\": null, \"_model_module\": \"@jupyter-widgets/controls\"}}, \"061184ba7947494e9be4fadc9e3a8a83\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}, \"753149e3dfc24270b644b213cbf1fac8\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"_view_name\": \"StyleView\", \"_model_name\": \"DescriptionStyleModel\", \"description_width\": \"\", \"_view_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.2.0\", \"_model_module\": \"@jupyter-widgets/controls\"}}, \"ead21bae4e1243feb1da3f14b164a05f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}}","title":"Gpytorch gp uncertain"},{"location":"notebooks/gpytorch_gp_uncertain/#gpytorch-uncertain-inputs","text":"This is my notebook where I play around with all things PyTorch. I use the following packages: PyTorch Pyro GPyTorch PyTorch Lightning #@title Install Packages !pip install --upgrade pyro-ppl gpytorch pytorch-lightning tqdm Requirement already up-to-date: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.4.0) Requirement already up-to-date: gpytorch in /usr/local/lib/python3.6/dist-packages (1.2.0) Requirement already up-to-date: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (0.9.0) Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.50.0) Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.5) Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.3.0) Requirement already satisfied, skipping upgrade: torch>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.6.0+cu101) Requirement already satisfied, skipping upgrade: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.2) Requirement already satisfied, skipping upgrade: tensorboard==2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.2.0) Requirement already satisfied, skipping upgrade: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1) Requirement already satisfied, skipping upgrade: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2) Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (20.4) Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.7.0) Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.35.1) Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (2.23.0) Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.15.0) Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.4.1) Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.10.0) Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.2.2) Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.17.2) Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.32.0) Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.12.4) Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (50.3.0) Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.0.1) Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytorch-lightning) (2.4.7) Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (2020.6.20) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (1.24.3) Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (3.0.4) Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning) (2.10) Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (1.3.0) Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (2.0.0) Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.2.8) Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.6) Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.1.1) Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (3.1.0) Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (3.2.0) Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.4.8) #@title Load Packages # TYPE HINTS from typing import Tuple, Optional, Dict, Callable, Union # PyTorch Settings import torch # Pyro Settings # GPyTorch Settings import gpytorch # PyTorch Lightning Settings import pytorch_lightning as pl import tqdm # NUMPY SETTINGS import numpy as np np.set_printoptions(precision=3, suppress=True) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns.set_context(context='talk',font_scale=0.7) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd.set_option(\"display.max_rows\", 120) pd.set_option(\"display.max_columns\", 120) # LOGGING SETTINGS import sys import logging logging.basicConfig( level=logging.INFO, stream=sys.stdout, format='%(asctime)s:%(levelname)s:%(message)s' ) logger = logging.getLogger() #logger.setLevel(logging.INFO) %load_ext autoreload %autoreload 2","title":"GPyTorch - Uncertain Inputs"},{"location":"notebooks/gpytorch_gp_uncertain/#data","text":"from sklearn.utils import check_random_state def near_square_wave( n_train: int=80, input_noise: float=0.3, output_noise: float=0.15, n_test: int=400, random_state: int=123, ): # function f = lambda x: np.sin(1.0 * np.pi / 1.6 * np.cos(5 + .5 * x)) # create clean inputs x_mu = np.linspace(-10, 10, n_train) # clean outputs y = f(x_mu) # generate noise x_rng = check_random_state(random_state) y_rng = check_random_state(random_state + 1) # noisy inputs x = x_mu + input_noise * x_rng.randn(x_mu.shape[0]) # noisy outputs y = f(x_mu) + output_noise * y_rng.randn(x_mu.shape[0]) # test points x_test = np.linspace(-12, 12, n_test) + x_rng.randn(n_test) y_test = f(np.linspace(-12, 12, n_test)) x_test = np.sort(x_test) return x[:, None], y[:, None], x_test[:, None], y_test","title":"Data"},{"location":"notebooks/gpytorch_gp_uncertain/#example-i-constant-input-error","text":"import math # function f = lambda x: torch.sin(1.0 * math.pi / 1.6 * torch.cos(5 + .5 * x)) # training inputs train_x_mean = torch.linspace(-10, 10, 80) # noise for outputs train_x_stdv = 0.3 * torch.ones(train_x_mean.shape[0]) # training points train_x = train_x_mean + train_x_stdv # true function (near squared sine wave) y_rng = check_random_state(123 + 1) output_noise = 0.15 real_y = f(train_x) train_y = real_y + output_noise * torch.randn(train_x_mean.size()) fig, ax = plt.subplots() ax.plot(train_x, real_y) ax.errorbar(train_x, train_y, xerr=1.96 * train_x_stdv, fmt='k.') plt.show() train_x_stdv.shape torch.Size([80]) #@title GP Model class ExactGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood): super(ExactGPModel, self).__init__(train_x, train_y, likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) def forward(self, x): mean_x = self.mean_module(x) covar_x = self.covar_module(x) return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) # initialize likelihood and model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(train_x, train_y, likelihood) training_iter = 100 # Find optimal model hyperparameters model.train() likelihood.train() # Use the adam optimizer optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # Includes GaussianLikelihood parameters # \"Loss\" for GPs - the marginal log likelihood mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) iterator = tqdm.notebook.tqdm(range(training_iter)) for i in iterator: # Zero gradients from previous iteration optimizer.zero_grad() # Output from model output = model(train_x) # Calc loss and backprop gradients loss = -mll(output, train_y) loss.backward() iterator.set_postfix( loss=loss.item(), length_scale=model.covar_module.base_kernel.lengthscale.item(), noise=model.likelihood.noise.item() ) # print('Iter %d/%d - Loss: %.3f lengthscale: %.3f noise: %.3f' % ( # i + 1, training_iter, loss.item(), # model.covar_module.base_kernel.lengthscale.item(), # model.likelihood.noise.item() # )) optimizer.step() var element = $('#b4488314-68b9-4e03-b6c5-31806f7dbff7'); {\"model_id\": \"a27034fa663d4f309c52cd9adc92fabf\", \"version_minor\": 0, \"version_major\": 2} #@title Predictions # Get into evaluation (predictive posterior) mode model.eval() likelihood.eval() # Test points are regularly spaced along [0,1] # Make predictions by feeding model through likelihood with torch.no_grad(), gpytorch.settings.fast_pred_var(): test_x = torch.linspace(-12, 12, 100) observed_pred = likelihood(model(test_x)) with torch.no_grad(): # Initialize plot # f, ax = plt.subplots(1, 1, figsize=(8, 3)) # Get upper and lower confidence bounds lower, upper = observed_pred.confidence_region() # # Plot training data as black stars # ax.errorbar(train_x_mean.numpy(), train_y.numpy(), xerr=train_x_stdv, fmt='k*') # # Plot predictive means as blue line # ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b') # # Shade between the lower and upper confidence bounds # ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5) # ax.set_ylim([-3, 3]) # ax.legend(['Observed Data', 'Mean', 'Confidence']) fig, ax = plt.subplots() ax.scatter(train_x_mean.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") ax.plot( test_x.numpy().squeeze(), observed_pred.mean.numpy().squeeze(), label=r\"Predictive Mean\", color=\"black\", linewidth=3, ) ax.fill_between( test_x.numpy().squeeze(), lower.numpy(), upper.numpy(), alpha=0.3, color=\"darkorange\", label=f\"Predictive Std\", ) ax.set_ylim([-3.5, 2.5]) ax.legend(fontsize=12) plt.tight_layout() # fig.savefig(FIG_PATH.joinpath(\"1d_gp_taylor_1o.png\")) # fig.savefig(\"figures/jaxgp/examples/taylor/1d_gp_taylor_1o.png\") plt.show()","title":"Example I - Constant Input Error"},{"location":"notebooks/gpytorch_gp_uncertain/#linearized-gp","text":"test_x = torch.linspace(-12, 12, 100) test_y = f(test_x) v = torch.ones(100) def predict_mean(test_x): return likelihood(model(test_x)).mean def predict_var(test_x): return likelihood(model(test_x)).variance def predict_covar(test_x): return likelihood(model(test_x)).covariance_matrix def predict_mean_sum(test_x): return likelihood(model(test_x)).mean.sum() mu = predict_mean(test_x) var = predict_var(test_x) covar = predict_covar(test_x) _ , dx = torch.autograd.functional.vjp(predict_mean, test_x, v) _, dx2 = torch.autograd.functional.vhp(predict_mean_sum, test_x, v) # f, ax = plt.subplots(1, 1, figsize=(8, 5)) plt.figure(figsize=(8, 5)) plt.plot(mu.detach().numpy(), label=\"Mean\") plt.plot(dx.detach().numpy(), label=\"Jacobian\") plt.plot(dx2.detach().numpy(), label=\"Hessian\") plt.legend() plt.show()","title":"Linearized GP"},{"location":"notebooks/gpytorch_gp_uncertain/#predictive-mean-variance","text":"var_e = var.detach().numpy()[:, np.newaxis] cov_e = covar.detach().numpy() var_x = train_x_stdv ** 2 cov_x = np.array([0.3])[:, np.newaxis] # 1st order approximation to1 = dx.detach().numpy()[:, np.newaxis].dot(cov_x.dot(dx.detach().numpy()[:, np.newaxis].T)) var_to1 = var_e + np.diag(to1)[:, np.newaxis] # 2nd order approximation to2 = np.dot(dx2.detach().numpy().squeeze()[:, np.newaxis, np.newaxis], cov_x) to2 = np.trace(to2, axis1=1, axis2=2) mu_2 = mu.detach().numpy() + 0.5 * to2.squeeze() var_to2 = var_to1 + to2[:, np.newaxis] # standard deviation std_e = np.sqrt(var_e) std_to1 = np.sqrt(var_to1) std_to2 = np.sqrt(var_to2) /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in sqrt plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy(), label=\"Predictive Mean\") plt.plot(test_x.detach().numpy().squeeze(), mu_2, label=\"Predictive Mean (2nd Order)\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"red\", label=\"Real Function\", color='black', linewidth=3) plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.legend() plt.show() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Saw kwargs ['c', 'color'] which are all aliases for 'color'. Kept value from 'color'. Passing multiple aliases for the same property will raise a TypeError in 3.3. after removing the cwd from sys.path. # f, ax = plt.subplots(1, 1, figsize=(8, 5)) plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy(), label=\"Predictive Mean\") plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"black\", label=\"Real Function\") plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() - 1.96 * std_e.squeeze(), label='Upper Limit') plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() + 1.96 * std_e.squeeze(), label='Lower Limit') plt.legend() plt.show() plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy(), label=\"Predictive Mean\") plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"black\", label=\"Real Function\") plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() - 1.96 * std_to1.squeeze(), label='Upper Limit') plt.plot(test_x.detach().numpy().squeeze(), mu.detach().numpy().squeeze() + 1.96 * std_to1.squeeze(), label='Lower Limit') plt.legend() plt.show() plt.figure(figsize=(8, 5)) plt.plot(test_x.detach().numpy().squeeze(), mu_2, label=\"Predictive Mean\") plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\", label=\"Training Data\") plt.plot(train_x.numpy(), real_y.numpy(), c=\"black\", label=\"Real Function\") plt.plot(test_x.detach().numpy().squeeze(), mu_2.squeeze() - 1.96 * std_to2.squeeze(), label='Upper Limit') plt.plot(test_x.detach().numpy().squeeze(), mu_2.squeeze() + 1.96 * std_to2.squeeze(), label='Lower Limit') plt.legend() plt.show() from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error scores = {} scores['mae_o1'] = mean_absolute_error( test_y.detach().numpy(), mu.detach().numpy() ) scores['mae_o2'] = mean_absolute_error( test_y.detach().numpy(), mu_2 ) print(f\"MAE (o1): {scores['mae_o1']:.4f}\") print(f\"MAE (o2): {scores['mae_o2']:.4f}\") scores['mse_o1'] = mean_squared_error( test_y.detach().numpy(), mu.detach().numpy() ) scores['mse_o2'] = mean_squared_error( test_y.detach().numpy(), mu_2 ) print(f\"MSE (o1): {scores['mse_o1']:.4f}\") print(f\"MSE (o2): {scores['mse_o2']:.4f}\") scores['rmse_o1'] = np.sqrt(scores['mse_o1']) scores['rmse_o2'] = np.sqrt(scores['mse_o2']) print(f\"RMSE (o1): {scores['rmse_o1']:.4f}\") print(f\"RMSE (o2): {scores['rmse_o2']:.4f}\") scores['r2_o1'] = r2_score( test_y.detach().numpy(), mu.detach().numpy() ) scores['r2_o2'] = r2_score( test_y.detach().numpy(), mu_2 ) print(f\"R2 (o1): {scores['r2_o1']:.4f}\") print(f\"R2 (o2): {scores['r2_o2']:.4f}\") MAE (o1): 0.0964 MAE (o2): 0.1047 MSE (o1): 0.0254 MSE (o2): 0.0261 RMSE (o1): 0.1594 RMSE (o2): 0.1616 R2 (o1): 0.9644 R2 (o2): 0.9635 {\"a27034fa663d4f309c52cd9adc92fabf\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"HBoxModel\", \"state\": {\"_view_name\": \"HBoxView\", \"_dom_classes\": [], \"_model_name\": \"HBoxModel\", \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"box_style\": \"\", \"layout\": \"IPY_MODEL_d74d1c4bfb704b06a839c2da92ba3b5c\", \"_model_module\": \"@jupyter-widgets/controls\", \"children\": [\"IPY_MODEL_85fe61b44f2843e39acd1781822e81da\", \"IPY_MODEL_b0e47dcde45f4da8b2d6b3838eb29ccb\", \"IPY_MODEL_6dc50433782f4658a04aa328c14bd8fa\"]}}, \"d74d1c4bfb704b06a839c2da92ba3b5c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}, \"85fe61b44f2843e39acd1781822e81da\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"HTMLModel\", \"state\": {\"_view_name\": \"HTMLView\", \"style\": \"IPY_MODEL_3ccd989ab4d94666b68ea3612d91e5ae\", \"_dom_classes\": [], \"description\": \"\", \"_model_name\": \"HTMLModel\", \"placeholder\": \"\\u200b\", \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"value\": \"100%\", \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"description_tooltip\": null, \"_model_module\": \"@jupyter-widgets/controls\", \"layout\": \"IPY_MODEL_752b2a0ba2cf4bf2afd2eb53d5709cbd\"}}, \"b0e47dcde45f4da8b2d6b3838eb29ccb\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"FloatProgressModel\", \"state\": {\"_view_name\": \"ProgressView\", \"style\": \"IPY_MODEL_fa0775971f8348aeab138f65e697c138\", \"_dom_classes\": [], \"description\": \"\", \"_model_name\": \"FloatProgressModel\", \"bar_style\": \"success\", \"max\": 100, \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"value\": 100, \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"orientation\": \"horizontal\", \"min\": 0, \"description_tooltip\": null, \"_model_module\": \"@jupyter-widgets/controls\", \"layout\": \"IPY_MODEL_061184ba7947494e9be4fadc9e3a8a83\"}}, \"6dc50433782f4658a04aa328c14bd8fa\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"HTMLModel\", \"state\": {\"_view_name\": \"HTMLView\", \"style\": \"IPY_MODEL_753149e3dfc24270b644b213cbf1fac8\", \"_dom_classes\": [], \"description\": \"\", \"_model_name\": \"HTMLModel\", \"placeholder\": \"\\u200b\", \"_view_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"value\": \" 100/100 [00:00&lt;00:00, 150.36it/s, length_scale=1.7, loss=0.014, noise=0.0287]\", \"_view_count\": null, \"_view_module_version\": \"1.5.0\", \"description_tooltip\": null, \"_model_module\": \"@jupyter-widgets/controls\", \"layout\": \"IPY_MODEL_ead21bae4e1243feb1da3f14b164a05f\"}}, \"3ccd989ab4d94666b68ea3612d91e5ae\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"_view_name\": \"StyleView\", \"_model_name\": \"DescriptionStyleModel\", \"description_width\": \"\", \"_view_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.2.0\", \"_model_module\": \"@jupyter-widgets/controls\"}}, \"752b2a0ba2cf4bf2afd2eb53d5709cbd\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}, \"fa0775971f8348aeab138f65e697c138\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"_view_name\": \"StyleView\", \"_model_name\": \"ProgressStyleModel\", \"description_width\": \"\", \"_view_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.2.0\", \"bar_color\": null, \"_model_module\": \"@jupyter-widgets/controls\"}}, \"061184ba7947494e9be4fadc9e3a8a83\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}, \"753149e3dfc24270b644b213cbf1fac8\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"_view_name\": \"StyleView\", \"_model_name\": \"DescriptionStyleModel\", \"description_width\": \"\", \"_view_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.5.0\", \"_view_count\": null, \"_view_module_version\": \"1.2.0\", \"_model_module\": \"@jupyter-widgets/controls\"}}, \"ead21bae4e1243feb1da3f14b164a05f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_name\": \"LayoutModel\", \"state\": {\"_view_name\": \"LayoutView\", \"grid_template_rows\": null, \"right\": null, \"justify_content\": null, \"_view_module\": \"@jupyter-widgets/base\", \"overflow\": null, \"_model_module_version\": \"1.2.0\", \"_view_count\": null, \"flex_flow\": null, \"width\": null, \"min_width\": null, \"border\": null, \"align_items\": null, \"bottom\": null, \"_model_module\": \"@jupyter-widgets/base\", \"top\": null, \"grid_column\": null, \"overflow_y\": null, \"overflow_x\": null, \"grid_auto_flow\": null, \"grid_area\": null, \"grid_template_columns\": null, \"flex\": null, \"_model_name\": \"LayoutModel\", \"justify_items\": null, \"grid_row\": null, \"max_height\": null, \"align_content\": null, \"visibility\": null, \"align_self\": null, \"height\": null, \"min_height\": null, \"padding\": null, \"grid_auto_rows\": null, \"grid_gap\": null, \"max_width\": null, \"order\": null, \"_view_module_version\": \"1.2.0\", \"grid_template_areas\": null, \"object_position\": null, \"object_fit\": null, \"grid_auto_columns\": null, \"margin\": null, \"display\": null, \"left\": null}}}","title":"Predictive Mean &amp; Variance"},{"location":"notebooks/numpyro_egp_mcmc/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exact GP w. MCMC \u00b6 In this example we show how to use NUTS to sample from the posterior over the hyperparameters of a gaussian process. Source : Numpyro Example import sys from pyprojroot import here sys . path . append ( str ( here ())) from dataclasses import dataclass import time import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers import jax from jax import vmap import jax.numpy as np import jax.random as random from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 @dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 num_chains = 1 num_warmup = 1_000 num_samples = 1_000 device = 'cpu' numpyro . set_platform ( args . device ) numpyro . set_host_device_count ( args . num_chains ) # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , ) GP Model \u00b6 # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = jnp . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * jnp . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * jnp . eye ( X . shape [ 0 ]) return k @jax . jit def model ( X , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = jnp . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y , ) Inference \u00b6 # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True , ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( \" \\n MCMC elapsed time:\" , time . time () - start ) return mcmc . get_samples () Training \u00b6 # do inference rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) samples = run_inference ( model , args , rng_key , X , y ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [42:33<00:00, 1.28s/it, 1023 steps of size 4.98e-06. acc. prob=0.95] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1960118272.00 256.13 1960118528.00 1960118528.00 1960118528.00 0.50 1.00 kernel_noise 0.00 0.00 0.00 0.00 0.00 nan nan kernel_var 2.63 0.00 2.63 2.63 2.63 6.13 1.00 Number of divergences: 0 MCMC elapsed time: 2557.5514323711395 /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/numpyro/diagnostics.py:172: RuntimeWarning: invalid value encountered in true_divide rho_k = 1. - (var_within - gamma_k_c.mean(axis=0)) / var_estimator Predictions \u00b6 def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = jnp . linalg . inv ( k_XX ) K = k_pp - jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , jnp . transpose ( k_pX ))) sigma_noise = jnp . sqrt ( jnp . clip ( jnp . diag ( K ), a_min = 0.0 )) * jax . random . normal ( rng_key , X_test . shape [: 1 ] ) mean = jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ \"kernel_var\" ], samples [ \"kernel_length\" ], samples [ \"kernel_noise\" ], ) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , y , X_test , var , length , noise ) )( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) Results \u00b6 # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , y , \"kx\" ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = \"lightblue\" ) # plot mean prediction ax . plot ( X_test , mean_prediction , \"blue\" , ls = \"solid\" , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) # plt.savefig(\"numpyro_gp_plot.png\") plt . tight_layout () Experiment \u00b6 GP Model - Uncertain Inputs \u00b6 def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"Numpyro egp mcmc"},{"location":"notebooks/numpyro_egp_mcmc/#exact-gp-w-mcmc","text":"In this example we show how to use NUTS to sample from the posterior over the hyperparameters of a gaussian process. Source : Numpyro Example import sys from pyprojroot import here sys . path . append ( str ( here ())) from dataclasses import dataclass import time import functools from typing import Callable , Dict , Tuple import argparse import jax import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS sns . reset_defaults () # sns.set_style('whitegrid') # sns.set_context('talk') sns . set_context ( context = \"talk\" , font_scale = 0.7 ) import numpy as onp import tqdm from jax.experimental import optimizers import jax from jax import vmap import jax.numpy as np import jax.random as random from src.models.jaxgp.data import get_data from src.models.jaxgp.exact import predictive_mean , predictive_variance from src.models.jaxgp.kernels import gram , rbf_kernel , ard_kernel from src.models.jaxgp.loss import marginal_likelihood from src.models.jaxgp.mean import zero_mean from src.models.jaxgp.utils import cholesky_factorization , get_factorizations , saturate % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Exact GP w. MCMC"},{"location":"notebooks/numpyro_egp_mcmc/#data","text":"@dataclass class args : num_train = 30 num_test = 1_000 smoke_test = False input_noise = 0.15 output_noise = 0.15 num_chains = 1 num_warmup = 1_000 num_samples = 1_000 device = 'cpu' numpyro . set_platform ( args . device ) numpyro . set_host_device_count ( args . num_chains ) # sigma_inputs = 0.15 input_cov = jnp . array ([ args . input_noise ]) . reshape ( - 1 , 1 ) X , y , Xtest , ytest = get_data ( N = args . num_train , input_noise = args . input_noise , output_noise = args . output_noise , N_test = args . num_test , )","title":"Data"},{"location":"notebooks/numpyro_egp_mcmc/#gp-model","text":"# squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = jnp . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * jnp . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * jnp . eye ( X . shape [ 0 ]) return k @jax . jit def model ( X , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = jnp . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y , )","title":"GP Model"},{"location":"notebooks/numpyro_egp_mcmc/#inference","text":"# helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True , ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( \" \\n MCMC elapsed time:\" , time . time () - start ) return mcmc . get_samples ()","title":"Inference"},{"location":"notebooks/numpyro_egp_mcmc/#training","text":"# do inference rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) samples = run_inference ( model , args , rng_key , X , y ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [42:33<00:00, 1.28s/it, 1023 steps of size 4.98e-06. acc. prob=0.95] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1960118272.00 256.13 1960118528.00 1960118528.00 1960118528.00 0.50 1.00 kernel_noise 0.00 0.00 0.00 0.00 0.00 nan nan kernel_var 2.63 0.00 2.63 2.63 2.63 6.13 1.00 Number of divergences: 0 MCMC elapsed time: 2557.5514323711395 /home/emmanuel/.conda/envs/egp/lib/python3.8/site-packages/numpyro/diagnostics.py:172: RuntimeWarning: invalid value encountered in true_divide rho_k = 1. - (var_within - gamma_k_c.mean(axis=0)) / var_estimator","title":"Training"},{"location":"notebooks/numpyro_egp_mcmc/#predictions","text":"def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = jnp . linalg . inv ( k_XX ) K = k_pp - jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , jnp . transpose ( k_pX ))) sigma_noise = jnp . sqrt ( jnp . clip ( jnp . diag ( K ), a_min = 0.0 )) * jax . random . normal ( rng_key , X_test . shape [: 1 ] ) mean = jnp . matmul ( k_pX , jnp . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ \"kernel_var\" ], samples [ \"kernel_length\" ], samples [ \"kernel_noise\" ], ) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , y , X_test , var , length , noise ) )( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 )","title":"Predictions"},{"location":"notebooks/numpyro_egp_mcmc/#results","text":"# make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , y , \"kx\" ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = \"lightblue\" ) # plot mean prediction ax . plot ( X_test , mean_prediction , \"blue\" , ls = \"solid\" , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) # plt.savefig(\"numpyro_gp_plot.png\") plt . tight_layout ()","title":"Results"},{"location":"notebooks/numpyro_egp_mcmc/#experiment","text":"","title":"Experiment"},{"location":"notebooks/numpyro_egp_mcmc/#gp-model-uncertain-inputs","text":"def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"GP Model - Uncertain Inputs"},{"location":"notebooks/numpyro_gps/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Jax PlayGround \u00b6 My starting notebook where I install all of the necessary libraries and load some easy 1D/2D Regression data to play around with. #@title Install Packages !pip install jax jaxlib --force !pip install \"git+https://github.com/pyro-ppl/numpyro.git#egg=numpyro\" Collecting jaxlib Using cached https://files.pythonhosted.org/packages/a0/e2/7e2c7e5b2b2b06c0868f8408f5ed016f8ee83540381cfe43d96bf1e8463b/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl Collecting numpy>=1.12 Using cached https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl Collecting scipy Using cached https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl Collecting absl-py Using cached https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl Collecting six Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl ERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed. ERROR: nbclient 0.5.0 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible. ERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible. Installing collected packages: numpy, scipy, six, absl-py, jaxlib Found existing installation: scipy 1.5.2 Uninstalling scipy-1.5.2: Successfully uninstalled scipy-1.5.2 Found existing installation: six 1.15.0 Uninstalling six-1.15.0: Successfully uninstalled six-1.15.0 Found existing installation: absl-py 0.10.0 Uninstalling absl-py-0.10.0: Successfully uninstalled absl-py-0.10.0 Found existing installation: jaxlib 0.1.55 Uninstalling jaxlib-0.1.55: Successfully uninstalled jaxlib-0.1.55 Successfully installed absl-py-0.10.0 jaxlib-0.1.55 numpy-1.19.2 scipy-1.5.2 six-1.15.0 #@title Load Packages # TYPE HINTS from typing import Tuple, Optional, Dict, Callable, Union # JAX SETTINGS import jax import jax.numpy as np from jax import random, lax import numpyro import numpyro.distributions as dist from numpyro.infer import SVI from numpyro.infer.autoguide import AutoDiagonalNormal from numpyro.infer import SVI, ELBO numpyro.set_platform(\"0\") # NUMPY SETTINGS import numpy as onp onp.set_printoptions(precision=3, suppress=True) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns.set_context(context='talk',font_scale=0.7) # PANDAS SETTINGS import pandas as pd pd.set_option(\"display.max_rows\", 120) pd.set_option(\"display.max_columns\", 120) # LOGGING SETTINGS import sys import logging logging.basicConfig( level=logging.INFO, stream=sys.stdout, format='%(asctime)s:%(levelname)s:%(message)s' ) logger = logging.getLogger() #logger.setLevel(logging.INFO) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload #@title Data def get_data( N: int = 30, input_noise: float = 0.15, output_noise: float = 0.15, N_test: int = 400, ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, None]: onp.random.seed(0) X = np.linspace(-1, 1, N) Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y += output_noise * onp.random.randn(N) Y -= np.mean(Y) Y /= np.std(Y) X += input_noise * onp.random.randn(N) assert X.shape == (N,) assert Y.shape == (N,) X_test = np.linspace(-1.2, 1.2, N_test) return X[:, None], Y[:, None], X_test[:, None] seed = 123 rng = onp.random.RandomState(seed) n_samples = 50 noise = 0.05 X = np.linspace(-2*np.pi, 2*np.pi, n_samples) y = np.sinc(X) + 0.01 * rng.randn(n_samples) # Store data as torch.tensors. # Plot data and true function. plt.scatter(X, y, label='data') plt.xlabel('x') plt.ylabel('y = f(x)') plt.legend(); Numpyro Model \u00b6 # One-dimensional squared exponential kernel with diagonal noise term. def squared_exp_cov_1D(X, Y, variance, lengthscale): deltaXsq = np.power((X[:, None] - Y) / lengthscale, 2.0) K = variance * np.exp(-0.5 * deltaXsq) return K # GP model. def GP(X, y): # Set informative log-normal priors on kernel hyperparameters. variance = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, 0.1)) lengthscale = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, 1.0)) sigma = numpyro.sample(\"sigma\", dist.LogNormal(0.0, 1.0)) # Compute kernel K = squared_exp_cov_1D(X, X, variance, lengthscale) K += np.eye(X.shape[0]) * np.power(sigma, 2) # Sample y according to the standard gaussian process formula numpyro.sample(\"y\", dist.MultivariateNormal(loc=np.zeros(X.shape[0]), covariance_matrix=K), obs=y) Inference - SVI \u00b6 from numpyro import handlers with handlers.seed(rng_seed=0): i = GP(X, y) print(i) None %%time from numpyro.infer.autoguide import AutoLaplaceApproximation, AutoMultivariateNormal, AutoDiagonalNormal from numpyro.infer import TraceMeanField_ELBO # Compile guide = AutoDiagonalNormal(GP) optimizer = numpyro.optim.Adam(step_size=0.01) svi = SVI(GP, guide, optimizer, loss=TraceMeanField_ELBO()) init_state = svi.init(random.PRNGKey(1), X, y) CPU times: user 130 ms, sys: 98.2 ms, total: 228 ms Wall time: 122 ms %%time # Run optimizer for 1000 iteratons. state, losses = lax.scan(lambda state, i: svi.update(state, X, y), init_state, np.arange(5000)) # Extract surrogate posterior. params = svi.get_params(state) plt.plot(losses); plt.title(\"Negative ELBO (Loss)\"); /usr/local/lib/python3.6/dist-packages/numpyro/infer/elbo.py:105: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: kernel_var kernel_length sigmaGuide sites: kernel_length kernel_var sigma \"Guide sites:\\n \" + \"\\n \".join(guide_sites)) CPU times: user 2.47 s, sys: 407 ms, total: 2.88 s Wall time: 2.24 s Sample Posterior Distribution \u00b6 n_test_samples = 100 advi_samples = guide.get_posterior(params).sample(random.PRNGKey(seed), (n_test_samples, )) Parameters \u00b6 fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(15, 3)) labels = [\"Variance\", \"Length Scale\", \"Noise\"] for i, ilabel in enumerate(labels): ax[i].hist(advi_samples[:, i], density=True, bins=20, label=ilabel) ax[i].legend() plt.show() Predictive Mean & Variance \u00b6 def predict(kernel, X, Y, X_test, var, length, noise): # compute kernels between train and test data, etc. k_pp = kernel(X_test, X_test, var, length) k_pX = kernel(X_test, X, var, length) k_XX = kernel(X, X, var, length) k_XX += np.eye(X.shape[0]) * np.power(noise, 2) K_xx_inv = np.linalg.inv(k_XX) K = k_pp - np.matmul(k_pX, np.matmul(K_xx_inv, np.transpose(k_pX))) mean = np.matmul(k_pX, np.matmul(K_xx_inv, Y)) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean n_test_samples = 10 advi_samples = guide.get_posterior(params).sample(random.PRNGKey(seed), (n_test_samples, )) X_test = np.linspace(-2.2 * np.pi, 2.2 * np.pi, 1_000) preds = [predict(squared_exp_cov_1D, X, y, X_test, iparam[0], iparam[1], iparam[2]) for iparam in advi_samples] predictions = np.vstack(preds) predictions.shape (10, 1000) # Summarize function posterior. ci = 95 ci_lower = (100 - ci) / 2 ci_upper = (100 + ci) / 2 preds_mean = predictions.mean(0) preds_lower = np.percentile(predictions, ci_lower, axis=0) preds_upper = np.percentile(predictions, ci_upper, axis=0) plt.plot(X_test, preds_mean) plt.plot(X_test, preds_lower) plt.plot(X_test, preds_upper) [<matplotlib.lines.Line2D at 0x7fa2508dd550>] fig, ax = plt.subplots() ax.plot(X_test, preds_mean) ax.fill_between( X_test.squeeze(), preds_lower.squeeze(), preds_upper.squeeze(), color='darkorange', alpha=0.2, label='95% Confidence' ) # ax.set_ylim([-3, 3]) # ax.set_xlim([-10.2, 10.2]) ax.legend() plt.tight_layout() plt.show()","title":"Numpyro gps"},{"location":"notebooks/numpyro_gps/#jax-playground","text":"My starting notebook where I install all of the necessary libraries and load some easy 1D/2D Regression data to play around with. #@title Install Packages !pip install jax jaxlib --force !pip install \"git+https://github.com/pyro-ppl/numpyro.git#egg=numpyro\" Collecting jaxlib Using cached https://files.pythonhosted.org/packages/a0/e2/7e2c7e5b2b2b06c0868f8408f5ed016f8ee83540381cfe43d96bf1e8463b/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl Collecting numpy>=1.12 Using cached https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl Collecting scipy Using cached https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl Collecting absl-py Using cached https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl Collecting six Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl ERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed. ERROR: nbclient 0.5.0 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible. ERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible. Installing collected packages: numpy, scipy, six, absl-py, jaxlib Found existing installation: scipy 1.5.2 Uninstalling scipy-1.5.2: Successfully uninstalled scipy-1.5.2 Found existing installation: six 1.15.0 Uninstalling six-1.15.0: Successfully uninstalled six-1.15.0 Found existing installation: absl-py 0.10.0 Uninstalling absl-py-0.10.0: Successfully uninstalled absl-py-0.10.0 Found existing installation: jaxlib 0.1.55 Uninstalling jaxlib-0.1.55: Successfully uninstalled jaxlib-0.1.55 Successfully installed absl-py-0.10.0 jaxlib-0.1.55 numpy-1.19.2 scipy-1.5.2 six-1.15.0 #@title Load Packages # TYPE HINTS from typing import Tuple, Optional, Dict, Callable, Union # JAX SETTINGS import jax import jax.numpy as np from jax import random, lax import numpyro import numpyro.distributions as dist from numpyro.infer import SVI from numpyro.infer.autoguide import AutoDiagonalNormal from numpyro.infer import SVI, ELBO numpyro.set_platform(\"0\") # NUMPY SETTINGS import numpy as onp onp.set_printoptions(precision=3, suppress=True) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns.set_context(context='talk',font_scale=0.7) # PANDAS SETTINGS import pandas as pd pd.set_option(\"display.max_rows\", 120) pd.set_option(\"display.max_columns\", 120) # LOGGING SETTINGS import sys import logging logging.basicConfig( level=logging.INFO, stream=sys.stdout, format='%(asctime)s:%(levelname)s:%(message)s' ) logger = logging.getLogger() #logger.setLevel(logging.INFO) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload #@title Data def get_data( N: int = 30, input_noise: float = 0.15, output_noise: float = 0.15, N_test: int = 400, ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, None]: onp.random.seed(0) X = np.linspace(-1, 1, N) Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y += output_noise * onp.random.randn(N) Y -= np.mean(Y) Y /= np.std(Y) X += input_noise * onp.random.randn(N) assert X.shape == (N,) assert Y.shape == (N,) X_test = np.linspace(-1.2, 1.2, N_test) return X[:, None], Y[:, None], X_test[:, None] seed = 123 rng = onp.random.RandomState(seed) n_samples = 50 noise = 0.05 X = np.linspace(-2*np.pi, 2*np.pi, n_samples) y = np.sinc(X) + 0.01 * rng.randn(n_samples) # Store data as torch.tensors. # Plot data and true function. plt.scatter(X, y, label='data') plt.xlabel('x') plt.ylabel('y = f(x)') plt.legend();","title":"Jax PlayGround"},{"location":"notebooks/numpyro_gps/#numpyro-model","text":"# One-dimensional squared exponential kernel with diagonal noise term. def squared_exp_cov_1D(X, Y, variance, lengthscale): deltaXsq = np.power((X[:, None] - Y) / lengthscale, 2.0) K = variance * np.exp(-0.5 * deltaXsq) return K # GP model. def GP(X, y): # Set informative log-normal priors on kernel hyperparameters. variance = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, 0.1)) lengthscale = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, 1.0)) sigma = numpyro.sample(\"sigma\", dist.LogNormal(0.0, 1.0)) # Compute kernel K = squared_exp_cov_1D(X, X, variance, lengthscale) K += np.eye(X.shape[0]) * np.power(sigma, 2) # Sample y according to the standard gaussian process formula numpyro.sample(\"y\", dist.MultivariateNormal(loc=np.zeros(X.shape[0]), covariance_matrix=K), obs=y)","title":"Numpyro Model"},{"location":"notebooks/numpyro_gps/#inference-svi","text":"from numpyro import handlers with handlers.seed(rng_seed=0): i = GP(X, y) print(i) None %%time from numpyro.infer.autoguide import AutoLaplaceApproximation, AutoMultivariateNormal, AutoDiagonalNormal from numpyro.infer import TraceMeanField_ELBO # Compile guide = AutoDiagonalNormal(GP) optimizer = numpyro.optim.Adam(step_size=0.01) svi = SVI(GP, guide, optimizer, loss=TraceMeanField_ELBO()) init_state = svi.init(random.PRNGKey(1), X, y) CPU times: user 130 ms, sys: 98.2 ms, total: 228 ms Wall time: 122 ms %%time # Run optimizer for 1000 iteratons. state, losses = lax.scan(lambda state, i: svi.update(state, X, y), init_state, np.arange(5000)) # Extract surrogate posterior. params = svi.get_params(state) plt.plot(losses); plt.title(\"Negative ELBO (Loss)\"); /usr/local/lib/python3.6/dist-packages/numpyro/infer/elbo.py:105: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: kernel_var kernel_length sigmaGuide sites: kernel_length kernel_var sigma \"Guide sites:\\n \" + \"\\n \".join(guide_sites)) CPU times: user 2.47 s, sys: 407 ms, total: 2.88 s Wall time: 2.24 s","title":"Inference - SVI"},{"location":"notebooks/numpyro_gps/#sample-posterior-distribution","text":"n_test_samples = 100 advi_samples = guide.get_posterior(params).sample(random.PRNGKey(seed), (n_test_samples, ))","title":"Sample Posterior Distribution"},{"location":"notebooks/numpyro_gps/#parameters","text":"fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(15, 3)) labels = [\"Variance\", \"Length Scale\", \"Noise\"] for i, ilabel in enumerate(labels): ax[i].hist(advi_samples[:, i], density=True, bins=20, label=ilabel) ax[i].legend() plt.show()","title":"Parameters"},{"location":"notebooks/numpyro_gps/#predictive-mean-variance","text":"def predict(kernel, X, Y, X_test, var, length, noise): # compute kernels between train and test data, etc. k_pp = kernel(X_test, X_test, var, length) k_pX = kernel(X_test, X, var, length) k_XX = kernel(X, X, var, length) k_XX += np.eye(X.shape[0]) * np.power(noise, 2) K_xx_inv = np.linalg.inv(k_XX) K = k_pp - np.matmul(k_pX, np.matmul(K_xx_inv, np.transpose(k_pX))) mean = np.matmul(k_pX, np.matmul(K_xx_inv, Y)) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean n_test_samples = 10 advi_samples = guide.get_posterior(params).sample(random.PRNGKey(seed), (n_test_samples, )) X_test = np.linspace(-2.2 * np.pi, 2.2 * np.pi, 1_000) preds = [predict(squared_exp_cov_1D, X, y, X_test, iparam[0], iparam[1], iparam[2]) for iparam in advi_samples] predictions = np.vstack(preds) predictions.shape (10, 1000) # Summarize function posterior. ci = 95 ci_lower = (100 - ci) / 2 ci_upper = (100 + ci) / 2 preds_mean = predictions.mean(0) preds_lower = np.percentile(predictions, ci_lower, axis=0) preds_upper = np.percentile(predictions, ci_upper, axis=0) plt.plot(X_test, preds_mean) plt.plot(X_test, preds_lower) plt.plot(X_test, preds_upper) [<matplotlib.lines.Line2D at 0x7fa2508dd550>] fig, ax = plt.subplots() ax.plot(X_test, preds_mean) ax.fill_between( X_test.squeeze(), preds_lower.squeeze(), preds_upper.squeeze(), color='darkorange', alpha=0.2, label='95% Confidence' ) # ax.set_ylim([-3, 3]) # ax.set_xlim([-10.2, 10.2]) ax.legend() plt.tight_layout() plt.show()","title":"Predictive Mean &amp; Variance"},{"location":"notebooks/objax_gp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Jax PlayGround \u00b6 My starting notebook where I install all of the necessary libraries and load some easy 1D/2D Regression data to play around with. #@title Install Packages !pip install jax jaxlib !pip install \"git+https://github.com/google/objax.git\" !pip install \"git+https://github.com/deepmind/chex.git\" !pip install \"git+https://github.com/deepmind/dm-haiku\" !pip install \"git+https://github.com/Information-Fusion-Lab-Umass/NuX\" !pip install \"git+https://github.com/pyro-ppl/numpyro.git#egg=numpyro\" !pip uninstall tensorflow -y -q !pip install -Uq tfp-nightly[jax] > /dev/null Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.75) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.52) Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.10.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0) Collecting git+https://github.com/google/objax.git Cloning https://github.com/google/objax.git to /tmp/pip-req-build-cmqfg5f3 Running command git clone -q https://github.com/google/objax.git /tmp/pip-req-build-cmqfg5f3 Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (1.4.1) Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (1.18.5) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (7.0.0) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (0.1.52) Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (0.1.75) Requirement already satisfied: tensorboard>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (2.3.0) Collecting parameterized Downloading https://files.pythonhosted.org/packages/ba/6b/73dfed0ab5299070cf98451af50130989901f50de41fe85d605437a0210f/parameterized-0.7.4-py2.py3-none-any.whl Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib->objax==1.0.2) (0.10.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->objax==1.0.2) (3.3.0) Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (50.3.0) Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.32.0) Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.0.1) Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (3.2.2) Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (0.4.1) Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (2.23.0) Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (0.35.1) Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.17.2) Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (3.12.4) Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.7.0) Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.15.0) Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.3.0->objax==1.0.2) (2.0.0) Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax==1.0.2) (1.3.0) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (2020.6.20) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (1.24.3) Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (4.1.1) Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (4.6) Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (0.2.8) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.3.0->objax==1.0.2) (3.2.0) Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax==1.0.2) (3.1.0) Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (0.4.8) Building wheels for collected packages: objax Building wheel for objax (setup.py) ... done Created wheel for objax: filename=objax-1.0.2-cp36-none-any.whl size=64182 sha256=83c24197ad3d62b42c8897a1c8c3bcb4d3c9684c159bfb500f67b5aac82c0753 Stored in directory: /tmp/pip-ephem-wheel-cache-wy9fypuj/wheels/ff/75/37/8672991ae92977b2180136f69557c03ced92f87c74eff31761 Successfully built objax Installing collected packages: parameterized, objax Successfully installed objax-1.0.2 parameterized-0.7.4 Collecting git+https://github.com/deepmind/chex.git Cloning https://github.com/deepmind/chex.git to /tmp/pip-req-build-oui3l10s Running command git clone -q https://github.com/deepmind/chex.git /tmp/pip-req-build-oui3l10s Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.10.0) Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.1.75) Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.1.52) Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (1.18.5) Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.11.1) Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.7) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.9.0->chex==0.0.2) (1.15.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax>=0.1.55->chex==0.0.2) (3.3.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib>=0.1.37->chex==0.0.2) (1.4.1) Building wheels for collected packages: chex Building wheel for chex (setup.py) ... done Created wheel for chex: filename=chex-0.0.2-cp36-none-any.whl size=44002 sha256=30674f7c9349eb65b8a34cb8528347bafb3dd29c4fb3f70ae868aa71bd168926 Stored in directory: /tmp/pip-ephem-wheel-cache-f79rv8h3/wheels/36/04/94/19c6b9a94d01685be55d12c1ada626f07828e0e4486dcb81ea Successfully built chex Installing collected packages: chex Successfully installed chex-0.0.2 Collecting git+https://github.com/deepmind/dm-haiku Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-h3p9xhwo Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-h3p9xhwo Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (0.10.0) Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (1.18.5) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.7.1->dm-haiku==0.0.2) (1.15.0) Building wheels for collected packages: dm-haiku Building wheel for dm-haiku (setup.py) ... done Created wheel for dm-haiku: filename=dm_haiku-0.0.2-cp36-none-any.whl size=293301 sha256=9576abccac877acd3875444d00f07df6a8d9f61067f382b32805324c2abb0c3c Stored in directory: /tmp/pip-ephem-wheel-cache-xnh_0n7r/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499 Successfully built dm-haiku Installing collected packages: dm-haiku Successfully installed dm-haiku-0.0.2 Collecting git+https://github.com/Information-Fusion-Lab-Umass/NuX Cloning https://github.com/Information-Fusion-Lab-Umass/NuX to /tmp/pip-req-build-9cddfy42 Running command git clone -q https://github.com/Information-Fusion-Lab-Umass/NuX /tmp/pip-req-build-9cddfy42 Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from nux==1.0.2) (1.18.5) Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from nux==1.0.2) (0.1.75) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from nux==1.0.2) (0.1.52) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->nux==1.0.2) (3.3.0) Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax->nux==1.0.2) (0.10.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib->nux==1.0.2) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax->nux==1.0.2) (1.15.0) Building wheels for collected packages: nux Building wheel for nux (setup.py) ... done Created wheel for nux: filename=nux-1.0.2-cp36-none-any.whl size=48811 sha256=aa131a1f61296fb04af02a29c9fa79a7a1e007b1165d9a7184eb7c48b0188a82 Stored in directory: /tmp/pip-ephem-wheel-cache-pd8_7ba1/wheels/66/78/5c/a78e73a116ea5b926e1a923369e0de3ebcb26f731b37321a23 Successfully built nux Installing collected packages: nux Successfully installed nux-1.0.2 Collecting numpyro Cloning https://github.com/pyro-ppl/numpyro.git to /tmp/pip-install-sw3d2b2p/numpyro Running command git clone -q https://github.com/pyro-ppl/numpyro.git /tmp/pip-install-sw3d2b2p/numpyro Collecting jax>=0.2 Downloading https://files.pythonhosted.org/packages/e1/d9/9bd335976d3b61f705c2e9c35da2c6e030f9cd9ffd3e111feb99d8d169a7/jax-0.2.0.tar.gz (454kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 460kB 2.8MB/s Collecting jaxlib>=0.1.55 Downloading https://files.pythonhosted.org/packages/a0/e2/7e2c7e5b2b2b06c0868f8408f5ed016f8ee83540381cfe43d96bf1e8463b/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl (31.9MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.9MB 142kB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.41.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax>=0.2->numpyro) (1.18.5) Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax>=0.2->numpyro) (0.10.0) Requirement already satisfied: opt_einsum in /usr/local/lib/python3.6/dist-packages (from jax>=0.2->numpyro) (3.3.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib>=0.1.55->numpyro) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax>=0.2->numpyro) (1.15.0) Building wheels for collected packages: numpyro, jax Building wheel for numpyro (setup.py) ... done Created wheel for numpyro: filename=numpyro-0.3.0-cp36-none-any.whl size=175165 sha256=d124ed6a83a31136cd17459e6bfa6fec7d50bcbc3a6f2fc5815ca76821f50e5f Stored in directory: /tmp/pip-ephem-wheel-cache-0gst43qz/wheels/d3/66/a6/667201f3fa85a83c93cc19efb1c1c5869a7b53f450a4031b52 Building wheel for jax (setup.py) ... done Created wheel for jax: filename=jax-0.2.0-cp36-none-any.whl size=522279 sha256=5381086a9e5a03dc3d075de4a3fae5acf2aa73d2238be2d02116a2df59202135 Stored in directory: /root/.cache/pip/wheels/99/f1/91/e9c21aca3142a6d2e5e760162fd65a1430438b7630a0b75591 Successfully built numpyro jax Installing collected packages: jax, jaxlib, numpyro Found existing installation: jax 0.1.75 Uninstalling jax-0.1.75: Successfully uninstalled jax-0.1.75 Found existing installation: jaxlib 0.1.52 Uninstalling jaxlib-0.1.52: Successfully uninstalled jaxlib-0.1.52 Successfully installed jax-0.2.0 jaxlib-0.1.55 numpyro-0.3.0 #@title Load Packages # TYPE HINTS from typing import Tuple, Optional, Dict, Callable, Union # JAX SETTINGS import jax import jax.numpy as np import jax.random as random import objax from tensorflow_probability.substrates import jax as tfp tfd = tfp.distributions tfb = tfp.bijectors tfpk = tfp.math.psd_kernels # NUMPY SETTINGS import numpy as onp onp.set_printoptions(precision=3, suppress=True) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns.set_context(context='talk',font_scale=0.7) # PANDAS SETTINGS import pandas as pd pd.set_option(\"display.max_rows\", 120) pd.set_option(\"display.max_columns\", 120) # LOGGING SETTINGS import sys import logging logging.basicConfig( level=logging.INFO, stream=sys.stdout, format='%(asctime)s:%(levelname)s:%(message)s' ) logger = logging.getLogger() #logger.setLevel(logging.INFO) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload #@title Data def get_data( N: int = 30, input_noise: float = 0.15, output_noise: float = 0.15, N_test: int = 400, ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, None]: onp.random.seed(0) X = np.linspace(-1, 1, N) Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y += output_noise * onp.random.randn(N) Y -= np.mean(Y) Y /= np.std(Y) X += input_noise * onp.random.randn(N) assert X.shape == (N,) assert Y.shape == (N,) X_test = np.linspace(-1.2, 1.2, N_test) return X[:, None], Y[:, None], X_test[:, None] X, y, Xtest = get_data(100, 0.0, 0.05, 100) /usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.') Kernel Functions \u00b6 from functools import partial def covariance_matrix( func: Callable, x: np.ndarray, y: np.ndarray, ) -> np.ndarray: \"\"\"Computes the covariance matrix. Given a function `Callable` and some `params`, we can use the `jax.vmap` function to calculate the gram matrix as the function applied to each of the points. Parameters ---------- kernel_func : Callable a callable function (kernel or distance) params : Dict the parameters needed for the kernel x : jax.numpy.ndarray input dataset (n_samples, n_features) y : jax.numpy.ndarray other input dataset (n_samples, n_features) Returns ------- mat : jax.ndarray the gram matrix. Notes ----- There is little difference between this function and `gram` See Also -------- jax.kernels.gram Examples -------- >>> covariance_matrix(kernel_rbf, {\"gamma\": 1.0}, X, Y) \"\"\" mapx1 = jax.vmap(lambda x, y: func(x=x, y=y), in_axes=(0, None), out_axes=0) mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1) return mapx2(x, y) def rbf_kernel(gamma: float, x: np.ndarray, y: np.ndarray) -> np.ndarray: \"\"\"Radial Basis Function (RBF) Kernel. The most popular kernel in all of kernel methods. .. math:: k(\\mathbf{x,y}) = \\\\ \\\\exp \\left( - \\\\gamma\\\\ ||\\\\mathbf{x} - \\\\mathbf{y}||^2_2\\\\ \\\\right) Parameters ---------- params : Dict the parameters needed for the kernel x : jax.numpy.ndarray input dataset (n_samples, n_features) y : jax.numpy.ndarray other input dataset (n_samples, n_features) Returns ------- kernel_mat : jax.numpy.ndarray the kernel matrix (n_samples, n_samples) References ---------- .. [1] David Duvenaud, *Kernel Cookbook* \"\"\" return np.exp(- gamma * sqeuclidean_distance(x, y)) def ard_kernel(x: np.ndarray, y: np.ndarray, length_scale, amplitude) -> np.ndarray: \"\"\"Radial Basis Function (RBF) Kernel. The most popular kernel in all of kernel methods. .. math:: k(\\mathbf{x,y}) = \\\\ \\\\exp \\left( - \\\\gamma\\\\ ||\\\\mathbf{x} - \\\\mathbf{y}||^2_2\\\\ \\\\right) Parameters ---------- params : Dict the parameters needed for the kernel x : jax.numpy.ndarray input dataset (n_samples, n_features) y : jax.numpy.ndarray other input dataset (n_samples, n_features) Returns ------- kernel_mat : jax.numpy.ndarray the kernel matrix (n_samples, n_samples) References ---------- .. [1] David Duvenaud, *Kernel Cookbook* \"\"\" x = x / length_scale y = y / length_scale # return the ard kernel return amplitude * np.exp(-sqeuclidean_distance(x, y)) def sqeuclidean_distance(x: np.array, y: np.array) -> float: return np.sum((x - y) ** 2) class RBFKernel(objax.Module): def __init__(self): self.gamma = objax.TrainVar(np.array([0.1])) def __call__(self, X: np.ndarray, Y: np.ndarray)-> np.ndarray: kernel_func = partial(rbf_kernel, gamma=self.gamma.value) return covariance_matrix(kernel_func, X, Y).squeeze() class ARDKernel(objax.Module): def __init__(self): self.length_scale = objax.TrainVar(np.array([0.1])) self.amplitude = objax.TrainVar(np.array([1.])) def __call__(self, X: np.ndarray, Y: np.ndarray)-> np.ndarray: kernel_func = partial( ard_kernel, length_scale=jax.nn.softplus(self.length_scale.value), amplitude=jax.nn.softplus(self.amplitude.value) ) return covariance_matrix(kernel_func, X, Y).squeeze() class ZeroMean(objax.Module): def __init__(self): pass def __call__(self, X: np.ndarray) -> np.ndarray: return np.zeros(X.shape[-1], dtype=X.dtype) class LinearMean(objax.Module): def __init__(self, input_dim, output_dim): self.w = objax.TrainVar(objax.random.normal((input_dim, output_dim))) self.b = objax.TrainVar(np.zeros(output_dim)) def __call__(self, X: np.ndarray) -> np.ndarray: return np.dot(X.T, self.w.value) + self.b.value class GaussianLikelihood(objax.Module): def __init__(self): self.noise = objax.TrainVar(np.array([0.1])) def __call__(self, X: np.ndarray) -> np.ndarray: return np.zeros(X.shape[-1], dtype=X.dtype) class ExactGP(objax.Module): def __init__(self, input_dim, output_dim, jitter): # MEAN FUNCTION self.mean = ZeroMean() # KERNEL Function self.kernel = ARDKernel() # noise level self.noise = objax.TrainVar(np.array([0.1])) # jitter (make it correctly conditioned) self.jitter = jitter def forward(self, X: np.ndarray) -> np.ndarray: # mean function mu = self.mean(X) # kernel function cov = self.kernel(X, X) # noise model cov += jax.nn.softplus(self.noise.value) * np.eye(X.shape[0]) # jitter cov += self.jitter * np.eye(X.shape[0]) # calculate cholesky cov_chol = np.linalg.cholesky(cov) # gaussian process likelihood return tfd.MultivariateNormalTriL(loc=mu, scale_tril=cov_chol) def predict(self, X: np.ndarray) -> np.ndarray: pass def sample(self, n_samples: int, key: None) -> np.ndarray: pass gp_model = ExactGP(X.shape[0], 1, 1e-5) dist = gp_model.forward(X) gp_model.vars() {'(ExactGP).kernel(ARDKernel).amplitude': <objax.variable.TrainVar at 0x7f9f94237f28>, '(ExactGP).kernel(ARDKernel).length_scale': <objax.variable.TrainVar at 0x7f9f94237e10>, '(ExactGP).noise': <objax.variable.TrainVar at 0x7f9f94237f98>} plt.imshow(dist.covariance()) <matplotlib.image.AxesImage at 0x7f9f8ff3a4a8> key = random.PRNGKey(0) samples = dist.sample(10, key) plt.plot(samples.T) # Settings lr = 0.01 # learning rate batch = 256 epochs = 50 gp_model = ExactGP(X.shape[0], 1, 1e-5) def loss(X, label): dist = gp_model.forward(X) return - dist.log_prob(label).mean() opt = objax.optimizer.SGD(gp_model.vars()) gv = objax.GradValues(loss, gp_model.vars()) def train_op(x, label): g, v = gv(x, label) # returns gradients, loss opt(lr, g) return v # This line is optional: it is compiling the code to make it faster. train_op = objax.Jit(train_op, gv.vars() + opt.vars()) losses = [] for epoch in range(epochs): # Train loss = train_op(X, y.squeeze()) losses.append(loss) gp_model.noise.value, jax.nn.softplus(gp_model.noise.value) (DeviceArray([-5.328], dtype=float32), DeviceArray([0.005], dtype=float32)) plt.plot(losses) [<matplotlib.lines.Line2D at 0x7f9fb6288048>] Posterior \u00b6 from typing import Tuple, Optional, Callable def cholesky_factorization(K: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, bool]: \"\"\"Cholesky Factorization\"\"\" # cho factor the cholesky L = jax.scipy.linalg.cho_factor(K, lower=True) # weights weights = jax.scipy.linalg.cho_solve(L, Y) return L, weights def get_factorizations( X: np.ndarray, Y: np.ndarray, likelihood_noise: float, mean_f: Callable, kernel: Callable, ) -> Tuple[Tuple[np.ndarray, bool], np.ndarray]: \"\"\"Cholesky Factorization\"\"\" # ========================== # 1. GP PRIOR # ========================== mu_x = mean_f(X) Kxx = kernel(X, X) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== print(mu_x) print(Y.reshape(-1, 1).shape, mu_x.reshape(-1, 1).shape) L, alpha = cholesky_factorization( Kxx + likelihood_noise * np.eye(Kxx.shape[0]), Y.reshape(-1, 1) - mu_x.reshape(-1, 1), ) # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ return L, alpha def posterior( Xnew, X, y, likelihood_noise, mean_f, kernel ): # L, alpha = get_factorizations( X, y, likelihood_noise, mean_f, kernel ) K_Xx = gp_model.kernel(Xnew, X) # Calculate the Mean mu_y = np.dot(K_Xx, alpha) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== v = jax.scipy.linalg.cho_solve(L, K_Xx.T) # Calculate kernel matrix for inputs K_xx = gp_model.kernel(Xnew, Xnew) cov_y = K_xx - np.dot(K_Xx, v) return mu_y, cov_y mu, cov = posterior( X, X, y.squeeze(), jax.nn.softplus(gp_model.noise.value), gp_model.mean, gp_model.kernel ) [0.] (100, 1) (1, 1) DeviceArray([[-1.585], [-1.575], [-1.562], [-1.546], [-1.527]], dtype=float32) (1.96 * np.sqrt(np.diag(cov))).shape, mu.shape ((100,), (100, 1)) plt.plot(X, mu) plt.plot(X, mu.squeeze() + 1.96 * np.sqrt(np.diag(cov) + jax.nn.softplus(gp_model.noise.value))) plt.plot(X, mu.squeeze() - 1.96 * np.sqrt(np.diag(cov) + jax.nn.softplus(gp_model.noise.value))) plt.show() dist loss(dist, y.squeeze()) DeviceArray(-0.267, dtype=float32) #@title Distribution Data from scipy.stats import beta a, b = 3.0, 10.0 data_dist = beta(a, b) x_samples = data_dist.rvs(1_000, 123) # x_samples = data_dist.rvs(1_000, 123) plt.hist(x_samples, bins=100);","title":"Objax gp"},{"location":"notebooks/objax_gp/#jax-playground","text":"My starting notebook where I install all of the necessary libraries and load some easy 1D/2D Regression data to play around with. #@title Install Packages !pip install jax jaxlib !pip install \"git+https://github.com/google/objax.git\" !pip install \"git+https://github.com/deepmind/chex.git\" !pip install \"git+https://github.com/deepmind/dm-haiku\" !pip install \"git+https://github.com/Information-Fusion-Lab-Umass/NuX\" !pip install \"git+https://github.com/pyro-ppl/numpyro.git#egg=numpyro\" !pip uninstall tensorflow -y -q !pip install -Uq tfp-nightly[jax] > /dev/null Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.75) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.52) Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.10.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0) Collecting git+https://github.com/google/objax.git Cloning https://github.com/google/objax.git to /tmp/pip-req-build-cmqfg5f3 Running command git clone -q https://github.com/google/objax.git /tmp/pip-req-build-cmqfg5f3 Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (1.4.1) Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (1.18.5) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (7.0.0) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (0.1.52) Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (0.1.75) Requirement already satisfied: tensorboard>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from objax==1.0.2) (2.3.0) Collecting parameterized Downloading https://files.pythonhosted.org/packages/ba/6b/73dfed0ab5299070cf98451af50130989901f50de41fe85d605437a0210f/parameterized-0.7.4-py2.py3-none-any.whl Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib->objax==1.0.2) (0.10.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->objax==1.0.2) (3.3.0) Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (50.3.0) Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.32.0) Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.0.1) Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (3.2.2) Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (0.4.1) Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (2.23.0) Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (0.35.1) Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.17.2) Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (3.12.4) Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.7.0) Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax==1.0.2) (1.15.0) Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.3.0->objax==1.0.2) (2.0.0) Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax==1.0.2) (1.3.0) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (2020.6.20) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax==1.0.2) (1.24.3) Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (4.1.1) Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (4.6) Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (0.2.8) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.3.0->objax==1.0.2) (3.2.0) Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax==1.0.2) (3.1.0) Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax==1.0.2) (0.4.8) Building wheels for collected packages: objax Building wheel for objax (setup.py) ... done Created wheel for objax: filename=objax-1.0.2-cp36-none-any.whl size=64182 sha256=83c24197ad3d62b42c8897a1c8c3bcb4d3c9684c159bfb500f67b5aac82c0753 Stored in directory: /tmp/pip-ephem-wheel-cache-wy9fypuj/wheels/ff/75/37/8672991ae92977b2180136f69557c03ced92f87c74eff31761 Successfully built objax Installing collected packages: parameterized, objax Successfully installed objax-1.0.2 parameterized-0.7.4 Collecting git+https://github.com/deepmind/chex.git Cloning https://github.com/deepmind/chex.git to /tmp/pip-req-build-oui3l10s Running command git clone -q https://github.com/deepmind/chex.git /tmp/pip-req-build-oui3l10s Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.10.0) Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.1.75) Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.1.52) Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (1.18.5) Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.11.1) Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from chex==0.0.2) (0.7) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.9.0->chex==0.0.2) (1.15.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax>=0.1.55->chex==0.0.2) (3.3.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib>=0.1.37->chex==0.0.2) (1.4.1) Building wheels for collected packages: chex Building wheel for chex (setup.py) ... done Created wheel for chex: filename=chex-0.0.2-cp36-none-any.whl size=44002 sha256=30674f7c9349eb65b8a34cb8528347bafb3dd29c4fb3f70ae868aa71bd168926 Stored in directory: /tmp/pip-ephem-wheel-cache-f79rv8h3/wheels/36/04/94/19c6b9a94d01685be55d12c1ada626f07828e0e4486dcb81ea Successfully built chex Installing collected packages: chex Successfully installed chex-0.0.2 Collecting git+https://github.com/deepmind/dm-haiku Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-h3p9xhwo Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-h3p9xhwo Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (0.10.0) Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (1.18.5) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.7.1->dm-haiku==0.0.2) (1.15.0) Building wheels for collected packages: dm-haiku Building wheel for dm-haiku (setup.py) ... done Created wheel for dm-haiku: filename=dm_haiku-0.0.2-cp36-none-any.whl size=293301 sha256=9576abccac877acd3875444d00f07df6a8d9f61067f382b32805324c2abb0c3c Stored in directory: /tmp/pip-ephem-wheel-cache-xnh_0n7r/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499 Successfully built dm-haiku Installing collected packages: dm-haiku Successfully installed dm-haiku-0.0.2 Collecting git+https://github.com/Information-Fusion-Lab-Umass/NuX Cloning https://github.com/Information-Fusion-Lab-Umass/NuX to /tmp/pip-req-build-9cddfy42 Running command git clone -q https://github.com/Information-Fusion-Lab-Umass/NuX /tmp/pip-req-build-9cddfy42 Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from nux==1.0.2) (1.18.5) Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from nux==1.0.2) (0.1.75) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from nux==1.0.2) (0.1.52) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->nux==1.0.2) (3.3.0) Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax->nux==1.0.2) (0.10.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib->nux==1.0.2) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax->nux==1.0.2) (1.15.0) Building wheels for collected packages: nux Building wheel for nux (setup.py) ... done Created wheel for nux: filename=nux-1.0.2-cp36-none-any.whl size=48811 sha256=aa131a1f61296fb04af02a29c9fa79a7a1e007b1165d9a7184eb7c48b0188a82 Stored in directory: /tmp/pip-ephem-wheel-cache-pd8_7ba1/wheels/66/78/5c/a78e73a116ea5b926e1a923369e0de3ebcb26f731b37321a23 Successfully built nux Installing collected packages: nux Successfully installed nux-1.0.2 Collecting numpyro Cloning https://github.com/pyro-ppl/numpyro.git to /tmp/pip-install-sw3d2b2p/numpyro Running command git clone -q https://github.com/pyro-ppl/numpyro.git /tmp/pip-install-sw3d2b2p/numpyro Collecting jax>=0.2 Downloading https://files.pythonhosted.org/packages/e1/d9/9bd335976d3b61f705c2e9c35da2c6e030f9cd9ffd3e111feb99d8d169a7/jax-0.2.0.tar.gz (454kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 460kB 2.8MB/s Collecting jaxlib>=0.1.55 Downloading https://files.pythonhosted.org/packages/a0/e2/7e2c7e5b2b2b06c0868f8408f5ed016f8ee83540381cfe43d96bf1e8463b/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl (31.9MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.9MB 142kB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.41.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax>=0.2->numpyro) (1.18.5) Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax>=0.2->numpyro) (0.10.0) Requirement already satisfied: opt_einsum in /usr/local/lib/python3.6/dist-packages (from jax>=0.2->numpyro) (3.3.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib>=0.1.55->numpyro) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax>=0.2->numpyro) (1.15.0) Building wheels for collected packages: numpyro, jax Building wheel for numpyro (setup.py) ... done Created wheel for numpyro: filename=numpyro-0.3.0-cp36-none-any.whl size=175165 sha256=d124ed6a83a31136cd17459e6bfa6fec7d50bcbc3a6f2fc5815ca76821f50e5f Stored in directory: /tmp/pip-ephem-wheel-cache-0gst43qz/wheels/d3/66/a6/667201f3fa85a83c93cc19efb1c1c5869a7b53f450a4031b52 Building wheel for jax (setup.py) ... done Created wheel for jax: filename=jax-0.2.0-cp36-none-any.whl size=522279 sha256=5381086a9e5a03dc3d075de4a3fae5acf2aa73d2238be2d02116a2df59202135 Stored in directory: /root/.cache/pip/wheels/99/f1/91/e9c21aca3142a6d2e5e760162fd65a1430438b7630a0b75591 Successfully built numpyro jax Installing collected packages: jax, jaxlib, numpyro Found existing installation: jax 0.1.75 Uninstalling jax-0.1.75: Successfully uninstalled jax-0.1.75 Found existing installation: jaxlib 0.1.52 Uninstalling jaxlib-0.1.52: Successfully uninstalled jaxlib-0.1.52 Successfully installed jax-0.2.0 jaxlib-0.1.55 numpyro-0.3.0 #@title Load Packages # TYPE HINTS from typing import Tuple, Optional, Dict, Callable, Union # JAX SETTINGS import jax import jax.numpy as np import jax.random as random import objax from tensorflow_probability.substrates import jax as tfp tfd = tfp.distributions tfb = tfp.bijectors tfpk = tfp.math.psd_kernels # NUMPY SETTINGS import numpy as onp onp.set_printoptions(precision=3, suppress=True) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns.set_context(context='talk',font_scale=0.7) # PANDAS SETTINGS import pandas as pd pd.set_option(\"display.max_rows\", 120) pd.set_option(\"display.max_columns\", 120) # LOGGING SETTINGS import sys import logging logging.basicConfig( level=logging.INFO, stream=sys.stdout, format='%(asctime)s:%(levelname)s:%(message)s' ) logger = logging.getLogger() #logger.setLevel(logging.INFO) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload #@title Data def get_data( N: int = 30, input_noise: float = 0.15, output_noise: float = 0.15, N_test: int = 400, ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, None]: onp.random.seed(0) X = np.linspace(-1, 1, N) Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y += output_noise * onp.random.randn(N) Y -= np.mean(Y) Y /= np.std(Y) X += input_noise * onp.random.randn(N) assert X.shape == (N,) assert Y.shape == (N,) X_test = np.linspace(-1.2, 1.2, N_test) return X[:, None], Y[:, None], X_test[:, None] X, y, Xtest = get_data(100, 0.0, 0.05, 100) /usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn('No GPU/TPU found, falling back to CPU.')","title":"Jax PlayGround"},{"location":"notebooks/objax_gp/#kernel-functions","text":"from functools import partial def covariance_matrix( func: Callable, x: np.ndarray, y: np.ndarray, ) -> np.ndarray: \"\"\"Computes the covariance matrix. Given a function `Callable` and some `params`, we can use the `jax.vmap` function to calculate the gram matrix as the function applied to each of the points. Parameters ---------- kernel_func : Callable a callable function (kernel or distance) params : Dict the parameters needed for the kernel x : jax.numpy.ndarray input dataset (n_samples, n_features) y : jax.numpy.ndarray other input dataset (n_samples, n_features) Returns ------- mat : jax.ndarray the gram matrix. Notes ----- There is little difference between this function and `gram` See Also -------- jax.kernels.gram Examples -------- >>> covariance_matrix(kernel_rbf, {\"gamma\": 1.0}, X, Y) \"\"\" mapx1 = jax.vmap(lambda x, y: func(x=x, y=y), in_axes=(0, None), out_axes=0) mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1) return mapx2(x, y) def rbf_kernel(gamma: float, x: np.ndarray, y: np.ndarray) -> np.ndarray: \"\"\"Radial Basis Function (RBF) Kernel. The most popular kernel in all of kernel methods. .. math:: k(\\mathbf{x,y}) = \\\\ \\\\exp \\left( - \\\\gamma\\\\ ||\\\\mathbf{x} - \\\\mathbf{y}||^2_2\\\\ \\\\right) Parameters ---------- params : Dict the parameters needed for the kernel x : jax.numpy.ndarray input dataset (n_samples, n_features) y : jax.numpy.ndarray other input dataset (n_samples, n_features) Returns ------- kernel_mat : jax.numpy.ndarray the kernel matrix (n_samples, n_samples) References ---------- .. [1] David Duvenaud, *Kernel Cookbook* \"\"\" return np.exp(- gamma * sqeuclidean_distance(x, y)) def ard_kernel(x: np.ndarray, y: np.ndarray, length_scale, amplitude) -> np.ndarray: \"\"\"Radial Basis Function (RBF) Kernel. The most popular kernel in all of kernel methods. .. math:: k(\\mathbf{x,y}) = \\\\ \\\\exp \\left( - \\\\gamma\\\\ ||\\\\mathbf{x} - \\\\mathbf{y}||^2_2\\\\ \\\\right) Parameters ---------- params : Dict the parameters needed for the kernel x : jax.numpy.ndarray input dataset (n_samples, n_features) y : jax.numpy.ndarray other input dataset (n_samples, n_features) Returns ------- kernel_mat : jax.numpy.ndarray the kernel matrix (n_samples, n_samples) References ---------- .. [1] David Duvenaud, *Kernel Cookbook* \"\"\" x = x / length_scale y = y / length_scale # return the ard kernel return amplitude * np.exp(-sqeuclidean_distance(x, y)) def sqeuclidean_distance(x: np.array, y: np.array) -> float: return np.sum((x - y) ** 2) class RBFKernel(objax.Module): def __init__(self): self.gamma = objax.TrainVar(np.array([0.1])) def __call__(self, X: np.ndarray, Y: np.ndarray)-> np.ndarray: kernel_func = partial(rbf_kernel, gamma=self.gamma.value) return covariance_matrix(kernel_func, X, Y).squeeze() class ARDKernel(objax.Module): def __init__(self): self.length_scale = objax.TrainVar(np.array([0.1])) self.amplitude = objax.TrainVar(np.array([1.])) def __call__(self, X: np.ndarray, Y: np.ndarray)-> np.ndarray: kernel_func = partial( ard_kernel, length_scale=jax.nn.softplus(self.length_scale.value), amplitude=jax.nn.softplus(self.amplitude.value) ) return covariance_matrix(kernel_func, X, Y).squeeze() class ZeroMean(objax.Module): def __init__(self): pass def __call__(self, X: np.ndarray) -> np.ndarray: return np.zeros(X.shape[-1], dtype=X.dtype) class LinearMean(objax.Module): def __init__(self, input_dim, output_dim): self.w = objax.TrainVar(objax.random.normal((input_dim, output_dim))) self.b = objax.TrainVar(np.zeros(output_dim)) def __call__(self, X: np.ndarray) -> np.ndarray: return np.dot(X.T, self.w.value) + self.b.value class GaussianLikelihood(objax.Module): def __init__(self): self.noise = objax.TrainVar(np.array([0.1])) def __call__(self, X: np.ndarray) -> np.ndarray: return np.zeros(X.shape[-1], dtype=X.dtype) class ExactGP(objax.Module): def __init__(self, input_dim, output_dim, jitter): # MEAN FUNCTION self.mean = ZeroMean() # KERNEL Function self.kernel = ARDKernel() # noise level self.noise = objax.TrainVar(np.array([0.1])) # jitter (make it correctly conditioned) self.jitter = jitter def forward(self, X: np.ndarray) -> np.ndarray: # mean function mu = self.mean(X) # kernel function cov = self.kernel(X, X) # noise model cov += jax.nn.softplus(self.noise.value) * np.eye(X.shape[0]) # jitter cov += self.jitter * np.eye(X.shape[0]) # calculate cholesky cov_chol = np.linalg.cholesky(cov) # gaussian process likelihood return tfd.MultivariateNormalTriL(loc=mu, scale_tril=cov_chol) def predict(self, X: np.ndarray) -> np.ndarray: pass def sample(self, n_samples: int, key: None) -> np.ndarray: pass gp_model = ExactGP(X.shape[0], 1, 1e-5) dist = gp_model.forward(X) gp_model.vars() {'(ExactGP).kernel(ARDKernel).amplitude': <objax.variable.TrainVar at 0x7f9f94237f28>, '(ExactGP).kernel(ARDKernel).length_scale': <objax.variable.TrainVar at 0x7f9f94237e10>, '(ExactGP).noise': <objax.variable.TrainVar at 0x7f9f94237f98>} plt.imshow(dist.covariance()) <matplotlib.image.AxesImage at 0x7f9f8ff3a4a8> key = random.PRNGKey(0) samples = dist.sample(10, key) plt.plot(samples.T) # Settings lr = 0.01 # learning rate batch = 256 epochs = 50 gp_model = ExactGP(X.shape[0], 1, 1e-5) def loss(X, label): dist = gp_model.forward(X) return - dist.log_prob(label).mean() opt = objax.optimizer.SGD(gp_model.vars()) gv = objax.GradValues(loss, gp_model.vars()) def train_op(x, label): g, v = gv(x, label) # returns gradients, loss opt(lr, g) return v # This line is optional: it is compiling the code to make it faster. train_op = objax.Jit(train_op, gv.vars() + opt.vars()) losses = [] for epoch in range(epochs): # Train loss = train_op(X, y.squeeze()) losses.append(loss) gp_model.noise.value, jax.nn.softplus(gp_model.noise.value) (DeviceArray([-5.328], dtype=float32), DeviceArray([0.005], dtype=float32)) plt.plot(losses) [<matplotlib.lines.Line2D at 0x7f9fb6288048>]","title":"Kernel Functions"},{"location":"notebooks/objax_gp/#posterior","text":"from typing import Tuple, Optional, Callable def cholesky_factorization(K: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, bool]: \"\"\"Cholesky Factorization\"\"\" # cho factor the cholesky L = jax.scipy.linalg.cho_factor(K, lower=True) # weights weights = jax.scipy.linalg.cho_solve(L, Y) return L, weights def get_factorizations( X: np.ndarray, Y: np.ndarray, likelihood_noise: float, mean_f: Callable, kernel: Callable, ) -> Tuple[Tuple[np.ndarray, bool], np.ndarray]: \"\"\"Cholesky Factorization\"\"\" # ========================== # 1. GP PRIOR # ========================== mu_x = mean_f(X) Kxx = kernel(X, X) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== print(mu_x) print(Y.reshape(-1, 1).shape, mu_x.reshape(-1, 1).shape) L, alpha = cholesky_factorization( Kxx + likelihood_noise * np.eye(Kxx.shape[0]), Y.reshape(-1, 1) - mu_x.reshape(-1, 1), ) # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ return L, alpha def posterior( Xnew, X, y, likelihood_noise, mean_f, kernel ): # L, alpha = get_factorizations( X, y, likelihood_noise, mean_f, kernel ) K_Xx = gp_model.kernel(Xnew, X) # Calculate the Mean mu_y = np.dot(K_Xx, alpha) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== v = jax.scipy.linalg.cho_solve(L, K_Xx.T) # Calculate kernel matrix for inputs K_xx = gp_model.kernel(Xnew, Xnew) cov_y = K_xx - np.dot(K_Xx, v) return mu_y, cov_y mu, cov = posterior( X, X, y.squeeze(), jax.nn.softplus(gp_model.noise.value), gp_model.mean, gp_model.kernel ) [0.] (100, 1) (1, 1) DeviceArray([[-1.585], [-1.575], [-1.562], [-1.546], [-1.527]], dtype=float32) (1.96 * np.sqrt(np.diag(cov))).shape, mu.shape ((100,), (100, 1)) plt.plot(X, mu) plt.plot(X, mu.squeeze() + 1.96 * np.sqrt(np.diag(cov) + jax.nn.softplus(gp_model.noise.value))) plt.plot(X, mu.squeeze() - 1.96 * np.sqrt(np.diag(cov) + jax.nn.softplus(gp_model.noise.value))) plt.show() dist loss(dist, y.squeeze()) DeviceArray(-0.267, dtype=float32) #@title Distribution Data from scipy.stats import beta a, b = 3.0, 10.0 data_dist = beta(a, b) x_samples = data_dist.rvs(1_000, 123) # x_samples = data_dist.rvs(1_000, 123) plt.hist(x_samples, bins=100);","title":"Posterior"},{"location":"talks/2020_kermes/","text":"KERMES Meetup 2020 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020 Input Uncertainty Propagation in Gaussian Process Regression Models \u00b6 Resources \u00b6 Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website . Slides \u00b6","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes/#kermes-meetup-2020","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes/#input-uncertainty-propagation-in-gaussian-process-regression-models","text":"","title":"Input Uncertainty Propagation in Gaussian Process Regression Models"},{"location":"talks/2020_kermes/#resources","text":"Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website .","title":"Resources"},{"location":"talks/2020_kermes/#slides","text":"","title":"Slides"}]}