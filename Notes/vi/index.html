


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Uncertain Inputs with Gaussian Processes">
      
      
        <link rel="canonical" href="https://jejjohnson.github.io/uncertain_gps/Notes/vi/">
      
      
        <meta name="author" content="J. Emmanuel Johnson">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.1.3">
    
    
      
        <title>Uncertain Inputs GPs - Variational Strategies - Uncertain Gaussian Processes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.62d34fff.min.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/palette.c8acc6db.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=source+code+pro:300,400,400i,700%7Csource+code+pro&display=fallback">
        <style>body,input{font-family:"source code pro",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"source code pro",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="gray">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#uncertain-inputs-gps-variational-strategies" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://jejjohnson.github.io/uncertain_gps" title="Uncertain Gaussian Processes" class="md-header-nav__button md-logo" aria-label="Uncertain Gaussian Processes">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Uncertain Gaussian Processes
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Uncertain Inputs GPs - Variational Strategies
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/jejjohnson/uncertain_gps/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    jejjohnson/uncertain_gps
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://jejjohnson.github.io/uncertain_gps" title="Uncertain Gaussian Processes" class="md-nav__button md-logo" aria-label="Uncertain Gaussian Processes">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    Uncertain Gaussian Processes
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/jejjohnson/uncertain_gps/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    jejjohnson/uncertain_gps
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../literature/" title="Literature" class="md-nav__link">
      Literature
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Taylor
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Taylor" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Taylor
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../Taylor/taylor/" title="Linearized GP" class="md-nav__link">
      Linearized GP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Taylor/error_propagation/" title="Error Propagation" class="md-nav__link">
      Error Propagation
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Notebooks
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Notebooks" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Notebooks
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-1" type="checkbox" id="nav-4-1">
    
    <label class="md-nav__link" for="nav-4-1">
      JAX
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="JAX" data-md-level="2">
      <label class="md-nav__title" for="nav-4-1">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        JAX
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../notebooks/1.0_gp_basics" title="Basics" class="md-nav__link">
      Basics
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../notebooks/1.1_gp_refactored" title="Refactor" class="md-nav__link">
      Refactor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../notebooks/objax_gp" title="Objax" class="md-nav__link">
      Objax
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2">
    
    <label class="md-nav__link" for="nav-4-2">
      Numpyro
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Numpyro" data-md-level="2">
      <label class="md-nav__title" for="nav-4-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Numpyro
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../notebooks/numpyro_gps" title="Numpyro (SVI)" class="md-nav__link">
      Numpyro (SVI)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../notebooks/numpyro_egp_mcmc" title="Numpyro (MCMC)" class="md-nav__link">
      Numpyro (MCMC)
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3">
    
    <label class="md-nav__link" for="nav-4-3">
      GPyTorch
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="GPyTorch" data-md-level="2">
      <label class="md-nav__title" for="nav-4-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        GPyTorch
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../notebooks/gpytorch_gp_uncertain" title="Uncertain Inputs (Taylor)" class="md-nav__link">
      Uncertain Inputs (Taylor)
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#posterior-approximations" class="md-nav__link">
    Posterior Approximations
  </a>
  
    <nav class="md-nav" aria-label="Posterior Approximations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variational-gp-model-with-latent-inputs" class="md-nav__link">
    Variational GP Model with Latent Inputs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evidence-lower-bound-elbo" class="md-nav__link">
    Evidence Lower Bound (ELBO)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#uncertain-inputs" class="md-nav__link">
    Uncertain Inputs
  </a>
  
    <nav class="md-nav" aria-label="Uncertain Inputs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#case-i-strong-prior" class="md-nav__link">
    Case I - Strong Prior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-ii-regularized-strong-prior" class="md-nav__link">
    Case II - Regularized Strong Prior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-iii-prior-with-openness" class="md-nav__link">
    Case III - Prior with Openness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-iv-bonus-conservative-freedom" class="md-nav__link">
    Case IV - Bonus, Conservative Freedom
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    Resources
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#important-papers" class="md-nav__link">
    Important Papers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-thesis" class="md-nav__link">
    Summary Thesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#talks" class="md-nav__link">
    Talks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/jejjohnson/uncertain_gps/edit/master/docs/Notes/vi.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                <h1 id="uncertain-inputs-gps-variational-strategies">Uncertain Inputs GPs - Variational Strategies<a class="headerlink" href="#uncertain-inputs-gps-variational-strategies" title="Permanent link">&para;</a></h1>
<p>This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error.</p>
<hr />
<ul>
<li><a href="#posterior-approximations">Posterior Approximations</a></li>
<li><a href="#variational-gp-model-with-latent-inputs">Variational GP Model with Latent Inputs</a></li>
<li><a href="#evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</a></li>
<li><a href="#uncertain-inputs">Uncertain Inputs</a></li>
<li><a href="#case-i---strong-prior">Case I - Strong Prior</a></li>
<li><a href="#case-ii---regularized-strong-prior">Case II - Regularized Strong Prior</a></li>
<li><a href="#case-iii---prior-with-openness">Case III - Prior with Openness</a></li>
<li><a href="#case-iv---bonus-conservative-freedom">Case IV - Bonus, Conservative Freedom</a></li>
<li><a href="#resources">Resources</a><ul>
<li><a href="#important-papers">Important Papers</a></li>
<li><a href="#summary-thesis">Summary Thesis</a></li>
<li><a href="#talks">Talks</a></li>
</ul>
</li>
</ul>
<h2 id="posterior-approximations">Posterior Approximations<a class="headerlink" href="#posterior-approximations" title="Permanent link">&para;</a></h2>
<p>What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only (<strong>???</strong>).</p>
<p>This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution <span><span class="MathJax_Preview">q(\mathbf u)\approx \mathcal{P}(\mathbf x)</span><script type="math/tex">q(\mathbf u)\approx \mathcal{P}(\mathbf x)</script></span>. Under some assumptions and a baseline distribution for <span><span class="MathJax_Preview">q(\mathbf u)</span><script type="math/tex">q(\mathbf u)</script></span>,  we can try to approximate the complex distribution <span><span class="MathJax_Preview">\mathcal{P}(\mathbf x)</span><script type="math/tex">\mathcal{P}(\mathbf x)</script></span> by minimizing the distance between the two distributions, <span><span class="MathJax_Preview">D\left[q(\mathbf u)||\mathcal{P}(\mathbf x)\right]</span><script type="math/tex">D\left[q(\mathbf u)||\mathcal{P}(\mathbf x)\right]</script></span>. Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data (<a href="https://www.prowler.io/blog/sparse-gps-approximate-the-posterior-not-the-model">example blog</a>, <a href="">VFE paper</a>). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation <span><span class="MathJax_Preview">q(\mathbf u)</span><script type="math/tex">q(\mathbf u)</script></span> and the true GP posterior <span><span class="MathJax_Preview">\mathcal{P}(\mathbf x)</span><script type="math/tex">\mathcal{P}(\mathbf x)</script></span>. From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more.</p>
<hr />
<h3 id="variational-gp-model-with-latent-inputs">Variational GP Model with Latent Inputs<a class="headerlink" href="#variational-gp-model-with-latent-inputs" title="Permanent link">&para;</a></h3>
<p><strong>Posterior Distribution:</strong>
<span><span class="MathJax_Preview"><span><span class="MathJax_Preview">p(Y) = \int_{\mathcal X} p(Y|X) P(X) dX</span><script type="math/tex">p(Y) = \int_{\mathcal X} p(Y|X) P(X) dX</script></span></span><script type="math/tex"><span><span class="MathJax_Preview">p(Y) = \int_{\mathcal X} p(Y|X) P(X) dX</span><script type="math/tex">p(Y) = \int_{\mathcal X} p(Y|X) P(X) dX</script></span></script></span></p>
<p><strong>Derive the Lower Bound</strong> (w/ Jensens Inequality):</p>
<div>
<div class="MathJax_Preview">\log p(Y) = \log \int_{\mathcal X} p(Y|X) P(X) dX</div>
<script type="math/tex; mode=display">\log p(Y) = \log \int_{\mathcal X} p(Y|X) P(X) dX</script>
</div>
<p><strong>importance sampling/identity trick</strong></p>
<div>
<div class="MathJax_Preview"> = \log \int_{\mathcal F} p(Y|X) P(X) \frac{q(X)}{q(X)}dF</div>
<script type="math/tex; mode=display"> = \log \int_{\mathcal F} p(Y|X) P(X) \frac{q(X)}{q(X)}dF</script>
</div>
<p><strong>rearrange to isolate</strong>: <span><span class="MathJax_Preview">p(Y|X)</span><script type="math/tex">p(Y|X)</script></span> and shorten notation to <span><span class="MathJax_Preview">\langle \cdot \rangle_{q(X)}</span><script type="math/tex">\langle \cdot \rangle_{q(X)}</script></span>.</p>
<div>
<div class="MathJax_Preview">= \log \left\langle  \frac{p(Y|X)p(X)}{q(X)} \right\rangle_{q(X)}</div>
<script type="math/tex; mode=display">= \log \left\langle  \frac{p(Y|X)p(X)}{q(X)} \right\rangle_{q(X)}</script>
</div>
<p><strong>Jensens inequality</strong></p>
<div>
<div class="MathJax_Preview">\geq \left\langle \log \frac{p(Y|X)p(X)}{q(X)} \right\rangle_{q(X)}</div>
<script type="math/tex; mode=display">\geq \left\langle \log \frac{p(Y|X)p(X)}{q(X)} \right\rangle_{q(X)}</script>
</div>
<p><strong>Split the logs</strong></p>
<div>
<div class="MathJax_Preview">\geq \left\langle \log p(Y|X) + \log \frac{p(X)}{q(X)} \right\rangle_{q(X)}</div>
<script type="math/tex; mode=display">\geq \left\langle \log p(Y|X) + \log \frac{p(X)}{q(X)} \right\rangle_{q(X)}</script>
</div>
<p><strong>collect terms</strong></p>
<div>
<div class="MathJax_Preview">\mathcal{L}_{2}(q)=\left\langle \log p(Y|X)\right\rangle_{q(F)} - D_{KL} \left( q(X) || p(X)\right) </div>
<script type="math/tex; mode=display">\mathcal{L}_{2}(q)=\left\langle \log p(Y|X)\right\rangle_{q(F)} - D_{KL} \left( q(X) || p(X)\right) </script>
</div>
<p><strong>plug in other bound</strong></p>
<div>
<div class="MathJax_Preview">\mathcal{L}_{2}(q)=\left\langle \mathcal{L}_{1}(q)\right\rangle_{q(F)} - D_{KL} \left( q(X) || p(X)\right) </div>
<script type="math/tex; mode=display">\mathcal{L}_{2}(q)=\left\langle \mathcal{L}_{1}(q)\right\rangle_{q(F)} - D_{KL} \left( q(X) || p(X)\right) </script>
</div>
<hr />
<h3 id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)<a class="headerlink" href="#evidence-lower-bound-elbo" title="Permanent link">&para;</a></h3>
<p>In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. 
Traditional marginal likelhood (evidence) function that we're given is given by:</p>
<div>
<div class="MathJax_Preview">\underbrace{\mathcal{P}(y|\mathbf x, \theta)}_{\text{Evidence}}=\int_f \underbrace{\mathcal{P}(y|f, \mathbf x, 
\theta)}_{\text{Likelihood}} \cdot \underbrace{\mathcal{P}(f|\mathbf x, \theta)}_{\text{GP Prior}}df</div>
<script type="math/tex; mode=display">\underbrace{\mathcal{P}(y|\mathbf x, \theta)}_{\text{Evidence}}=\int_f \underbrace{\mathcal{P}(y|f, \mathbf x, 
\theta)}_{\text{Likelihood}} \cdot \underbrace{\mathcal{P}(f|\mathbf x, \theta)}_{\text{GP Prior}}df</script>
</div>
<p>where:</p>
<ul>
<li><span><span class="MathJax_Preview">\mathcal{P}(y|f, \mathbf x, \theta)=\mathcal{N}(y|f, \sigma_n^2\mathbf{I})</span><script type="math/tex">\mathcal{P}(y|f, \mathbf x, \theta)=\mathcal{N}(y|f, \sigma_n^2\mathbf{I})</script></span></li>
<li><span><span class="MathJax_Preview">\mathcal{P}(f|\mathbf x, \theta)=\mathcal{N}(f|\mu, K_\theta)</span><script type="math/tex">\mathcal{P}(f|\mathbf x, \theta)=\mathcal{N}(f|\mu, K_\theta)</script></span></li>
</ul>
<p>In this case we are marginalizing by the latent functions <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>'s. But we no longer consider <span><span class="MathJax_Preview">\mathbf x</span><script type="math/tex">\mathbf x</script></span> to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the <span><span class="MathJax_Preview">\mathbf x</span><script type="math/tex">\mathbf x</script></span>'s. In doing so we get:</p>
<div>
<div class="MathJax_Preview">\mathcal{P}(y| \theta)=\int_f\int_\mathcal{X} \mathcal{P}(y|f, \mathbf x, 
\theta)\cdot\mathcal{P}(f|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x)\cdot df \cdot d\mathbf{x}</div>
<script type="math/tex; mode=display">\mathcal{P}(y| \theta)=\int_f\int_\mathcal{X} \mathcal{P}(y|f, \mathbf x, 
\theta)\cdot\mathcal{P}(f|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x)\cdot df \cdot d\mathbf{x}</script>
</div>
<p>We can rearrange this equation to change notation:</p>
<div>
<div class="MathJax_Preview">\mathcal{P}(y| \theta)=\int_\mathcal{X} 
\underbrace{\left[ \int_f \mathcal{P}(y|f, \mathbf x, 
\theta)\cdot\mathcal{P}(f|\mathbf x, \theta)\cdot df\right]}_{\text{Evidence}}
\cdot \underbrace{\mathcal{P}(\mathbf x)}_{\text{Prior}} \cdot d\mathbf{x}</div>
<script type="math/tex; mode=display">\mathcal{P}(y| \theta)=\int_\mathcal{X} 
\underbrace{\left[ \int_f \mathcal{P}(y|f, \mathbf x, 
\theta)\cdot\mathcal{P}(f|\mathbf x, \theta)\cdot df\right]}_{\text{Evidence}}
\cdot \underbrace{\mathcal{P}(\mathbf x)}_{\text{Prior}} \cdot d\mathbf{x}</script>
</div>
<p>where we find that that term is simply the same term as the original likelihood, <span><span class="MathJax_Preview">\mathcal{P}(y|\mathbf x, \theta)</span><script type="math/tex">\mathcal{P}(y|\mathbf x, \theta)</script></span>. So our new simplified equation is:</p>
<div>
<div class="MathJax_Preview">\mathcal{P}(y| \theta)=\int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot d\mathbf{x}</div>
<script type="math/tex; mode=display">\mathcal{P}(y| \theta)=\int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot d\mathbf{x}</script>
</div>
<p>where we have effectively marginalized out the <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>'s. We already know that it's difficult to propagate the <span><span class="MathJax_Preview">\mathbf x</span><script type="math/tex">\mathbf x</script></span>'s through the nonlinear functions <span><span class="MathJax_Preview">\mathbf K^{-1}</span><script type="math/tex">\mathbf K^{-1}</script></span> and <span><span class="MathJax_Preview">|</span><script type="math/tex">|</script></span>det <span><span class="MathJax_Preview">\mathbf K|</span><script type="math/tex">\mathbf K|</script></span> (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution <span><span class="MathJax_Preview">q(\mathbf x)</span><script type="math/tex">q(\mathbf x)</script></span> to approximate the posterior distribution <span><span class="MathJax_Preview">\mathcal{P}(\mathbf x| y)</span><script type="math/tex">\mathcal{P}(\mathbf x| y)</script></span>. The distribution is normally chosen to be Gaussian:</p>
<div>
<div class="MathJax_Preview">q(\mathbf x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf x|\mathbf \mu_z, \mathbf \Sigma_z)</div>
<script type="math/tex; mode=display">q(\mathbf x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf x|\mathbf \mu_z, \mathbf \Sigma_z)</script>
</div>
<p>So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution <span><span class="MathJax_Preview">q(\mathbf x)</span><script type="math/tex">q(\mathbf x)</script></span> and the true posterior distribution <span><span class="MathJax_Preview">\mathcal{P} (\mathbf x)</span><script type="math/tex">\mathcal{P} (\mathbf x)</script></span>. Using the standard derivation for the ELBO, we arrive at the final formula:</p>
<div>
<div class="MathJax_Preview">\mathcal{F}(q)=\mathbb{E}_{q(\mathbf x)}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right] - \text{D}_\text{KL}\left[ q(\mathbf x) || \mathcal{P}(\mathbf x) \right]</div>
<script type="math/tex; mode=display">\mathcal{F}(q)=\mathbb{E}_{q(\mathbf x)}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right] - \text{D}_\text{KL}\left[ q(\mathbf x) || \mathcal{P}(\mathbf x) \right]</script>
</div>
<p>If we optimize <span><span class="MathJax_Preview">\mathcal{F}</span><script type="math/tex">\mathcal{F}</script></span> with respect to <span><span class="MathJax_Preview">q(\mathbf x)</span><script type="math/tex">q(\mathbf x)</script></span>, the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the <span><span class="MathJax_Preview">\mathbf x</span><script type="math/tex">\mathbf x</script></span>'s through. So that's nothing new and we've done nothing useful. If we introduce some special structure in <span><span class="MathJax_Preview">q(f)</span><script type="math/tex">q(f)</script></span> by introducing sparsity, then we can achieve something useful with this formulation.
 But through augmentation of the variable space with <span><span class="MathJax_Preview">\mathbf u</span><script type="math/tex">\mathbf u</script></span> and <span><span class="MathJax_Preview">\mathbf Z</span><script type="math/tex">\mathbf Z</script></span> we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.</p>
<hr />
<h2 id="uncertain-inputs">Uncertain Inputs<a class="headerlink" href="#uncertain-inputs" title="Permanent link">&para;</a></h2>
<p>So how does this relate to uncertain inputs exactly? Let's look again at our problem setting.</p>
<div>
<div class="MathJax_Preview">\begin{aligned}
y &amp;= f(x) + \epsilon_y \\
x &amp;\sim \mathcal{N}(\mu_x, \Sigma_x) \\
\end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned}
y &= f(x) + \epsilon_y \\
x &\sim \mathcal{N}(\mu_x, \Sigma_x) \\
\end{aligned}</script>
</div>
<p>where:</p>
<ul>
<li><span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> - noise-corrupted outputs which have a noise parameter characterized by <span><span class="MathJax_Preview">\epsilon_y \sim \mathcal{N}(0, \sigma^2_y)</span><script type="math/tex">\epsilon_y \sim \mathcal{N}(0, \sigma^2_y)</script></span></li>
<li><span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> - is the standard GP function</li>
<li><span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> - "latent variables" but we assume that the come from a normal distribution, <span><span class="MathJax_Preview">x \sim \mathcal{N}(\mu_x, \Sigma_x)</span><script type="math/tex">x \sim \mathcal{N}(\mu_x, \Sigma_x)</script></span> where you have some observations <span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span> but you also have some prior uncertainty <span><span class="MathJax_Preview">\Sigma_x</span><script type="math/tex">\Sigma_x</script></span> that you would like to incorporate.</li>
</ul>
<p>Now the ELBO that we want to minimize has the following form:</p>
<div>
<div class="MathJax_Preview">\mathcal{F}(q)=\mathbb{E}_{q(\mathbf x | m_{p_z}, S_{p_z})}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right] - \text{D}_\text{KL}\left[ q(\mathbf x | m_{p_z}, S_{p_z}) || \mathcal{P}(\mathbf x | m_{p_x}, S_{p_x}) \right]</div>
<script type="math/tex; mode=display">\mathcal{F}(q)=\mathbb{E}_{q(\mathbf x | m_{p_z}, S_{p_z})}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right] - \text{D}_\text{KL}\left[ q(\mathbf x | m_{p_z}, S_{p_z}) || \mathcal{P}(\mathbf x | m_{p_x}, S_{p_x}) \right]</script>
</div>
<p>Notice that I have expanded the parameters for <span><span class="MathJax_Preview">p(X)</span><script type="math/tex">p(X)</script></span> and <span><span class="MathJax_Preview">q(X)</span><script type="math/tex">q(X)</script></span> so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the <span><span class="MathJax_Preview">m_{p_x}</span><script type="math/tex">m_{p_x}</script></span>, <span><span class="MathJax_Preview">S_{p_x}</span><script type="math/tex">S_{p_x}</script></span>, <span><span class="MathJax_Preview">m_{p_z}</span><script type="math/tex">m_{p_z}</script></span>, and <span><span class="MathJax_Preview">S_{p_z}</span><script type="math/tex">S_{p_z}</script></span>. The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.</p>
<hr />
<h3 id="case-i-strong-prior">Case I - Strong Prior<a class="headerlink" href="#case-i-strong-prior" title="Permanent link">&para;</a></h3>
<p><strong>Prior</strong>, <span><span class="MathJax_Preview">p(X)</span><script type="math/tex">p(X)</script></span></p>
<p>We can directly assume that we know the parameters for the prior distribution. So we let <span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span> be our noisy observations and we let <span><span class="MathJax_Preview">\Sigma_x</span><script type="math/tex">\Sigma_x</script></span> be our known covariance matrix for <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>. These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of:</p>
<div>
<div class="MathJax_Preview">\mathcal{P}(\mathbf X|\mu_x, \Sigma_x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |\mathbf \mu_{\mathbf{x}_i},\mathbf \Sigma_\mathbf{x_i})</div>
<script type="math/tex; mode=display">\mathcal{P}(\mathbf X|\mu_x, \Sigma_x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |\mathbf \mu_{\mathbf{x}_i},\mathbf \Sigma_\mathbf{x_i})</script>
</div>
<p>and this will be our regularization that we use for the KL divergence term.</p>
<p><strong>Variational</strong>, <span><span class="MathJax_Preview">q(X)</span><script type="math/tex">q(X)</script></span></p>
<p>However, the variational parameters <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> and <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> are also important because that is being directly evaluated with the KL divergence term and the likelihood function <span><span class="MathJax_Preview">\log p(y|X, \theta)</span><script type="math/tex">\log p(y|X, \theta)</script></span>. So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, <span><span class="MathJax_Preview">m=\mu_x</span><script type="math/tex">m=\mu_x</script></span> and <span><span class="MathJax_Preview">S_{p_z} = \Sigma_x</span><script type="math/tex">S_{p_z} = \Sigma_x</script></span>. So our prior for our variational distribution will be:</p>
<div>
<div class="MathJax_Preview">q(\mathbf X|\mu_x, \Sigma_x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |\mathbf \mu_{\mathbf{x}_i},\mathbf \Sigma_\mathbf{x_i})</div>
<script type="math/tex; mode=display">q(\mathbf X|\mu_x, \Sigma_x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |\mathbf \mu_{\mathbf{x}_i},\mathbf \Sigma_\mathbf{x_i})</script>
</div>
<p>We now have our variational bound with the assumed parameters:</p>
<div>
<div class="MathJax_Preview">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf X|\mu_x, \Sigma_x)} - \text{KL}\left( q(\mathbf X|\mu_x, \Sigma_x) || p(\mathbf X|\mu_x, \Sigma_x)  \right)</div>
<script type="math/tex; mode=display">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf X|\mu_x, \Sigma_x)} - \text{KL}\left( q(\mathbf X|\mu_x, \Sigma_x) || p(\mathbf X|\mu_x, \Sigma_x)  \right)</script>
</div>
<p><strong>Assessment</strong></p>
<p>So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.</p>
<hr />
<h3 id="case-ii-regularized-strong-prior">Case II - Regularized Strong Prior<a class="headerlink" href="#case-ii-regularized-strong-prior" title="Permanent link">&para;</a></h3>
<p>This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this</p>
<div>
<div class="MathJax_Preview">\mathcal{P}(\mathbf X|0, 1) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |0, 1)</div>
<script type="math/tex; mode=display">\mathcal{P}(\mathbf X|0, 1) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |0, 1)</script>
</div>
<p>This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is:</p>
<div>
<div class="MathJax_Preview">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf X|\mu_x, \Sigma_x)} - \text{KL}\left( q(\mathbf X|\mu_x, \Sigma_x) || p(\mathbf X|0, 1)  \right)</div>
<script type="math/tex; mode=display">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf X|\mu_x, \Sigma_x)} - \text{KL}\left( q(\mathbf X|\mu_x, \Sigma_x) || p(\mathbf X|0, 1)  \right)</script>
</div>
<hr />
<h3 id="case-iii-prior-with-openness">Case III - Prior with Openness<a class="headerlink" href="#case-iii-prior-with-openness" title="Permanent link">&para;</a></h3>
<p>The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, </p>
<p>We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we </p>
<p>We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. </p>
<div>
<div class="MathJax_Preview">q(\mathbf X|\mathbf Z) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |\mathbf z_i,\mathbf \Sigma_\mathbf{z_i})</div>
<script type="math/tex; mode=display">q(\mathbf X|\mathbf Z) = \prod_{i=1}^{N}\mathcal{N}(\mathbf{x}_i |\mathbf z_i,\mathbf \Sigma_\mathbf{z_i})</script>
</div>
<p>We will have a new variational bound now:</p>
<div>
<div class="MathJax_Preview">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf X|\mu_x, S)} - \text{KL}\left( q(\mathbf X|\mu_x, S) || p(\mathbf X|\mu_x, \Sigma_x)  \right)</div>
<script type="math/tex; mode=display">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf X|\mu_x, S)} - \text{KL}\left( q(\mathbf X|\mu_x, S) || p(\mathbf X|\mu_x, \Sigma_x)  \right)</script>
</div>
<p>So the only free parameter in the variational bound is the actual variance of our inputs <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> that stems from our variational distribution <span><span class="MathJax_Preview">q(X)</span><script type="math/tex">q(X)</script></span>. Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. </p>
<hr />
<h3 id="case-iv-bonus-conservative-freedom">Case IV - Bonus, Conservative Freedom<a class="headerlink" href="#case-iv-bonus-conservative-freedom" title="Permanent link">&para;</a></h3>
<p>Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We </p>
<div>
<div class="MathJax_Preview">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf{X|Z})} - \text{KL}\left( q(\mathbf{X|Z}) || \mathcal{P}(\mathbf{X}) \right)</div>
<script type="math/tex; mode=display">\mathcal{F}=\langle \log \mathcal{P}(\mathbf{Y|X}) \rangle_{q(\mathbf{X|Z})} - \text{KL}\left( q(\mathbf{X|Z}) || \mathcal{P}(\mathbf{X}) \right)</script>
</div>
<p><center></p>
<table>
<thead>
<tr>
<th align="center">Options</th>
<th align="center"><span><span class="MathJax_Preview">m_{p_x}</span><script type="math/tex">m_{p_x}</script></span></th>
<th align="center"><span><span class="MathJax_Preview">S_{p_x}</span><script type="math/tex">S_{p_x}</script></span></th>
<th align="center"><span><span class="MathJax_Preview">m_{p_z}</span><script type="math/tex">m_{p_z}</script></span></th>
<th align="center"><span><span class="MathJax_Preview">S_{p_z}</span><script type="math/tex">S_{p_z}</script></span></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">No Prior</td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center">1</td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center">S_z</td>
</tr>
<tr>
<td align="center">Strong Conservative Prior</td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center">1</td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center"><span><span class="MathJax_Preview">\Sigma_x</span><script type="math/tex">\Sigma_x</script></span></td>
</tr>
<tr>
<td align="center">Strong Prior</td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center"><span><span class="MathJax_Preview">\Sigma_x</span><script type="math/tex">\Sigma_x</script></span></td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center"><span><span class="MathJax_Preview">\Sigma_x</span><script type="math/tex">\Sigma_x</script></span></td>
</tr>
<tr>
<td align="center">Bayesian Prior</td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center"><span><span class="MathJax_Preview">\Sigma_x</span><script type="math/tex">\Sigma_x</script></span></td>
<td align="center"><span><span class="MathJax_Preview">\mu_x</span><script type="math/tex">\mu_x</script></span></td>
<td align="center">S_z</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><strong>Caption</strong>: Summary of Options</p>
<hr />
<h2 id="resources">Resources<a class="headerlink" href="#resources" title="Permanent link">&para;</a></h2>
<h4 id="important-papers">Important Papers<a class="headerlink" href="#important-papers" title="Permanent link">&para;</a></h4>
<p>These are the important papers that helped me understand what was going on throughout the learning process.</p>
<h4 id="summary-thesis">Summary Thesis<a class="headerlink" href="#summary-thesis" title="Permanent link">&para;</a></h4>
<p>Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put  in their articles especially with cryptic explanations like "it's easy to show that..." or "trivially it can be shown that...". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult.</p>
<ul>
<li><a href="https://lib.ugent.be/fulltxt/RUG01/002/367/115/RUG01-002367115_2017_0001_AC.pdf">Non-Stationary Surrogate Modeling with Deep Gaussian Processes</a> - Dutordoir (2016)</li>
<li>Chapter IV - Finding Uncertain Patterns in GPs</li>
<li><a href="http://etheses.whiterose.ac.uk/18492/1/MaxZwiesseleThesis.pdf">Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences</a> - Zwießele (2017)</li>
<li>Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM</li>
<li><a href="http://etheses.whiterose.ac.uk/9968/1/Damianou_Thesis.pdf">Deep GPs and Variational Propagation of Uncertainty</a> - Damianou (2015)</li>
<li>Chapter IV - Uncertain Inputs in Variational GPs</li>
<li>Chapter II (2.1) - Lit Review</li>
</ul>
<h4 id="talks">Talks<a class="headerlink" href="#talks" title="Permanent link">&para;</a></h4>
<ul>
<li>Damianou - Bayesian LVM with GPs - <a href="http://gpss.cc/gpss15/talks/gpss_BGPLVMs.pdf">MLSS2015</a></li>
<li>Lawrence - Deep GPs - <a href="">MLSS2019</a></li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2020 J. Emmanuel Johnson
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/jejjohnson" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
      <a href="https://twitter.com/jejjohnson" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
      
      
      <a href="https://linkedin.com/in/jejjohnson" target="_blank" rel="noopener" title="linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
      
      
      <a href="https://jejjohnson.netlify.com" target="_blank" rel="noopener" title="jejjohnson.netlify.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M336.5 160C322 70.7 287.8 8 248 8s-74 62.7-88.5 152h177zM152 256c0 22.2 1.2 43.5 3.3 64h185.3c2.1-20.5 3.3-41.8 3.3-64s-1.2-43.5-3.3-64H155.3c-2.1 20.5-3.3 41.8-3.3 64zm324.7-96c-28.6-67.9-86.5-120.4-158-141.6 24.4 33.8 41.2 84.7 50 141.6h108zM177.2 18.4C105.8 39.6 47.8 92.1 19.3 160h108c8.7-56.9 25.5-107.8 49.9-141.6zM487.4 192H372.7c2.1 21 3.3 42.5 3.3 64s-1.2 43-3.3 64h114.6c5.5-20.5 8.6-41.8 8.6-64s-3.1-43.5-8.5-64zM120 256c0-21.5 1.2-43 3.3-64H8.6C3.2 212.5 0 233.8 0 256s3.2 43.5 8.6 64h114.6c-2-21-3.2-42.5-3.2-64zm39.5 96c14.5 89.3 48.7 152 88.5 152s74-62.7 88.5-152h-177zm159.3 141.6c71.4-21.2 129.4-73.7 158-141.6h-108c-8.8 56.9-25.6 107.8-50 141.6zM19.3 352c28.6 67.9 86.5 120.4 158 141.6-24.4-33.8-41.2-84.7-50-141.6h-108z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.f81b9e8b.min.js"></script>
      <script src="../../assets/javascripts/bundle.23546af0.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.58d22e8e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../javascripts/extra.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
      
    
  </body>
</html>